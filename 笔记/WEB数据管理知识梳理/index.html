<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>WEB数据管理知识梳理 | 姜将的个人博客</title><meta name="author" content="姜将"><meta name="copyright" content="姜将"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="第1讲  前言本文是根据山东大学软件学院连莉老师的《WEB数据管理》课程课件所整理的。近三万字长文，实在充实。 第2讲  网络爬虫技术一 、爬虫定义一种自动获取网页内容的程序，是搜索引擎的重要组成部分。通俗的讲，也就是通过HTML源码解析来获得想要的内容 二、爬取过程从一个或若干初始网页的URL开始，直到满足系统的一定停止条件。 三、 URL 判重•访问标记 由于搜索引擎在爬取时要访问大量的网页，">
<meta property="og:type" content="article">
<meta property="og:title" content="WEB数据管理知识梳理">
<meta property="og:url" content="http://jiang54864.github.io/%E7%AC%94%E8%AE%B0/WEB%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/index.html">
<meta property="og:site_name" content="姜将的个人博客">
<meta property="og:description" content="第1讲  前言本文是根据山东大学软件学院连莉老师的《WEB数据管理》课程课件所整理的。近三万字长文，实在充实。 第2讲  网络爬虫技术一 、爬虫定义一种自动获取网页内容的程序，是搜索引擎的重要组成部分。通俗的讲，也就是通过HTML源码解析来获得想要的内容 二、爬取过程从一个或若干初始网页的URL开始，直到满足系统的一定停止条件。 三、 URL 判重•访问标记 由于搜索引擎在爬取时要访问大量的网页，">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg">
<meta property="article:published_time" content="2023-06-07T07:46:51.000Z">
<meta property="article:modified_time" content="2025-01-25T08:00:35.046Z">
<meta property="article:author" content="姜将">
<meta property="article:tag" content="web">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg"><link rel="shortcut icon" href="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg"><link rel="canonical" href="http://jiang54864.github.io/%E7%AC%94%E8%AE%B0/WEB%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'WEB数据管理知识梳理',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-01-25 16:00:35'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = 'hidden';
    document.getElementById('loading-box').classList.remove("loaded")
  }
}

preloader.initLoading()
window.addEventListener('load',()=> { preloader.endLoading() })

if (false) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk/"><i class="fa-fw fa fa-comment"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="姜将的个人博客"><img class="site-icon" src="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg"/><span class="site-name">姜将的个人博客</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk/"><i class="fa-fw fa fa-comment"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">WEB数据管理知识梳理</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-06-07T07:46:51.000Z" title="发表于 2023-06-07 15:46:51">2023-06-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-01-25T08:00:35.046Z" title="更新于 2025-01-25 16:00:35">2025-01-25</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">24.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>76分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="WEB数据管理知识梳理"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="第1讲-前言"><a href="#第1讲-前言" class="headerlink" title="第1讲  前言"></a>第1讲  前言</h1><p>本文是根据山东大学软件学院连莉老师的《WEB数据管理》课程课件所整理的。近三万字长文，实在充实。</p>
<h1 id="第2讲-网络爬虫技术"><a href="#第2讲-网络爬虫技术" class="headerlink" title="第2讲  网络爬虫技术"></a>第2讲  网络爬虫技术</h1><h2 id="一-、爬虫定义"><a href="#一-、爬虫定义" class="headerlink" title="一 、爬虫定义"></a>一 、爬虫定义</h2><p>一种<strong>自动</strong>获取网页内容的程序，是搜索引擎的重要组成部分。通俗的讲，也就是通过<strong>HTML</strong>源码<strong>解析</strong>来获得想要的内容</p>
<h2 id="二、爬取过程"><a href="#二、爬取过程" class="headerlink" title="二、爬取过程"></a>二、爬取过程</h2><p>从一个或若干初始网页的URL开始，直到满足系统的一定停止条件。</p>
<h2 id="三、-URL-判重"><a href="#三、-URL-判重" class="headerlink" title="三、 URL 判重"></a>三、 URL 判重</h2><p>•访问标记</p>
<p>由于搜索引擎在爬取时要访问大量的网页，因此在查找网址l是否访问过及标记网址l已经访问时为了提高查找和访问效率通常建立一个散列，其中存放访问过每一个网址，为了减少这个散列表所占用的空间, 通常在其中存放<strong>网址经过散列函数（如MD5、SHA-1等）计算出的对应的固定长度的散列值</strong>，这样便可以在平均情况下O(1)的时间内查找和更新占用O(n)空间的网址列表（n为已访问的网址数目）。</p>
<h2 id="四、-必须具有的功能"><a href="#四、-必须具有的功能" class="headerlink" title="四、  必须具有的功能"></a>四、  必须具有的功能</h2><h3 id="•4-1-礼貌性"><a href="#•4-1-礼貌性" class="headerlink" title="•4.1 礼貌性"></a>•<strong>4.1</strong> <strong>礼貌性</strong></h3><p>Web服务器有显式或隐式的策略控制爬虫的访问</p>
<p>–<strong>只爬允许爬的内容、尊重 robots.txt</strong></p>
<p>•隐式的礼貌:即使没有特别的说明，也不应该频繁的访问同一个网站</p>
<p>•显式的礼貌:根据网站站长的说明，选择允许爬取的部分进行爬取</p>
<h3 id="•4-2-鲁棒性"><a href="#•4-2-鲁棒性" class="headerlink" title="•4.2 鲁棒性"></a>•<strong>4.2</strong> <strong>鲁棒性</strong></h3><p>能从采集器陷阱中跳出，能处理Web服务器的其他恶意行为</p>
<h3 id="•4-3-性能和效率"><a href="#•4-3-性能和效率" class="headerlink" title="•4.3 性能和效率"></a>•<strong>4.3</strong> <strong>性能和效率</strong></h3><p>充分利用不同的系统资源，包括处理器、存储器和网络带宽。优先抓取“有用的网页”</p>
<p>•BFS广度优先搜索优于DFS:<br>工程上，网络爬虫更应该定义为“如何<strong>在有限的时间里最多的爬下那些重要的网页</strong>”，我们一般认为一个网页的<strong>首页</strong>是相当重要的。</p>
<p>•DFS深度优先优于BFS:<br>与<strong>爬虫的分布式结构以及网络通信的握手成本</strong>有关，“握手”就是指下载服务器与网站的服务器建立通信的过程。实际网络爬虫是由成百上千万台服务器组成的分布式系统，对于某一个网页，一般由特定的一台或者几台服务器专门下载，这样可以<strong>避免握手次数太多</strong>。</p>
<p>•实际应用的网络爬虫不是对网页次序的简单BFS或者BFS，而是一个相对复杂的下载优先级排序的方法，管理这个系统的叫做“调度系统”(Scheduler)，会有一个Priority Queue。BFS成分更加多一些。</p>
<p>•DFS 要限定爬取的深度<br>在爬取时为了防止有些错误链接导致的<strong>无穷递归</strong>爬取，需要限定爬取的深度。此外层次越深的网页对用户来说可用的信息越少。</p>
<h3 id="•4-4-分布式"><a href="#•4-4-分布式" class="headerlink" title="•4.4 分布式"></a>•<strong>4.4</strong> <strong>分布式</strong></h3><p>可以在多台机器上分布式运行，添加更多机器后采集率应该提高</p>
<p>•分布式带来的问题<br>问题1、哈希表太大，一台下载服务器存不下。<br>问题2、每台下载服务器在开始下载前和完成下载后都要维护这张哈希表，这个存储哈希表的通信就成为爬虫系统的瓶颈。</p>
<p>•解决方法：<br>A、明确每台下载服务器的分工，即一看到某个URL就知道交给哪台服务器去执行<br>B、批量处理，减少通信的次数</p>
<h3 id="•4-5-新鲜度"><a href="#•4-5-新鲜度" class="headerlink" title="•4.5 新鲜度"></a>•<strong>4.5</strong> <strong>新鲜度</strong></h3><p>对原来抓取的网页进行更新</p>
<p>•需要为更新较快的页面提高刷新率<br>•含有那些突增的搜索关键字的网站会得到较快的更新频率。</p>
<h3 id="•4-6-功能可扩展性"><a href="#•4-6-功能可扩展性" class="headerlink" title="•4.6 功能可扩展性"></a>•<strong>4.6</strong> <strong>功能可扩展性</strong></h3><p>支持多方面的功能扩展，例如处理新的数据格式、新的抓取协议等。</p>
<h2 id="五、爬虫分类"><a href="#五、爬虫分类" class="headerlink" title="五、爬虫分类"></a>五、爬虫分类</h2><p>实际的采集器往往是几种采集技术的结合</p>
<h3 id="5-1-基于整个Web的信息采集"><a href="#5-1-基于整个Web的信息采集" class="headerlink" title="5.1 基于整个Web的信息采集"></a>5.1 基于整个Web的信息采集</h3><p>•传统的采集方式：作为门户搜索引擎和大型的Web服务提供商的数据收集部分，是指<strong>从一些种子URL扩充到整个Web的信息采集</strong></p>
<p>•优点：采集数据广，采集速度快，适用于广泛主题的搜索<br>•缺点：采集数据乱，数据利用率低，页面失效率高，采集周期长</p>
<p>开源工具Nutch：一个开源Java 实现的<strong>搜索引擎</strong>。它提供了我们运行自己的搜索引擎所需的全部工具。包括全文搜索和Web爬虫。</p>
<p>Nutch爬取：多线程；宽度优先（广度优先）；遵循机器人协议，先读取/robots.txt 然后判断是否允许访问；nutch中采用的是socket获得连接并getInputStream，实际调用的是protocol的插件。边爬取边解析：获取一个页面后，对其进行解析，存储原始页面和解析后的文档。并提取出链outlink；页面评分：解析页面时、更新crawldb时。</p>
<h3 id="5-2-增量式Web信息采集"><a href="#5-2-增量式Web信息采集" class="headerlink" title="5.2 增量式Web信息采集"></a>5.2 增量式Web信息采集</h3><p>•在页面刷新时，只需要采集<strong>新产生的或者已经发生变化</strong>的页面，而对于没有变化的页面不进行采集</p>
<p>•预测变化的策略：<br>–基于统计的方法：观察网站的平均变化周期<br>–基于数据建模的方法：通过网页中变化估计模型和参数</p>
<p>•优点：极大地减小数据采集量进而极大地减小采集时空开销 。<br>•缺点：增加了一定的判别开销。网页相似度比较</p>
<h3 id="5-3-基于主题的Web信息采集"><a href="#5-3-基于主题的Web信息采集" class="headerlink" title="5.3 基于主题的Web信息采集"></a>5.3 基于主题的Web信息采集</h3><p>•选择性的搜寻那些<strong>与预先定义好的主题集相关</strong>页面进行采集<br>–给定特定的种子URL</p>
<p>•目前是研究热点，垂直搜索<br>•优点:采集页面更加有针对性，采集效率更高。<br>•缺点:采集速度较慢，判别相关性带来较大的开销。</p>
<p>–采集系统首先保存一个经典的主题分类,每个主题分类都保存若干个内容样本</p>
<h3 id="5-4-基于用户个性化的Web信息采集"><a href="#5-4-基于用户个性化的Web信息采集" class="headerlink" title="5.4 基于用户个性化的Web信息采集"></a>5.4 基于用户个性化的Web信息采集</h3><p>•不同的用户对一个搜索引擎提交同一个检索词，他们期望的返回结果是不同的 </p>
<p>–通过用户兴趣制导或与用户交互等灵活手段来采集信息 :用户画像profile、日志</p>
<h1 id="第3讲-网页分析技术"><a href="#第3讲-网页分析技术" class="headerlink" title="第3讲    网页分析技术"></a>第3讲    网页分析技术</h1><p>对于HTML文档，有两种看待方式:一种是将文档看作字符流：正则表达式；一种是将文档看作树结构。基于DOM</p>
<h2 id="一、正则表达式"><a href="#一、正则表达式" class="headerlink" title="一、正则表达式"></a>一、正则表达式</h2><p>规则表达式   Regular Expression，在代码中常简写为regex、regexp或RE</p>
<p>正则表达式是<strong>对字符串操作的一种逻辑公式</strong>，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，用来表达对字符串的一种<strong>过滤逻辑</strong>。通常被用来检索、替换那些符合某个模式(规则)的文本。</p>
<p>正则表达式由一组普通字符和一组元字符组成的字符串，用来表示符合一定模式的一组字符串，常用于字符串处理，表单验证等场合，表示能力与正规文法相同。</p>
<p>^ 匹配开始位置，$ 匹配结束位置，.  匹配除换行符以外的任意字符，[0-9] 匹配单个数字，[0-9]+匹配多个数字， ? 问号代表前面的字符最多只可以出现一次（0次、或1次）</p>
<h2 id="二、基于HTML-DOM提取内容"><a href="#二、基于HTML-DOM提取内容" class="headerlink" title="二、基于HTML DOM提取内容"></a>二、基于HTML DOM提取内容</h2><p>•2.1 DOM：文档对象模型（document object model）</p>
<p>•DOM将一个XML<strong>文档转换成一个对象集合</strong>，然后可以<strong>任意处理</strong>该对象模型。<br>这一机制也称为“<strong>随机访问</strong>”协议，可以在任何时间访问数据的任何一部分，然后修改、删除或插入新数据。 </p>
<p>•DOM将HTML视为树状结构的元素，所有元素以及他们的文字和属性可通过DOM树来操作与访问。</p>
<p>2.2正则表达式与DOM树方法的比较</p>
<p>•正则表达式匹配：匹配<strong>速度快</strong>，但<strong>表达能力较弱</strong>，只具有<strong>正规文法</strong>的表示能力。在对网页内容的<strong>信噪比要求不高</strong>的情况下可以使用基于正则表达式匹配的爬取程序</p>
<p>•HTML DOM树：HTML DOM树提取在解析HTML时速度较慢，但其表达能力相当于上下文无关文法。在网页自动分类等需要进行网页<strong>去噪处理</strong>的情况时使用基于HTML DOM树的爬取程序。</p>
<p>2.3HTML解析器的工作：将html标识解析为解析树。</p>
<h2 id="三、-Beautiful-Soup模块"><a href="#三、-Beautiful-Soup模块" class="headerlink" title="三、 Beautiful Soup模块"></a>三、 Beautiful Soup模块</h2><p>•python的一个模块，第三方的库，使用之前需要安装。提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。</p>
<p>•它是一个<strong>工具箱</strong>，通过解析文档为用户提供需要抓取的数据，简单，不需要多少代码就可以写出一个完整的应用程序</p>
<p>•bs4库将任何读入的html文件或字符串都转换为<strong>utf-8</strong>编码</p>
<p><strong>比较</strong>：</p>
<p>•re正则表达式：匹配神器，速度快效率高</p>
<p>•BeautifulSoup：比较常用且使用简单的技术，由于在操作过程中，会将整个文档树进行加载然后进行查询匹配操作，使用过程中消耗资源较多。</p>
<h2 id="四、-python爬虫框架-Scrapy"><a href="#四、-python爬虫框架-Scrapy" class="headerlink" title="四、  python爬虫框架 Scrapy"></a>四、  python爬虫框架 Scrapy</h2><p>快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取<strong>结构化</strong>的数据。Scrapy吸引人的地方在于它是一个框架</p>
<p><img src="1685590914977.png" alt="1685590914977"></p>
<p><strong>比较：</strong></p>
<p>•request和bs4:页面级爬虫，功能库;并行性考虑不足，性能较差;重点在于页面下载;初学容易上手，大型开发就比较麻烦，要自己造轮子，适合学生党偶尔有个需求</p>
<p>•Scrapy:网站级爬虫，框架;并行性好，性能较高;重点在于爬虫结构;初学麻烦，或者觉得它过于庞大，但是做项目后发现还是用轮子更方便，你能想到的小功能，它很多都封装好</p>
<h2 id="五、元搜索引擎"><a href="#五、元搜索引擎" class="headerlink" title="五、元搜索引擎"></a>五、元搜索引擎</h2><p>元搜索引擎又称多搜索引擎。通过一个<strong>统一的用户界面</strong>帮助用户在<strong>多个搜索引擎中选择和利用</strong>合适的（甚至是同时利用若干个）搜索引擎来实现检索操作，是对分布于网络的多种检索工具的全局控制机制</p>
<h1 id="第4讲-爬虫与网站的博弈"><a href="#第4讲-爬虫与网站的博弈" class="headerlink" title="第4讲  爬虫与网站的博弈"></a>第4讲  爬虫与网站的博弈</h1><p>•反爬虫策略：页面希望能在用户面前正常展示，同时又不给爬虫机会，就必须要做到识别真人与机器人。</p>
<p>•爬虫策略：可以无限逼近于真人，模拟浏览器操作</p>
<p>•<strong>网站反爬后端策略，在后端拦截</strong></p>
<p>•User-Agent + Referer检测（服务器可以通过Referer字段来判断用户是从哪个页面链接进来的）</p>
<p>•账号及Cookie验证</p>
<p>•验证码</p>
<p>•IP限制频次</p>
<p>•<strong>网站反爬前端策略</strong>，<strong>网页在前端显示需要浏览器配合</strong></p>
<p>•FONT-FACE拼凑式  ：猫眼电影里，对于票房数据，展示的并不是纯粹的数字。必须同时查询字符集，才能识别出数字</p>
<p>–CSS, JS</p>
<h2 id="一、Robot-协议"><a href="#一、Robot-协议" class="headerlink" title="一、Robot 协议"></a>一、Robot 协议</h2><p>•也称为爬虫协议、机器人协议等，全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。</p>
<p>•robots.txt文件是一个文本文件，在网站虚拟根目录</p>
<p>•当一个礼貌的爬虫访问一个站点时，它会首先检查该站点根目录下是否存在robots.txt，如果存在，搜索机器人就会按照该文件中的内容来确定访问的范围</p>
<p>urllib 是一个收集了多个涉及 URL 的模块的包：urllib.request 打开和读取 URL；urllib.robotparser 用于解析 robots.txt 文件</p>
<h2 id="二、User-agent"><a href="#二、User-agent" class="headerlink" title="二、User-agent"></a>二、User-agent</h2><p>•<strong>网站：分析用户的User-agent，根据大全过滤未知的或者指定的</strong></p>
<p>•<strong>爬虫：篡改自己的User-agent，伪装浏览器</strong></p>
<p>•User Agent 用户代理简称UA。是Http协议中的一部分，属于头域的组成部分，向访问网站提供访问者信息：所使用的浏览器类型、浏览器语言、浏览器插件、操作系统及版本、CPU 类型、浏览器渲染引擎</p>
<p>•UA字符串在每次浏览器 HTTP 请求时发送到服务器！</p>
<p>•通过这个标识，用户所访问的网站可以显示不同的排版从而为用户提供更好的体验或者进行信息统计　</p>
<h2 id="三、IP屏蔽"><a href="#三、IP屏蔽" class="headerlink" title="三、IP屏蔽"></a>三、IP屏蔽</h2><p><strong>网站：同一 IP 访问频繁：封！网站限制某些ip访问，仅允许指定ip地址访问</strong><br><strong>爬虫：多IP并行；增大爬取时间间隔；连接代理服务器，使用IP代理池中的代理IP</strong></p>
<p>Proxy Server代理服务器，其功能就是代理网络用户去取得网络信息。形象的说：它是网络信息的中转站<br>代理的分类：<br>正向代理：代理客户端获取数据。正向代理是为了保护客户端防止被追究责任。<br>反向代理：代理服务器提供数据。反向代理是为了保护服务器或负责负载均衡。</p>
<p>爬虫代理池：在各大网站爬取免费代理ip，检查ip可用，可用存入数据库，定时检查数据库的代理数量，以及是否可用</p>
<h2 id="四、用户登陆"><a href="#四、用户登陆" class="headerlink" title="四、用户登陆"></a>四、用户登陆</h2><p>网站：使用Cookie或会话来跟踪用户状态和活动。</p>
<p>爬虫：模拟常见的浏览器行为，包括发送标准的用户代理、处理Cookie和会话，以及执行JavaScript渲染等。这可以使爬虫程序更难被检测和阻止。</p>
<p>工作过程<br>用户输入：用户名，口令，通过GET（POST)参数提交参数，后台PHP 程序生成的网页，数据在后台数据库，登陆成功，带着Cookie继续访问其他网页</p>
<p>网站登陆网页<br>1.表单<br>action属性，action的值是表单处理程序的网络路径和程序名<br>method属性，用来定义服务器表单处理程序从表单中获得信息的方式<br>method的值（get、post，默认get）<br>GET方法，将数据打包放置在环境变量QUERY_STRING中作为<strong>URL整体的一部分</strong>传递给服务器<br>POST方法，分离地传递数据给服务器表单处理程序，不需要设置QUERY_STRING环境变量，有更好的安全性，表单中数据的多少是任意的<br>2.文本框控件   输入用户名user，口令psw<br>3.submit 按钮构造URL</p>
<p>分析post过程中隐藏的变量名：<br>方法：<br>HTTP分析工具：HttpFox插件<br>模拟浏览器</p>
<p>•Cookie：小文本文件，服务器在HTTP响应头中发送给用户浏览器，维持客户端与服务器端的状态，保存在客户端的一个目录中</p>
<p>•用于维护浏览器和服务器的会话</p>
<p>–HTTP协议是无状态的</p>
<p>•Cookie指的是当你浏览某网站时，网站存储在你电脑上的一个小文本文件，伴随着用户请求和页面在 Web 服务器和浏览器之间传递。</p>
<p>–http 请求带着Cookie</p>
<p>•它记录了你的用户ID，密码、浏览过的网页、停留的时间等信息，用于用户身份的辨别。</p>
<p>•Cookie通常是以user@domain格式命名的，user是你的本地用户名，domain是所访问的网站的域名。</p>
<p>•python2的时候用cookielib来进行cookie的相关操作，Python3  cookielib 模块改名为 http.cookiejar</p>
<h2 id="五、模拟浏览器进行交互"><a href="#五、模拟浏览器进行交互" class="headerlink" title="五、模拟浏览器进行交互"></a>五、模拟浏览器进行交互</h2><p>网站：采用动态页面和JavaScript渲染来生成内容</p>
<p>爬虫：解析动态页面和执行JavaScript，需要具备解析和执行JavaScript的能力，以获取完整的页面内容。</p>
<p>5.1   爬虫神器：selenium</p>
<p>WEB自动化工具，用于Web应用程序测试。Selenium测试直接运行在浏览器中，就像真正的用户在操作一样。</p>
<p>Webdriver：可以认为是浏览器的驱动器，要驱动浏览器必须用到webdriver，支持多种浏览器</p>
<p>5.2   模拟AJAX </p>
<p>AJAX: Asynchronous异步JavaScript+XML<br>传统的网页（不使用 AJAX）如果需要更新内容，必须重载整个网页页面<br>通过在后台与服务器进行少量数据交换，AJAX 可以使网页实现异步更新。<br>在不重新加载整个网页的情况下，对网页的某部分进行更新</p>
<p>5.3    应用实例<br><strong>实例1：懒加载</strong></p>
<p>•为了避免页面一次性向服务器发送大量请求而造成页面阻塞，我们需要控制请求数量，按照我们需要的量去加载图片。</p>
<p>–预加载</p>
<p>–懒加载：延时加载，即当对象需要用到的时候再去加载</p>
<p>•懒加载的优点：提高前端性能，按需加载图片减轻服务器负担，提高页面加载速度。</p>
<p>图片懒加载的原理：图片的加载是依赖于src路径，设置一个暂存器，把图片路径放到暂存器中，当需要这个图片加载显示时，把路径赋值给src，这样就能实现按需加载，也就是懒加载。</p>
<p>•爬取懒加载图片</p>
<p>–简单方法： 寻找替代src属性的data-属性，Src2属性，original属性数值</p>
<p>–模拟浏览器方法，可以解决多种反爬虫</p>
<p><strong>实例2：Iframe</strong></p>
<p>\<iframe>，又叫浮动帧标记，是内嵌的网页元素(图片，网页),可以将一个html文件嵌入到另一个html文件中显示</p>
<h2 id="六、验证码"><a href="#六、验证码" class="headerlink" title="六、验证码"></a>六、验证码</h2><p>网站：可以要求用户进行验证码或人机验证，以区分人类用户和爬虫。这可以是简单的图像验证码、数学问题或复杂的行为分析等方式。</p>
<p>爬虫：对于要求验证码的网站，可以使用验证码识别技术或人力识别服务来解决验证码问题。</p>
<p>获取图片（下载或截图）——图片处理——获取图片中文字内容。</p>
<p>OCR：光学字符识别，将图像中的文字转换成文本格式。</p>
<p>滑动验证码：<br>判断验证码什么时候出现。<br>验证码出现时，判断何时加载完成。<br>自动识别出鼠标拖拽轨迹的初始位置和终止位置。<br>用鼠标模拟拖动。<br>检验是否成功。</p>
<p>selenium处理方式：获取不带缺口的图片，再获取带缺口的图片，灰度比较得到缺口的各个坐标，进行滑动。（滑动不能过于匀速，可以滑过头再返回）。</p>
<p>由于selenium是在后台完成的滑动，而有些平台会检测鼠标光标未移动，可以使用pyautogui。</p>
<h1 id="第5讲-数据抽取与包装器"><a href="#第5讲-数据抽取与包装器" class="headerlink" title="第5讲  数据抽取与包装器"></a>第5讲  数据抽取与包装器</h1><h2 id="一、WEB数据抽取"><a href="#一、WEB数据抽取" class="headerlink" title="一、WEB数据抽取"></a>一、WEB数据抽取</h2><p><strong>定义(页面模板)</strong>：是不完全的页面，它是相似页面之间不变的部分，用来按照结构化数据生成供用户浏览的页面。页面模板T= &lt; C，L，S&gt;，包含了一些<strong>共同的页面内容C</strong>、<strong>严格定义的格式L</strong>和<strong>既定的页面数据模式S</strong>。</p>
<p>C包含了导航、版权声明、固定页面修饰等这些<strong>不变</strong>的内容;L包含了页面数据的格式规范；S则是能够从页面数据中观察到的模式。</p>
<p>•<strong>定义</strong> <strong>( Web数据抽取</strong>) web 数据抽取是指从<strong>页面</strong>中将用户感兴趣的数据利用<strong>程序</strong>自动抽取到<strong>本地</strong>的过程。给定页面集合W={wi}，它通过页面模板T生成，包含数据D={di}，即<strong>W={wi| wi =T(di))</strong>，Web数据抽取问题则可以定义为通过一定的技术手段，从W中逆向推导出T， 还原数据D。</p>
<p>•抽取方式 手工爬虫;机器学习爬虫  有标注数据集</p>
<p>•基本前提:包含数据的页面是由通过页面模板生成的。Web数据抽取被定义为其逆过程，即反向获取页面中数据的过程。</p>
<p>•解决该问题的关键在于逆向推导页面模板，即找到大量页面中的<strong>共同部分、格式约定和页面数据模式</strong>。</p>
<h2 id="二、Web数据抽取方法"><a href="#二、Web数据抽取方法" class="headerlink" title="二、Web数据抽取方法"></a>二、Web数据抽取方法</h2><p>Web数据抽取的目的是获得页面中的数据，需要借助一个或多个页面逆向推导出页面模板T</p>
<p>从<strong>自动化程度</strong>来区分，形成了</p>
<p>人工抽取:<strong>人工分析出页面模板</strong>，针对具体问题生成具体的包装器。一个问题一个解决方案</p>
<p>人工包装的解决途径容易采纳，在具体问题上可以取得满意的结果。<br>只适合小规模的、即时的数据抽取，并不适合大规模的、持续性的数据抽取</p>
<p>半自动抽取<br><strong>由计算机应用页面模板抽取数据生成具体包装器，而页面模板的分析仍然需要人工参与。</strong><br>半自动方式也能大大减少工作量。</p>
<p>自动抽取<br>仅仅需要很少的人工参与(例如检查结果进行校准) 或者完全不需要人工参与，因而更加适合大规模、系统化、持续性的Web数据抽取。</p>
<h2 id="三、Web-数据抽取评价标准"><a href="#三、Web-数据抽取评价标准" class="headerlink" title="三、Web 数据抽取评价标准"></a>三、Web 数据抽取评价标准</h2><p>对某个测试参考集,对应的相关数据集合为R。假设用某个检索策略进行处理后，得到一个结果集合A。令Ra表示R与A的交集。                 </p>
<p>召回率(Recall):也称为查全率，指抽取到的正确结果与要抽取页面的全部结果的比。即R=|Ra| / |R|</p>
<p>召回率是用来衡量一项抽取技术能否将所有要抽取出的数据都抽取出来的指标。</p>
<p>准确率(Precision):也称为查准率，指抽取到的正确结果与抽取到的全部结果的比。即P=|Ra| / |A|</p>
<p>准确率用来衡量一项抽取技术抽取出的数据是否符合预期指标。</p>
<p>•<em>F</em>值(<em>F</em>-measure)：召回率<em>R</em>和查准率<em>P</em>的加权调和平均值， </p>
<p>•F1 标准则综合了精度和查全率，将两者赋予同样的重要性来考虑。F1的计算由下面的公式决定</p>
<p>$F(i,j)=\frac{2×recall(i,j)×precision(i,j)}{recall(i,j)+precision(i,j)}$</p>
<p>•准确程度</p>
<p>•抽取自动化程度:这项标准用来衡量用户在抽取过程中的参与程度，分为手工、半自动和全自动三类。</p>
<p>•适应性:指在页面的内容和结构发生<strong>较小变化</strong>的情况下，该抽取方法或工具具有<strong>自适应能力</strong>，仍然能够<strong>继续正常工作</strong>。</p>
<p>•修正率:其含义是需要<strong>手工调整使得准确率和召回率达到100%的Web数据库数量</strong>。</p>
<h2 id="四、包装器"><a href="#四、包装器" class="headerlink" title="四、包装器"></a>四、包装器</h2><p>包装器是针对某一类特定的网页、计算机可以理解并执行的<strong>程序或抽取规则</strong>，任务就是负责<strong>将HTML格式的数据抽取并转化为结构化的格式</strong>。——模板T的表示形式。</p>
<p>在<strong>半自动化</strong>抽取系统中，包装器需要通过和用户的<strong>交互</strong>生成。</p>
<p>包装器的核心：抽取规则。</p>
<p><strong>基于分界符的规则</strong>将HTML文档看作<strong>字符流</strong>，给出数据项的<strong>起始和结束分界符</strong>，将其中的数据抽取出来。</p>
<p><strong>基于树路径的规则</strong>是将文档看作一个<strong>树结构</strong>。所抽取的数据存储在树节点中，因而可<strong>根据其路径来定位</strong>。</p>
<p>包装器与爬虫软件的不同：包装器是一个能够将数据从HTML网页中抽取出来，并且将他们还原为结构化的数据的软件程序。手工方法：爬虫</p>
<p>包装器归纳：使用机器学习的方法产生抽取规则，基于有监督学习的，用于从<strong>其他</strong>相同标记或相同网页模板抽取目标数据。</p>
<p>步骤：<br>网页清洗：有些网页结构不规范，例如前后标签不对称，没有结束标签符，不规范的网页结构容易在抽取过程中产生噪声。<br>网页标注：给网页中的某个位置打上特殊的标签表明这是要抽取的数据。<br>包装器空间的生成：对标注的数据生成XPath集合空间，对生成的集合进行归纳，形成若干子集。归纳的规则是在子集中的XPath能够覆盖多个标注的数据项，具有一定的泛化能力。<br>包装器评估：准确率和召回率。</p>
<p>自动抽取：通过挖掘多个数据记录中的重复模式来寻找这些模板，是无监督学习</p>
<p>步骤：<br>包装器训练：将一组网页通过聚类将相似的网页分成若干组。每组相似的网页将获得不同的包装器。<br>包装器应用：将需要抽取的网页与之前生成包装器的网页进行比较。在某个分类下则使用该分类下的包装器来获取网页中的信息。</p>
<h1 id="第6讲-包装器页面抽取方法"><a href="#第6讲-包装器页面抽取方法" class="headerlink" title="第6讲 包装器页面抽取方法"></a>第6讲 包装器页面抽取方法</h1><h2 id="一、数据抽取对象—网页的分类"><a href="#一、数据抽取对象—网页的分类" class="headerlink" title="一、数据抽取对象—网页的分类"></a>一、数据抽取对象—网页的分类</h2><p>按照页面内<strong>数据组织形式</strong>的不同，分为单记录页面、多记录页面 </p>
<p>•<strong>定义(单记录页面)</strong> 页面中只嵌入了唯一的记录</p>
<p>•详情页 A   <strong>Authority</strong></p>
<p>每页关注一个特定的对象，同时也含有其他相关和非相关信息 </p>
<p>•<strong>定义</strong>( <strong>多记录页面</strong>) 页面中嵌入了数量不等、由相同模板生成的记录，记录之间按照单列或多列布局整齐排列。</p>
<p>•同结构的记录在页面内重复排列出现</p>
<p>•列表页 H   <strong>Hub</strong></p>
<p>列表在页内的特定位置，每页多个数据记录</p>
<p>按照<strong>页面承载内容</strong>的不同，分为数据型页面、文档型页面</p>
<p>•<strong>定义</strong> <strong>(数据型页面)</strong> 页面中嵌入了一个或多个结构化的数据记录</p>
<p>•页面展示的是带有属性标签和数据值的信息，数据记录按照一定的格式规范和属性次序被载入在页面中，部分属性名称出现在页面中。</p>
<p>•<strong>定义</strong>( <strong>文档型页面</strong>)这类页面嵌入的是半结构化文档内容或文档标题。<br>这些信息通常存储于Web 应用的后台文档数据库中。在嵌入到页面中时仍要遵循一定的格式约束，按照一定的格式规范和一定的排列顺序嵌入页面。</p>
<h2 id="二、多记录数据型页面的抽取方法"><a href="#二、多记录数据型页面的抽取方法" class="headerlink" title="二、多记录数据型页面的抽取方法"></a>二、多记录数据型页面的抽取方法</h2><p>•多记录数据型页面是指页面由多条<strong>结构相同的数据记录</strong>组成。</p>
<p>抽取方法：</p>
<p>2.1 若干观察得到的规则</p>
<p>2.2 <strong>数据记录抽取</strong></p>
<p><strong>定义( 数据记录抽取)</strong>   在网页中，一个数据记录对应着现实世界的一个实体。如果把一个网页解析成DOM树结构，<strong>一个数据记录则对应着一棵子树或多棵相邻的子树</strong>。<br>数据记录抽取问题事实上就是<strong>如何找出包含数据记录的这些子树</strong>。</p>
<p>抽取的步骤：<br>2.2.1  确定数据区域<br>比较多个例子页面，发现可能包含数据记录的父节点<br>2.2.2计算数据记录的<strong>边界</strong><br>即对一系列节点进行聚类并且得到数据记录;<br>2.2.3<strong>去除噪声</strong>数据记录<br>即去除在结构或者内容上与正确数据有某种程度相似的噪声数据记录。</p>
<p>2.3 <strong>数据项抽取</strong>：数据项识别、匹配</p>
<p>比较DOM 树的算法：从两棵HTML树的根节点出发递归地进行深度遍历，对两棵DOM树中的节点对逐个进行比较，如果两个节点相同，则依次递归地比较其子节点。否则将该节点的路径记为不同子树的路径并且返回上一层。</p>
<p>语义块：HTML文档的一个片段，如果内容满足模式定义，则为语义块。不能划分的成为最小语义块。最小语义块是在包装器的生成维护中的最基本的抽取单位。</p>
<h2 id="三、单记录数据型页面抽取方法"><a href="#三、单记录数据型页面抽取方法" class="headerlink" title="三、单记录数据型页面抽取方法"></a>三、单记录数据型页面抽取方法</h2><p>•增量式抽取：连续性数据抽取，即从多个连续页面中抽取同结构的记录，以增量方式推导网页模板。<br>•所谓增量：就是网页像流水一样，一个一个进来</p>
<p>•<strong>定义(连续性数据抽取)</strong>: 假设存在页面队列P1, P2,.., Pn，它们包含了将被抽取的数据(假设一个页面仅包含一个数据记录)，试图找到一种<strong>抽取方法E</strong>，它能够完成如下目标:<br>不需要获得所有的页面，不需要对每一批页面重新进行页面模板的推导，在处理E(Pi+1, Pi+2, …, Pn)时，可以借助前i个页面E(P1, P2,.., Pi)，<strong>i&lt;n</strong>得到的模板。</p>
<p>部分树对齐算法：主要关心<strong>页面模式发生变化导致的页面模板变化</strong>时，如何调整和持续抽取数据的问题。对于<strong>页面模式可能发生的变化</strong>，有如下几种情况:<br>属性的增加：可插入情况<br>属性的减少<br>属性的更新：某些原本不能拥有多值，或者没有表现出多值的属性，在日后的更新中可能变化为多值属性。</p>
<p><img src="1685608353660.png" alt="1685608353660"><br>算法：首先是选择一棵树作为对齐开始的种子。<br>接下来，将剩下的树(T2和T3) 逐个与T1进行对齐。<br>首先处理T2。不幸的是，T2中的节点n,c,k,g都不能找到唯一插入位置，属于歧义情况。于是，这时算法将跳过T2。<br>接下来将T1与T3进行对齐。可以看到，T3中的b,c和h,k 都属于可插入情况，因此能将它们插入到T1中，形成一个页面模式。<br>这时用新生成页面模式与跳过的T2 继续对齐。这次，n和g属于可插入情况。再次生成新的页面模式。<br>如果还剩下跳过的页面，继续进行对齐，直到不能生成新的页面模式为止。<br>在部分树对齐之后，将产生一个页面模式对数据进行抽取。</p>
<p>这种方法本身所具备的循环迭代特性，可用于改进适合增量式数据抽取的基本算法。<br>问题尚需要解决:部分树对齐能够处理属性的增加和减少，然而对于属性的变更缺乏考虑。在属性的多值方面，部分树对齐算法并没有做相应处理。</p>
<h2 id="四、单文档型页面抽取方法"><a href="#四、单文档型页面抽取方法" class="headerlink" title="四、单文档型页面抽取方法"></a>四、单文档型页面抽取方法</h2><p>浏览器阅读模式<br>浏览器会对页面进行智能判断，如果需要启用阅读模式，则在地址栏给出提示图标，用户点击该图标即可开启阅读模式。<br>阅读模式化繁为简，隐藏无关元素，让阅读的过程更加专注，而且会在用户阅读过程中自动加载下面的章节，无需用户手动翻页。</p>
<p>4.1结合视觉信息的抽取方法</p>
<p>正文数据块识别：正文的文本块大小是页面中最大的文本数据块，利用此特性，可以帮助确定正文的区域。通过<strong>比较parentElement所包含的文本块大小(包含孩子节点中的文本)与seedElement 所包含的文本块大小</strong>，如果它们的<strong>比值超过阈值α</strong>，则文本区域块继续向上扩大，直到比值小于α</p>
<p>•视觉信息的抽取方法，将无法很好地处理如下情况:<br>(1)<strong>短正文</strong>的抽取：<br>仅依靠数据块大小数据信息，在处理<strong>正文长度较短的数据</strong>时，可能无法正确识别出正文区域。<br>(2)包含大量评论的页面:<br>•由于<strong>评论内容较多</strong>，其所占用的文本块大小可能超过正文所占用的文本块大小，由此导致将评论区域被误认为是正文区域。</p>
<p>4.2路径规则</p>
<p><strong>抽取路径学习</strong><br>除了使用种子点seedElement找出正文内容外，在内容返回前，还将正文的<strong>抽取路径</strong>保存到数据库中。充分挖掘同一数据源不同页面之间的<strong>结构关联</strong>，针对抽取规则进行学习<br>针对一个新的文档型页面，先采用<strong>基于视觉信息</strong>的抽取方法进行数据抽取，并获取<strong>抽取路径</strong>。<br>将<strong>抽取路径与数据库中的抽取路径记录</strong>进行比对，若与数据库中该数据源最多的抽取路径一致，则认为此次抽取数据块即为所需要的正文内容。若两者不一致，则需要进行选择。<br><strong>选择采用基于视觉的数据抽取方法</strong>，意味着可能是出现了网站模板的变化<br><strong>选择采用基于抽取规则的方法</strong>。意味着可能是遇到了短正文页面或者是包含大量评论的页面。</p>
<p>4.3改进的自适应数据抽取方法</p>
<p>•实质：基于视觉的方法+基于规则的方法 自动选择<br>•一种基于贝叶斯最优决策的方法<br>•定义(决策结果集合DecisionSet) :冲突解决的决策结果集合包含两个元素:<br>•Vision: 表示最终将采用基于视觉的方法来抽取数据;<br>•Rule: 表示最终将采用基于抽取路径的方法来抽取数据。<br>所有特征规则需要进行综合考虑，才能得到该数据块是否是正文文本的可能性。</p>
<p>定义 (特征规则空间FeatureSpace) :影响决策结果的一些特征规则。</p>
<p>在方法选择时，如果最终采用的是vision则说明可能是网页的模板发生了变化，因此需要进行抽取规则的更新，为自适应抽取积累抽取经验。最直观的<strong>更新方法</strong>就是<strong>在完全匹配的抽取规则命中次数上进行累加</strong>。这种方法能够进行抽取经验的积累。</p>
<p>问题：网页模板发生变动时，如果原来的抽取规则积累的命中次数较多，则需要经过较多次的冲突解决后，才能够达到一个<strong>重新平衡的状态</strong>。<br>解决：在<strong>增加</strong>新规则命中次数时，<strong>同时减少</strong>其它最大命中次数的抽取规则的命中次数。</p>
<h1 id="第7讲-web数据存储与应用"><a href="#第7讲-web数据存储与应用" class="headerlink" title="第7讲 web数据存储与应用"></a>第7讲 web数据存储与应用</h1><h2 id="一、-爬虫数据存储"><a href="#一、-爬虫数据存储" class="headerlink" title="一、 爬虫数据存储"></a>一、 爬虫数据存储</h2><h3 id="1-1-结构化文件"><a href="#1-1-结构化文件" class="headerlink" title="1.1  结构化文件"></a>1.1  结构化文件</h3><p>CSV(comma-separated values)是以逗号间隔的文本文件。优点:被Excel和很多的应用程序支持;用来做数据存储容量小;很多数据集采用格式</p>
<p>JSON （JavaScript 对象表示法JavaScript Object Notation）是轻量级的文本数据交换格式。JSON 是存储和交换文本信息的语法。<br>类似 XML。 比 XML 更小、更快，更易解析。在JSON中，有两种结构：对象和数组。JSON最常用的格式是对象的<strong>键值对</strong>。Json在数据交换中起到了一个载体的作用，承载相互传递的数据。</p>
<p>XML文件 Extensible Markup Language 可扩展标记语言<br>数据重用：用来存储，携带，交换数据的，不是用来显示数据的<br>半结构化集成数据</p>
<p>pickle,    python二进制序列化格式。用于序列化和反序列化Python对象结构<br>pickle协议和JSON的区别 ：</p>
<ol>
<li>JSON是一种文本序列化格式，而pickle是二进制序列化格式;</li>
<li>JSON是人类可读的，而pickle则不是;</li>
<li>JSON是可互操作的，并且在Python生态系统之外广泛使用，而pickle是特定于Python的;</li>
</ol>
<h3 id="1-2-数据库"><a href="#1-2-数据库" class="headerlink" title="1.2 数据库"></a>1.2 数据库</h3><p>轻型关系型数据库：Sqlite<br>SQLite是一种嵌入式数据库，SQLite本身是C写的，体积很小<br>它的数据库就是一个文件。Python就内置了SQLite3。<br>关系型数据库：1. 复杂查询 2. 事务支持<br>非关系型数据库的实质：传统关系型数据库的功能阉割版本，通过减少用不到或很少用的功能，来大幅度提高产品性能。</p>
<h2 id="二、-结构化数据处理"><a href="#二、-结构化数据处理" class="headerlink" title="二、 结构化数据处理"></a>二、 结构化数据处理</h2><h3 id="2-1-数据清洗"><a href="#2-1-数据清洗" class="headerlink" title="2.1  数据清洗"></a>2.1  数据清洗</h3><p>数据清洗(Data cleaning)– 对数据进行重新审查和校验的过程，目的：删除重复信息、纠正存在的错误，并提供数据一致性。</p>
<p>缺失值处理：直接删除（常用）； 或者进行插值处理:可以用平均值、最大值、最小值或更为复杂的概率估计代替缺失的值<br>异常值/错误值处理：<br>识别：<br>统计分析的方法：偏差分析、识别不遵守分布或回归方程的值;<br>简单规则库：常识性规则、业务特定规则等;<br>使用不同属性间的约束;<br>外部的数据<br>处理：直接删除； 极值处理到一个正常区间（推荐）</p>
<h3 id="2-2-结构化数据应用"><a href="#2-2-结构化数据应用" class="headerlink" title="2.2  结构化数据应用"></a>2.2  结构化数据应用</h3><p>应用：<br>数据集成，展示；<br>用机器学习处理数据，展示结果。<br>无标注数据：聚类；有标注数据：回归；分类。定量输出称为回归，定性输出称为分类</p>
<h3 id="2-3-特征工程"><a href="#2-3-特征工程" class="headerlink" title="2.3  特征工程"></a>2.3  特征工程</h3><p><strong>好的特征集</strong>对于一个机器学习模型的预测效果的边际贡献远远大于<strong>好的模型</strong>的贡献：不同的机器学习模型（SVM或RandomForest等）对于有效的特征集的训练效果，差别不会特别大；但是，如果特征集很烂，无论什么模型，都很难有较好的训练效果。“数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”。</p>
<p>•特征：在观测现象中的一种独立、可测量的属性。</p>
<p>•特征工程就是一个把原始数据转变成特征的过程<br>特征工程：利用数据领域的相关知识来创建能够使机器学习算法达到最佳性能的特征的过程。<br>目的: 获取更好的训练数据</p>
<p>主成分分析法：使用最广泛的数据降维算法，顾名思义就是基于某个具体任务，将数据的主要成分提取出来。是无监督学习<br>目标：将互相相关的特征，通过线性组合，使得数据变换到新的空间，可能最大程度保持原来的信息，并且特征之间互相不相关</p>
<p>PCA的目标:  要保持最大重构性或者<strong>变换后数据集的最大可分性</strong><br>PCA的问题其实是一个<strong>基的变换</strong>，使得<strong>变换后的数据有着最大的方差</strong><br>方差：数据离散程度的度量。一个模型的方差很大，那就说明模型不稳定了。<br>对于用于机器学习的数据（主要是训练数据），方差大才有意义，不然输入的数据都是同一个点，那方差就为0了，这样输入的多个数据就等同于一个数据</p>
<p>方差大的方向是信号的方向，方差小的方向是噪声的方向<br>PCA就是对原始的空间中顺序地找一组相互正交的坐标轴，(正交基   相互垂直， 内积为0）<br>第一个轴是使得方差最大的，第二个轴是在与第一个轴正交的平面中使得方差最大的，第三个轴是在与第1、2个轴正交的平面中方差最大的…</p>
<h3 id="2-4-机器学习的数据预处理"><a href="#2-4-机器学习的数据预处理" class="headerlink" title="2.4 机器学习的数据预处理"></a>2.4 机器学习的数据预处理</h3><p>归一化  Normalization 把数据特征转换为相同尺度的方法<br>尺度(Scale）：取值范围</p>
<p>标准化  Standardization:也叫(Z-Score Normalization)Z值归一化，来源于统计上的标准分数，将每一个维特征都调整为均值为0，方差为1</p>
<p>白化  Whitening:<strong>消除不同特征之间的相关性</strong>，降低输入数据特征之间的冗余性，输入数据经过白化处理后，特征之间相关性较低，并且所有特征具有相同的方差，白化的一个主要实现方式是使用PCA方法。</p>
<h3 id="2-5-连续值与离散值"><a href="#2-5-连续值与离散值" class="headerlink" title="2.5 连续值与离散值"></a>2.5 连续值与离散值</h3><p>连续值：缺失数据处理/归一化/离散化<br>离散值处理方式：<br>One-Hot Encoding(若特征种类很多，高稀疏化)<br>Hash Encoding(低稀疏，高压缩)</p>
<h2 id="三、-非结构化数据处理"><a href="#三、-非结构化数据处理" class="headerlink" title="三、 非结构化数据处理"></a>三、 非结构化数据处理</h2><p>文本数据—-SQL查询/NLP文本特征<br>文本特征 —-IR（信息检索）：IR模型，倒排表，搜索引擎<br>                —-文本处理：文本分类，文本聚类，情感分析<br>怎么获得特征？人类知识/机器学习特征—深度学习</p>
<p>图像数据 —-图像特征<br>图像特征 —-图像识别，图像相似度计算，图像检索</p>
<p>文本+图片特征 —-跨模态检索/文本生成图片</p>
<h1 id="第8讲-文本表示"><a href="#第8讲-文本表示" class="headerlink" title="第8讲  文本表示"></a>第8讲  文本表示</h1><h2 id="1、文本预处理"><a href="#1、文本预处理" class="headerlink" title="1、文本预处理"></a>1、文本预处理</h2><p>语言是具有组合性的。人类将这些文档拆解为各个组件或单词，然后从左至右地阅读一个个的单词。意义是具有组合性的，单词也是具有组合性的，而单词流构成了意义。<br>预处理：将文档拆解成单词以便计算机程序能够解释。</p>
<p>预处理的步骤<br><strong>1.文档解析</strong>  移除不需要的格式（例如：HTML 标签）<br>文档包含哪些格式？包含的语言？使用何种编码方式？</p>
<p><strong>2.句子分割</strong>：将文档拆分成句子</p>
<p><strong>3.分词</strong>：将句子拆分成单词  利用标点符号进行分割<br><strong>词条化</strong>：将给定的字符序列拆分成一系列子序列的过程，其中每一个子序列称之为一个“词条”Token。英文本身就已经通过空白符进行了分词。</p>
<p><strong>4.词规范化</strong>：将单词转换为规范形式</p>
<p>•<strong>归一化</strong>：<br>–词条的不同表示方式“归一化”成一致的形式 。实现方式：建立同义词词表<br>•car = automobile,  color = colour ，USA=U.S.A</p>
<p>•词干还原 (Stemming)，通常指去除单词两端词缀的启发式过程。<br>•e.g., automate(s), automatic, automation à automat<br>Porter算法:英文处理中最常用的<strong>词干还原</strong>算法</p>
<p>词形归并(Lemmatization)：利用词汇表和词形分析来减少屈折变化的形式，将其转变为<strong>基本形式</strong>。<br>am, are, is  -&gt;be          car, cars, car’s, cars’ -&gt;car</p>
<p><strong>5.去停用词</strong>：删除不需要的词</p>
<p>停用词  Stop Words<br>•IR 信息检索<br>•词的应用太广泛，太常见，对这样的词搜索引擎无法保证能够给出真正相关的搜索结果<br>•消除方法：查表法<br>•停用词表:<br><strong>词性</strong>   冠词，介词，代词eg.a, an, the, to, and, be …<br><strong>利用词频</strong></p>
<p>词袋模型(Bag-of-words model：BOW)<br>假定对于给定文本，<strong>忽略单词出现的顺序和语法</strong>等因素，将其视为词汇的简单集合，文档中每个单词的出现属于独立关系，不依赖于其它单词。</p>
<p>•预处理的结果<br>•文档-&gt;词条集合<br> •词袋（bag-of-words, BOW）例如：搜索引擎<br>•文档-&gt;词条序列<br>•用于单词序列很重要的情况 例如：语言模型</p>
<p>开源 NLP库/工具:<br>NLTK :Natural Language Toolkit<br>用于诸如词条化、词形还原、词干化、解析、POS标注等任务。<br>该库具有几乎所有NLP任务的工具。<br>开源的项目。在NLP领域中，最常使用的一个Python库，不支持中文</p>
<p>Spacy    NLTK的主要竞争对手。这两个库可用于相同的任务</p>
<p>Stanford NLP 支持中文、英文、阿拉伯语、法语、德语、西班牙语等多种语言 提供了一系列自然语言分析工具</p>
<h2 id="2、文本向量化"><a href="#2、文本向量化" class="headerlink" title="2、文本向量化"></a>2、文本向量化</h2><p>文本数据往往是非结构化杂乱无章的文本，而机器学习算法处理的数据往往是固定长度的输入和输出。因而机器学习并不能直接处理原始的文本数据。必须<strong>把文本数据转换成数字</strong>，比如向量。所谓文档信息的向量化，就是将<strong>文本信息数值化</strong>，从而便于进行建模分析</p>
<p>文本表示：把字词处理成向量或矩阵，以便计算机能进行处理。文本表示是自然语言处理的开始环节。</p>
<p>One-hot Representation：向量维度和字典维度一致，第 i 维上的数字1代表 ID 为 i 的词语在文本里出现</p>
<p>词频-逆文档频率（TF-IDF）</p>
<p>•词项频率tf (Term frequency )：词项<em>t</em>在文档<em>d</em>中出现的次数，记为 $ tf<em>{t,d}$ ，向量第 i 维上的数字代表 ID 为 i 的词语在文本里的$ tf</em>{t,d}$ </p>
<p>一种替代原始tf的方法: 对数词频</p>
<p><img src="1685626633962.png" alt="1685626633962"></p>
<p>•逆文档频 idf (inverse document frequency)</p>
<p>文档频率 (Document frequency，df):出现词项的文档数目<br>$df<em>{t}$:文档集合中包含t的文档数目,与词项t包含的信息量成反比,dft  &lt;= N  (N是文档的总数)<br>idf (inverse document frequency)逆文档频<br>$idf</em>{t}$ = $log<em>{10}(N/df_t)$, 是反映词项t的信息量的一个指标，$log</em>{10}(N/df_t)$ 代替N/dft 来抑制idf的作用</p>
<p>TF-IDF是一种<strong>统计方法</strong>，用以评估字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性<strong>随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降</strong>。词项t的tf-idf 由它的tf和idf组合而成：$w<em>{t,d}=(1+log</em>{10}tf<em>{t,d}) × log</em>{10}(N/df_t)$</p>
<p>Gensim:一个简单高效的自然语言处理Python库，开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达</p>
<p>离散表示文本：One-hot Representation（TF-IDF）、基于词袋（BOW）</p>
<p>•缺点:<br>不考虑词与词之间的顺序<br>它假设词与词相互独立（在大多数情况下，词与词是相互有关联的）<br>它得到的特征是离散稀疏的（维度的灾难）</p>
<p>分布式表示：也叫做词嵌入（word embedding），经典模型是word2vec。表示的一种低维实数向量。维度以 50 维和 100 维比较常见。每一维可以看成词的语义或者主题信息<br>•维度压缩<br>•解决语义鸿沟</p>
<p>实现方式：主题模型、文档哈希、语言模型:N-GRAM,<strong>Word2vec</strong>,<strong>Doc2Vec</strong></p>
<p>LDA:Latent Dirichlet Allocation. 隐含狄利克雷分布,LDA 采用词袋模型,常用来文本分类。无监督学习算法</p>
<p><img src="1685629276033.png" alt="1685629276033"></p>
<p>特征：在语料库上进行训练，指定主题数目 K ，文档表示为K维向量，LDA <strong>超级耗时</strong></p>
<h2 id="3、文档哈希"><a href="#3、文档哈希" class="headerlink" title="3、文档哈希"></a>3、文档哈希</h2><p>Hash算法可以认为是一种思想：压缩映射。把任意长度的输入（文本、图像）通过Hash算法变换成固定长度的输出（f-bit  的0,1、Hamming Distance ）</p>
<p>密码学哈希：将任意输入数据映射成一个固定长度的数字序列<br>MD5和SHA-1等传统密码学哈希算法只负责将原始内容尽量均匀随机地映射为一个签名值，原理上相当于伪随机数产生算法<br>两个签名，如果相等，说明原始内容在一定概率下是相等的<br>如果不相等，除了说明原始内容不相等外，不再提供任何信息</p>
<p>密码学哈希对输入数据的变化敏感，任何1比特输入的改变将会完全改变最终输出的哈希值。因此，传统密码学哈希算法只适用于比特流级的认证，如文件认证。</p>
<p>•图像哈希算法<br>将输入图像映射成一串短小的数字序列，该数字序列通常称为输入图像的哈希。</p>
<p>•在实际应用中，用图像哈希来代表图像本身，有效降低了图像存储代价和计算复杂度，实现图像数据的高效处理。<br>•对于图像而言，常常会经历诸如JPEG压缩、亮度调整等数字操作，这些数字操作改变了图像文件的比特数据，但图像视觉内容并没有发生改变。如果直接将传统密码学哈希算法应用于这些图像，那么视觉相似图像将被误判为篡改图像。因此，图像哈希算法必须是基于图像视觉内容来计算哈希序列，与具体的图像文件数据无关。</p>
<p>•图像哈希研究大致可以分为两大流派：</p>
<p><img src="1685629903844.png" alt="1685629903844"></p>
<p>–信号处理流派<br>•主要关注鲁棒图像特征提取，在此基础上再研究相应的特征压缩技术，主要的目标应用是图像认证、图像质量评价、拷贝检测等。</p>
<p>–机器学习流派<br>•不关注图像特征提取，利用机器学习理论从公开的图像特征数据集中学习哈希码，和数据集相关，主要的目标应用是图像检索。</p>
<p>文档hash</p>
<p>目标：hash code的相似程度要能直接反映输入内容的相似程度。</p>
<p>应用：网上到处都是相同的内容，大多数情况是近似重复 Near-Duplication。E.g., 两份文本仅仅是日期不同<br>通过编辑距离计算语法上的相似性<br>通过一定的阈值来检测近似复制，E.g., Similarity &gt; 80% =&gt; 文档近似复制<br>通过阈值来检测是不可传递的 ，AB近似，BC近似不能推断AC近似 </p>
<p>•文档哈希判断重复的思路：为每一个web文档通过hash的方式生成一个指纹（fingerprint）。将高维的特征向量映射成一个f-bit的指纹(fingerprint)，通过比较两篇文章的f-bit指纹的Hamming Distance来确定文章是否重复或者高度近似。</p>
<h3 id="3-1-shingle算法"><a href="#3-1-shingle算法" class="headerlink" title="3.1 shingle算法"></a>3.1 shingle算法</h3><p>•Shingle（搭叠）算法的核心思想是将文件相似性问题转换为集合的相似性问题</p>
<p>•<strong>Shingles</strong> (<em>N</em>元词 <em>N</em>-Grams)</p>
<p>–给定正整数<em>k</em>及文档<em>d</em>的一个词项序列，可以定义文档<em>d</em>的<em>k</em>-shingle为<em>d</em>中所有<em>k</em>个连续词项构成的序列。</p>
<p>a rose is a rose is a rose → 4-Grams</p>
<p> a_rose_is_a</p>
<p> rose_is_a_rose</p>
<p> is_a_rose_is </p>
<p> a_rose_is_a …</p>
<p>•直观上看，如果两个文档的shingle集合几乎一样，那么它们就满足近似重复。</p>
<p><img src="1685630766355.png" alt="1685630766355"></p>
<p>•使用shingle的hash值代表shingle进行相似性计算，能够节省一定计算开销</p>
<p><img src="1685630783536.png" alt="1685630783536"></p>
<p>为每篇文档生成一个素描向量“sketch vector”(大小约为 ~200)。相同向量个数 ≥ <em>t</em> (一般80%) 判定为近似near duplicates</p>
<h3 id="3-2-局部敏感哈希-LSH"><a href="#3-2-局部敏感哈希-LSH" class="headerlink" title="3.2 局部敏感哈希   LSH"></a>3.2 局部敏感哈希   LSH</h3><p>•locality sensitive hash 一种常见的用于处理高维向量的索引办法。LSH是指面对海量高维数据时，一般的算法无法快速降维查询相似度高的数据子集，利用特定的hash算法，将高维数据映射到低维空间，以较高概率快速寻找相似度高的数据子集。</p>
<p>LSH是一种概率方法<br>采用<strong>过滤一验证</strong>的框架(Filter一and一Refine framework)。在过滤阶段，LSH利用哈希技术把非相似、不可能成为结果的数据对象过滤掉，过滤之后的数据对象作为候选集(CandidateSet)，使得相似的数据对象以很高的概率留在候选集合中，进而在候选集合上进行实际的距离或者相似性度量计算。LSH多被用于文本、多媒体（图像、音频）的相似性判断。</p>
<p>3.2.1 MinHash<br>MinHash  是LSH的一种，可以用来快速估算两个集合的相似度。用于在搜索引擎中检测重复网页。它也可以应用于大规模聚类问题。</p>
<p>h(x): 把x映射成一个整数的哈希函数。<br>有一个假设，h(x)是一个良好的哈希函数，它具有很好的均匀性，能够把不同元素映射成不同的整数<br>hmin(S)：集合S中的元素经过h(x)哈希后，具有最小哈希值的元素。<br>那么对集合A、B，hmin(A) = hmin(B)成立的条件是<strong>A ∪ B 中具有最小哈希值的元素也在 A ∩ B中</strong>。<br>结论：集合A和B的相似度为集合A、B经过hash后<strong>最小哈希值相等的概率</strong>。</p>
<p>•MinHash计算两个集合的相似度</p>
<p>–第一种：使用多个hash函数<br>•可以选择一定数量的hash函数，比如K个。用这K个hash函数分别对集合A、B求哈希值，定义 Min(S)为集合S中具有最小哈希值的一个元素，对每个集合都得到K个最小值。比如Min(A)k={a1,a2,…,ak}，Min(B)k={b1,b2,…,bk}。</p>
<p>–集合A、B的相似度为Min(A)k和Min(B)k中相同元素个数与总的元素个数的比例。|Min(A)k ∩ Min(B)k| / |Min(A)k ∪ Min(B)k|<br>•计算复杂度高</p>
<p>–第二种：使用单个hash函数<br>•定义hmink(S)为集合S中具有最小哈希值的K个元素。只需要对每个集合求一次哈希，然后取最小的K个元素。<br>•两个集合A、B的相似度，就是集合A中<strong>最小的K个元素</strong>与集合B中最小的K个元素的交集个数与并集个数的比例。</p>
<p>3.2.2 Simhash算法</p>
<p>simhash是LSH的一种，算法分为5个步骤：分词、hash、加权、合并、降维</p>
<p><img src="1685671595377.png" alt="1685671595377"></p>
<ol>
<li><p>分词<br>给定一段语句，进行分词，得到有效的特征向量，然后为每一个特征向量设置权重<br>权重代表这个单词在整条语句中的重要程度，数字越大代表越重要。<br>权重可以是这个词出现的次数。<br>例如给定一段语句：“CSDN博客结构之法算法之道的作者July”，<br>1-5等5个级别的权重<br>CSDN(4) 博客(5) 结构(3) 之(1) 法(2) 算法(3) 之(1) 道(2) 的(1) 作者(5) July(5)</p>
</li>
<li><p>hash<br>选择simhash的位数<br>综合考虑存储成本以及数据集的大小，比如说32位<br>通过hash函数计算各个特征向量的hash值<br>hash值为二进制数01组成的n-bit签名。<br>比如“CSDN”的hash值Hash(CSDN)为100101，“博客”的hash值Hash(博客)为“101011”。</p>
</li>
<li><p>加权</p>
<p>•在hash值的基础上，给所有特征向量进行加权，即W = Hash * weight，遇到1则hash值和权值正相乘，遇到0则hash值和权值负相乘。</p>
</li>
</ol>
<p>例如给“CSDN”的hash值“100101”，加权得到：W(CSDN) = 100101 4 = 4 -4 -4 4 -4 4，给“博客”的hash值“101011”加权得到：W(博客)=101011 5 = 5 -5 5 -5 5 5</p>
<ol>
<li><p>合并</p>
<p>•将上述各个特征向量的加权结果累加，变成只有一个序列串。</p>
<p>例如“CSDN”的“4 -4 -4 4 -4 4”和“博客”的“5 -5 5 -5 5 5”进行累加，得到“4+5 -4+-5 -4+5 4+-5 -4+5 4+5”，得到“9 -9 1 -1 1”。</p>
</li>
<li><p>降维</p>
<p>•对于n-bit签名的累加结果，如果大于0则置1，否则置0，从而得到该语句的simhash值</p>
<p>例如把上面计算出来的“9 -9 1 -1 1 9”降维得到的01串为：“1 0 1 0 1 1”，从而形成它们的simhash签名。</p>
</li>
</ol>
<p>•每篇文档得到SimHash签名值后，接着计算两个签名的海明距离即可。根据经验值，对64位的 SimHash值，海明距离在3以内的可认为相似度比较高</p>
<p>•64 位的二进制simhash签名，顺序比较$2^{64}$次。可以把 64 位的二进制simhash签名均分成4块，每块16位。</p>
<p>•何在海量的样本库中查询与其海明距离在3以内的记录呢？把<strong>分成的4 块中的每一个块分别作为前16位来进行查找</strong>，建倒排索引</p>
<p>倒排索引：普通的搜索算法是从文档里搜索关键字，而倒排索引是事先知道了每个关键词出现的文档，从关键词搜索文档。</p>
<p><img src="1685672277094.png" alt="1685672277094"></p>
<p>•如果样本库中存有$2^{34}$（差不多10亿）的simhash签名，则每个table返回$2^{34-16}$=262144个候选结果，大大减少了海明距离的计算成本。四个块返回的总结果数为 4* 262144 （大概 100 万）。这样，原本需要比较10亿次，经过索引后，大概只需要处理100万次。</p>
<p>•<strong>百度的去重算法</strong>：直接找出此文章的最长的n句话，做一遍hash签名。n一般取3。 </p>
<h1 id="第9讲-词嵌入word-embedding"><a href="#第9讲-词嵌入word-embedding" class="headerlink" title="第9讲 词嵌入word embedding"></a>第9讲 词嵌入word embedding</h1><p>•嵌入（embedding） ：一种可用于将离散变量表示成连续向量的方法。深度学习一大显著成功的用途是嵌入</p>
<p>•表征学习：在机器学习中，<strong>表征学习</strong>是学习一个特征的技术的集合。将原始数据转换成为能够被机器学习来有效开发的一种形式。（向量）</p>
<h2 id="1、词嵌入"><a href="#1、词嵌入" class="headerlink" title="1、词嵌入"></a>1、词嵌入</h2><p>词嵌入(Word Embedding)：一种将文本中的词转换成数字向量的方法，属于文本向量化处理的范畴。</p>
<p>词嵌入的表示</p>
<p>•<strong>One-hot Representation</strong>：<strong>每个词表示为一个很长的向量</strong>。这个向量的<strong>维度是词表大小</strong>，其中绝大多数元素为 0，<strong>只有一个维度的值为 1，这个维度就代表了当前的词</strong>。稀疏，高维，没有语义关联</p>
<p>•<strong>分布式表示</strong></p>
<p>分布式表示就是将词转化成向量，将一个单词转换成固定长度的向量表示，降维。其中向量之间的相似性与词之间的语义相似性相关。</p>
<p>根据建模不同，大体可以分为三类：</p>
<p>1.基于矩阵分布表示：LDA</p>
<p>2.基于聚类的分布表示</p>
<p>3.基于神经网络的分布表示：著名的Word2Vec</p>
<p>主题模型-LDA：将一个单词转换成固定长度的向量表示</p>
<p>•降维, 有语义信息</p>
<p><img src="1685673467944.png" alt="1685673467944"></p>
<p>词嵌入（word embedding）是一种词的类型表示，是NLP中语言模型与表征学习技术的统称。<br>把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量（不是one-hot   稀疏向量）。<br>具有相似意义的词具有相似的表示，是将词汇映射到实数向量的方法总称</p>
<h2 id="2、语言模型"><a href="#2、语言模型" class="headerlink" title="2、语言模型"></a>2、语言模型</h2><p>•语言模型 language model ：根据语言客观事实而进行的语言抽象数学建模，是一种对应关系</p>
<p>统计语言模型LMs：Statistical Language Models</p>
<p>自然语言处理的基础，为上下文相关的特性建立数学模型</p>
<p>S 可以表示某一个由一连串特定顺序排练的词而组成的一个有意义的句子。<br>S ：一连串特定顺序排列的词ω1，ω2，…，ωn<br>S在文本中出现的可能性，即S的概率P(S)，P(S)=P(ω1，ω2，…，ωn)<br>利用条件概率的公式：S 的概率 P(S)等于每一个词出现的概率相乘<br>P(S) =P(ω1)•P(ω2|ω1)•P(ω3|ω1,ω2)•••P(ωn|ω1，ω2，…，ωn-1)</p>
<p>应用：<strong>文本生成、机器翻译</strong><br>语言模型就是用来判断一个句子的合理性的置信度 </p>
<p>•<strong>拼写纠错</strong>：</p>
<p>•P(about fifteen <strong>minutes</strong> from) &gt;P(about fifteen <strong>minuets</strong> from)</p>
<p>•<strong>语音识别</strong>：</p>
<p>•P(I saw a van) &gt; P(eyes awe of an)</p>
<p>•<strong>音字转换</strong>：</p>
<p>•P(你现在干什么|nixianzaiganshenme) &gt;P(你西安在干什么|<em>nixianzaiganshenme</em>) </p>
<p>•分词</p>
<p>S可以有几种分词方法，假定有以下三种：</p>
<p>•A1, A2, A3, …, Ak/B1, B2, B3, …, Bm /C1, C2, C3, …, Cn </p>
<p>计算P(A1, A2, A3, …, Ak)，P(B1, B2, B3, …, Bm )和P(C1, C2, C3, …, Cn )，找到概率最大的情况</p>
<p>统计语言模型的工程诀窍</p>
<p>S 的概率 P(S)等于每一个词出现的概率相乘<br>P(S) =P(ω1)•P(ω2|ω1)•P(ω3|ω1,ω2)•••P(ωn|ω1，ω2，…，ωn-1)</p>
<p>P (w1) 表示第一个词w1 出现的概率；根据大数定理，只要统计量足够，相对频度等于频率，P (w2|w1) 是在已知第一个词的前提下，第二个词出现的概率；</p>
<p>条件概率 P(wi|wi-1) = P(wi-1,wi)/ P (wi-1)。<br>联合概率 P(wi-1,wi）：这对词在统计的文本中出现了多少次/文档数<br>边缘概率P (wi-1) ：wi-1 本身出现了多少次/文档数<br>以此类推。词wn 的出现概率取决于它前面所有词。在统计语言模型中，该条件概率通过极大似然估计计算</p>
<p>存在两个问题：<br>1.要求的概率太多了，因为ω1，ω2，…，ωn的组合实在太多了（词表数的n次方）———-n-gram语言模型<br>2.由于语料的数量有限，数据中可能不存在ω1，ω2，…，ωn的组合，导致求得的条件概率为0。</p>
<p>n-gram语言模型</p>
<p>•N-1阶马尔可夫假设:假定文本中的每个词ωi和前面的N-1个词有关，而与更前面的词无关，对应的语言模型称为N元模型(N-Gram Model)。</p>
<p>如何选择依赖词的个数，即n。<br>更大的n：对下一个词出现的约束信息更多，具有更大的<strong>辨别力</strong>；理论上，n越大越好<br>更小的n：在训练语料库中出现的次数更多，具有更可靠的统计信息，具有更高的可靠性。<br>经验上，trigram（前面出现的两个词）用的最多，尽管如此，原则上，<strong>能用bigram（前面出现的一个词）解决，绝不使用trigram</strong>。</p>
<p>四元甚至更高阶的模型是否能覆盖所有的语言现象呢？答案显然是否定的。为什么N取值一般都这么小呢？N元模型的空间复杂度，时间复杂度几乎是N的指数函数。</p>
<p>LM模型中所有的条件概率，我们称之为模型的参数。通过对语料的统计，得到这些参数的过程称作模型的训练。通过极大似然估计MLE 计算，保证有足够的观测值（在数理统计中，大数定理( Law of Large Numbers)）。<br>零概率问题：大规模数据统计方法与有限的训练语料之间必然产生数据稀疏问题，导致零概率问题。</p>
<p>统计样本不足时的概率估计问题解决方法：<br>1.增加数据量<br>2.平滑技术<br>基本思想：“降低已出现n-gram条件概率分布，以使未出现的n-gram条件概率分布非零”，且经数据平滑后一定保证概率和为1，<br>平滑方法：<br>Add-one（Laplace） Smoothing加一平滑法，又称拉普拉斯平滑，<br>保证每个n-gram在训练语料中至少出现1次，V是所有bigram的个数<br>古德-图灵估计（Good-Turing Estimate）：是把非零的n元语法的概率降低匀给一些低概率n元语法，以修改最大似然估计与真实概率之间的偏离。是实用比较多的平滑算法</p>
<p>更多语言模型：考虑语法、语义等语言学作用</p>
<p>Class-based ngram model，基于词类建立语言模型，以缓解数据稀疏问题，且可以方便融合部分语法信息<br>topic-based ngram model，将训练集按主题划分成多个子集，并对每个子集分别建立N-gram语言模型，以解决语言模型的主题自适应问题<br>cache-based ngram model，<br>利用cache缓存前一时刻的信息，以用于计算当前时刻概率，以解决语言模型动态自适应问题<br>指数语言模型：最大熵模型   MaxEnt、最大熵马尔科夫模型   MEMM、条件随机域模型  CRF （Conditional Random Fields）<br>融入多种知识源，刻画语言序列特点，较好的用于解决序列标注问题<br>skipping ngram model，刻画距离约束关系</p>
<h2 id="3、用神经网络训练语言模型"><a href="#3、用神经网络训练语言模型" class="headerlink" title="3、用神经网络训练语言模型"></a>3、用神经网络训练语言模型</h2><h3 id="3-1、-神经网络语言模型"><a href="#3-1、-神经网络语言模型" class="headerlink" title="3.1、 神经网络语言模型"></a>3.1、 神经网络语言模型</h3><p>神经网络语言模型（NNLM）也是对n 元语言模型进行建模，直接通过一个神经网络对其建模求解。解决了N-gram模型当 n 较大时会发生数据稀疏的问题。</p>
<p>使用了低维紧凑的词向量对上文进行表示，nnlm是一种更好的n元语言模型，解决了传统n-gram的两个缺陷：<br>(1)词语之间的相似性可以通过词向量来体现。<br>在相似的上下文语境中，nnlm模型可以预测出相似的目标词，而传统模型无法做到这一点。<br>(2)自带平滑功能。解决了词袋模型带来的数据稀疏、语义鸿沟等问题。</p>
<p>存在的问题：NNLM模型只能处理定长的序列（Ngram模型也是）；训练太慢了</p>
<h3 id="3-2、-word2vec"><a href="#3-2、-word2vec" class="headerlink" title="3.2、 word2vec"></a>3.2、 word2vec</h3><p>对原始的NNLM模型做如下改造：移除前向反馈神经网络中非线性的hidden layer（ tanh 隐藏层），直接将中间层的embedding layer与输出层的softmax layer连接；忽略上下文环境的序列信息：输入的所有词向量均汇总到同一个embedding layer；将future words纳入上下文环境。得到的模型称之为CBoW模型（Continuous Bag-of-Words Model）。从数学上看，CBoW模型等价于一个词袋模型的向量乘以一个embedding矩阵，从而得到一个连续的embedding向量。这也是CBoW模型名称的由来。</p>
<p><img src="1685676504319.png" alt="1685676504319"></p>
<p>连续词袋模型Continous Bag of Words Model(CBOW)<br>根据某个词前面的C个词或者前后C个连续的词，来计算某个词出现的概率。<br>Skip-Gram Model：Skip-Gram Model相反，是根据某个词，然后分别计算它前后出现某几个词的各个概率。</p>
<h4 id="3-2-1连续词袋模型-CBOW"><a href="#3-2-1连续词袋模型-CBOW" class="headerlink" title="3.2.1连续词袋模型  CBOW"></a>3.2.1连续词袋模型  CBOW</h4><p>假设 Courpus = { I drik coffee everyday } ，<br>Target: “coffee”，C＝2时它的上下文分别“I”“drink”“everyday”<br>模型输入：上下文的one hot表示方式：1xV的向量，V  词汇表大小</p>
<p>输入分别跟同一个VxN的大小的系数矩阵W1相乘得到C个1xN的隐藏层hidden layer，然后C个取平均所以只算一个隐藏层。</p>
<p><img src="1685677201067.png" alt="1685677201067"></p>
<p>隐藏层跟另一个NxV大小的系数矩阵W2相乘得到1xV的输出层，这个输出层每个元素代表的就是词库里每个词的事后概率。输出层需要跟ground truth也就是“coffee”的one hot形式做比较计算loss</p>
<p>通过大量的数据迭代，使用梯度下降更新W和W’，来最小化loss函数，训练结束后的W就是词向量的矩阵，任何一个单词的One-Hot表示乘以这个矩阵W就可以得到其词向量的表示</p>
<h4 id="3-2-2-Skip-Gram模型"><a href="#3-2-2-Skip-Gram模型" class="headerlink" title="3.2.2  Skip-Gram模型"></a>3.2.2  Skip-Gram模型</h4><p><img src="1685677240272.png" alt="1685677240272"></p>
<p>输入向量x代表某个单词的one-hot编码，对应的输出向量{$y_1$,…,$y_c$}。C为窗口大小。<br>有个句子“I drive my car to the store”。<br>“car”作为训练输入数据，输出单词组{“I”, “drive”, “my”, “to”, “the”, “store”}</p>
<p>基于成对的单词来对神经网络进行训练，训练样本是 ( input word, output word ) 这样的单词对，input word和output word都是one-hot编码的向量。<br>隐层没有使用任何激活函数,输出层参数矩阵是所有词向量共享的,最终模型的输出是一个概率分布。输出层使用了sotfmax。</p>
<p>最终的目标:学习这个隐层的权重矩阵 </p>
<p><img src="1685677609804.png" alt="1685677609804"></p>
<p>模型的本质:计算输入word的input vector与目标word的output vector之间的余弦相似度，并进行softmax归一化。<br>直接对词典里的 V 个词计算相似度并归一化，显然是一件极其耗时的impossible mission。<br>为此，Mikolov引入了两种优化算法:层次Softmax、负采样,时间复杂度就从O(V)变成了O(logV)：</p>
<p>负采样（Negative Sampling）<br>针对训练样本（ants, able），able这个词是正样本，词表中除able外的所有词都是负样本。<br>不进行负采样时，对每一个训练样本模型需要拟合一个正样本和九千九百九十九个负样本。加入负采样后，只需要从这九千九百九十九个负样本中挑出来几个进行拟合，大大节省了计算资源。<br>Google给出的建议是挑5-20个负样本，根据词在语料中出现的概率，概率越大越有可能被选中。</p>
<p>负采样是加快训练速度的一种方法，不直接让模型从整个词表找最可能的词，直接给定这个词（即正例）和几个随机采样的噪声词（即采样出来的负例），只要模型能从这里面找出正确的词就认为完成目标啦。</p>
<p>层次Softmax（Hierarchical Softmax）</p>
<p>在模型训练的时候首先统计语料中词语的词频，然后根据词频来构建Huffman树，树的根节点可理解为输入词的词向量，叶子节点表示词表中的词，其它节点没有什么实际含义，仅起到辅助作用</p>
<p><img src="1685681376653.png" alt="1685681376653"></p>
<p>使用Huffman树可以加快训练速度：输出层不使用one-hot来表示，softmax回归就不需要对那么多0（也即负样本）进行拟合，仅仅只需要拟合输出值在Huffman树中的一条路径。由于Huffman编码是不等长编码，频率越高的词越接近根节点，这也使计算量有所降低。</p>
<h3 id="3-3、-Doc2Vec"><a href="#3-3、-Doc2Vec" class="headerlink" title="3.3、 Doc2Vec"></a>3.3、 Doc2Vec</h3><p>Doc2vec是Word2vec的扩展,不仅学习单词的向量，还学习文本的向量表示。<br>能够使一个变长的句子、段落或文档表示为一个定长向量的算法<br>作用:获取定长的段落向量;用于聚类、分类</p>
<p>Doc2Vec原理与Word2Vec非常的相似,段落向量模型添加了一个段落向量。段落向量也可为认为是另一个词向量，同样是随机初始化而来。在训练时，词向量会随着滑动窗口的变化而变化，段落向量就不会更换。</p>
<h3 id="3-4、-Glove"><a href="#3-4、-Glove" class="headerlink" title="3.4、 Glove"></a>3.4、 Glove</h3><p>全称是global vector，改进word2vector，成功利用语料库的全局信息。考虑了语料库中词出现的次数。</p>
<p>文本特征表示：文本特征表示的目的让将文本转变成一种能够让计算机更容易处理的形式，同时减少信息的损失。</p>
<p>常见的文本特征表示方法包括：<br>•BOW用词典大小的向量来表征文本，每个值代表该词在文中出现的次数，该方法忽略了文本当中的词序。<br>•N-gram将相邻的文字和词组信息纳入到表征的词典当中。<br>•TF-IDF使用词频和逆文档频率来建模文本。<br>•Word2vec使用局部上下文信息来获取词向量。<br>•Glove采用了局部上下文信息和全局统计特征。</p>
<h1 id="第10讲-文本分类"><a href="#第10讲-文本分类" class="headerlink" title="第10讲 文本分类"></a>第10讲 文本分类</h1><h2 id="1、文本分类"><a href="#1、文本分类" class="headerlink" title="1、文本分类"></a>1、文本分类</h2><p>文本分类指：用计算机对文本(或其他实体)按照一定的分类体系或标准进行自动分类标记</p>
<p>包含两大基础结构：特征表示；分类模型（浅层学习模型、深度学习模型）</p>
<p>浅层学习模型</p>
<p>结构较为简单，依赖于人工获取的文本特征，虽然模型参数相对较少，但是在复杂任务中往往能够表现出较好的效果，具有很好的领域适应性。<br>常用模型：<br>PGM（概率图模型）：代表NB、HMM<br>KNN（K近邻）、NWKNN<br>SVM（支持向量积）、TSVM<br>DT(决策树)<br>RF（随机森林）、Adaboost、XGBoost、stacking<br>总体来讲，浅层模型学习学习预定义的特征表示，其中人工特征是问题难点；不过，浅层模型在小规模数据上表现要优于深度学习模型。</p>
<p>深度学习模型</p>
<p>结构相对复杂，不依赖人工获取的文本特征，可以直接对文本内容进行学习、建模，但是深度学习模型对于数据的依赖性较高，且存在领域适应性不强的问题。</p>
<p>将Doc2vec用于文本分类</p>
<ol>
<li>数据预处理：进行分词。</li>
<li>训练Doc2vec模型：得到文档级别的嵌入向量。</li>
<li>创建数据集文档向量：对于每个文档，使用训练好的Doc2vec模型来获取该文档的向量表示。</li>
<li>数据集拆分：将数据集分成训练集和测试集。</li>
<li>训练分类器：使用训练集训练一个分类器，比如朴素贝叶斯、SVM、逻辑回归、神经网络等。</li>
</ol>
<p>对Doc2Vec模型进行微调以优化文档向量的表示效果</p>
<ol>
<li>增加训练迭代次数：<br>增加模型的迭代次数可以提高模型的准确性和稳定性</li>
<li>调整向量维度：<br>向量维度越高，表示效果会更好，但会消耗更多的计算资源。</li>
<li>调整学习率：<br>可以通过逐步减小学习率，让模型更加准确地学习到数据。</li>
<li>调整窗口大小：<br>较小的窗口可以提供更多的关注点，并且可能会导致更准确的表示，但会损失到更广泛的文本上下文。</li>
<li>增加负采样数量</li>
<li>使用不同的训练算法</li>
</ol>
<p>将Word2vec用于文本分类</p>
<p>在进行Word2vec模型训练时，可调节模型的不同超参数以优化模型性能。如向量维度、单词频率阈值、采样率等</p>
<p>创建文本向量表示：对于每个文本，将其所有的单词或子词的Word2vec向量取平均或者加权平均得到一个文本向量表示。</p>
<h2 id="2、fastText"><a href="#2、fastText" class="headerlink" title="2、fastText"></a>2、fastText</h2><p>Facebook于2016年开源的一个<strong>词向量</strong>计算和文本分类工具</p>
<p>Fastext使用子单词嵌入的概念，利用单词的内部结构来改进从skip-gram法获得的向量表示。单词嵌入不是基于整个单词而是基于其组成子单词生成的。有助于更好地捕捉单词的含义和上下文，尤其是对于未知词汇的情况。</p>
<p>•子词嵌入</p>
<p>•<strong>将单词划分成不同的n-gram子序列，将这些子序列表示为向量</strong>，将这些向量合并成单个单词的向量表示。这种方法可以更好地处理未登录词，因为许多未登录词可能包含和已知单词类似的子词或字符。</p>
<p>•存在大量的唯一的n-grams？应用哈希来限制内存需求。</p>
<p>fastText：文本分类模型<br>embedding是fastText分类的产物<br>fastText模型也只有三层：<br>输入层<br>fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档<br>单词：onehot，被embedding嵌入过<br>单词的字符级别的n-gram向量作为额外的特征<br>隐含层<br>对一个文档中所有单词的向量进行叠加平均<br>输出层（Hierarchical Softmax）<br>fastText的输出是文档对应的类标。</p>
<p>比较fastText 与word2vec-skipgram：</p>
<p>对于形态丰富的语言(如捷克语和德语)，FastText显著提高了句法词类比任务的性能。<br>与Word2Vec相比，FastText降低了语义类比任务的性能。FastText比常规的skipgram慢1.5倍，因为增加了n-grams的开销。<br>在单词相似度任务中，使用带有字符ngrams的sub-word信息比CBOW和skip-gram基线具有更好的性能。用子词求和的方法表示词外词比用空向量表示具有更好的性能。</p>
<p>Fasttext优点：<br>模型本身复杂度低，但效果不错，能快速产生任务的baseline<br>对于文本长且对速度要求高的场景，Fasttext是baseline首选<br>Facebook使用C++进行实现，进一步提升了计算效率<br>采用了char-level的n-gram作为附加特征<br>当类别过多时，支持采用hierarchical softmax进行分类，提升效率<br>用它在无监督语料上训练词向量，进行文本表示也不错</p>
<h2 id="3、TextCNN"><a href="#3、TextCNN" class="headerlink" title="3、TextCNN"></a>3、TextCNN</h2><p>•与传统图像的CNN网络相比，TextCNN 在网络结构上没有任何变化(甚至更加简单了)</p>
<p>•TextCNN的成功，不是网络结构的成功，而是通过引入已经训练好的词向量来在多个数据集上达到了超越benchmark的表现</p>
<p>•输入数据的不同：图像是二维数据，图像的卷积核是从左到右，从上到下进行滑动来进行特征抽取。自然语言是一维数据，卷积核一个方向上滑动</p>
<p>TextCNN </p>
<p>•输入：通过词向量文件及词向量矩阵，将文本向量化，支持后续进行卷积池化等操作</p>
<p>•一层卷积，一层max-pooling，输出外接softmax来n分类</p>
<h1 id="第11讲-web链接分析"><a href="#第11讲-web链接分析" class="headerlink" title="第11讲 web链接分析"></a>第11讲 web链接分析</h1><h2 id="一、链接分析"><a href="#一、链接分析" class="headerlink" title="一、链接分析"></a>一、链接分析</h2><p>希望排序靠前的文档既是<strong>相关的</strong>又是<strong>权威的</strong>。相关性通过<strong>余弦相似度</strong>得分来判断。权威性是与query查询无关的<strong>文档本身的属性</strong>决定的。</p>
<p>网站权威性大部分是由外部链接来衡量。高质量的外部链接越多，网站或网页本身的权威性就越高。<br>另外，域名注册历史，网站的稳定性，隐私权政策等一些细节，也会影响网站的权威性。</p>
<p>将整个静态Web看成是静态HTML网页通过超链接互相连接而成的有向图，其中每个<strong>网页</strong>是图的<strong>顶点</strong>，而每个<strong>超链接</strong>则代表一个<strong>有向边</strong>。<br>顶点和有向边集合称为Web图，链接分析是指源于对Web结构中超链接的多维分析</p>
<h2 id="二、PageRank算法"><a href="#二、PageRank算法" class="headerlink" title="二、PageRank算法"></a>二、PageRank算法</h2><p>链接分析排序算法：利用网页之间的超级链接，用于衡量特定网页相对于搜索引擎索引中的其他网页而言的重要程度的算法</p>
<p>随机游走（Random Walk，缩写为 RW）数学统计模型： 它是一连串的轨迹所组成，其中每一次都是随机的。能用来表示不规则的变动形式，随机游走的形式有：马尔可夫链或马可夫过程、醉汉走路（drunkard’s walk）</p>
<p>一阶马尔可夫链：从一个随机的页面开始，每一步从当前页等概率地选择一个链接，进入链接所在页面，在稳定状态下，每个页面都有一个访问概率 – 用这个概率作为页面的分数<br>当冲浪者在 Web 上进行节点间的随机游走时，某些节点的访问次数会比其它节点更多。直观地看，这些访问频繁的节点具有很多从其它频繁访问节点中指向的<strong>入链接</strong>。</p>
<p>直观上， PageRank值高，这个网页重要<br>指向该网页的超链接越多，随机跳转到该网页的概率也就越高<br>指向该网页的PageRank值越高，指向该网页的PageRank值被分走的越少</p>
<p>PageRank值依赖于网络的拓扑结构，一旦网络的拓扑（连接关系）确定，PageRank值就确定 </p>
<p>在一定条件下，极限情况访问每个结点的概率收敛到平稳分布，这时各个结点的平稳概率值就是其PageRank值，表示结点的重要度。<br>PageRank表示这个马尔可夫链的平稳分布，每个网页的PageRank值就是平稳概率。<br>PageRank是递归定义的，PageRank的计算可以通过迭代算法进行。PageRank的计算通常是一个迭代过程。先假设一个初始分布，通过迭代，不断计算所有网页的PageRank值，直到收敛为止</p>
<p>随机游走模型：在有向图上的随机游走形成马尔可夫链。也就是说，随机游走者每经一个单位时间转移一个状态。如果当前时刻在第j个结点（状态），那么下一个时刻在第i个结 点（状态）的概率是mij。这一概率只依赖于当前的状态，与过去无关，具有马尔可夫性。</p>
<p>一般的有向图未必满足强连通且非周期性的条件。所以PageRank的基本定义不适用。（比如有的节点没有出边）。</p>
<p>PageRank一般定义的想法是在基本定义的基础上导入平滑项。<br>1.假设考虑一个在图上 随机游走模型，即一阶马尔可夫链，其转移矩阵是M，从一个结点到其连出的所有结点的转移概率相等。<br>2.假设考虑另一个完全随机游走的模型，其转移矩阵的元素全部为1/n，也就是说从任意一个结点到任意一个结点的转移概率都是1/n。<br>3.两个转移矩阵的线性组合又构成一个新的转移矩阵，在其上可以定义一个新的马尔可夫链。这个马尔可夫链一定具有平稳分布，且平稳分布满足$R=dMR+\frac{1-d}{n}1$,R表示的就是有向图的一般PageRank,1是所有分量为1的n维向量</p>
<p>第一项表示（状态分布是平稳分布时）依照转移矩阵M访问各个结点的概率，<br>第二项表示完全随机访问各个结点的概率,阻尼因子d取值由经验决定<br>当d接近1时，随机游走主要依照转移矩阵M进行<br>当d接近0时， 随机游走主要以等概率随机访问各个结点。 </p>
<p>第二项称为平滑项，由于采用平滑项，所有结点PageRank值都不会为0，且$PR(v<em>i)&gt;0,i=1,2,…,n$,$\sum\limits</em>{i=1}^nPR(v_i)=1$</p>
<p>一般PageRank的定义意味着互联网浏览者，按照以下方法在网上随机游走：<br><strong>在任意一个网页上，浏览者或者以概率d决定按照超链接随机跳转，这时以等概率从连接出去的超链接跳转到下一个网页。或者以概率（1-d)决定完全随机跳转，这时以等概率1/n跳转到任意一个网页。</strong></p>
<p>第二个机制保证从没有连接出去的超链接的网页也可以跳转出。这样可以保证平稳分布，即一般PageRank的存在，因而一般PageRank适用于任何结构的网络。 </p>
<p>算法实现：迭代法或者代数法</p>
<p>(PageRank 的迭代算法)<br>输入:含有n个结点的有向图,转移矩阵M,阻尼因子d,初始向量$R<em>0$; .<br>输出:有向图的PageRank向量R。<br>(1)令t=0<br>(2)计算$R</em>{t+1}=dMR<em>t+\frac{1-d}{n}1$<br>(3)如果$R</em>{t+1}$与$R<em>t$充分接近，令R=$R</em>{t+1}$, 停止迭代。<br>(4)否则，令t=t+1,执行步(2)。</p>
<p>代数算法（求逆矩阵）：由$R=dMR+\frac{1-d}{n}1$得到$(I-dM)R=\frac{1-d}{n}1$,所以$R=(I-dM)^{-1}\frac{1-d}{n}1$</p>
<h2 id="三、textrank算法"><a href="#三、textrank算法" class="headerlink" title="三、textrank算法"></a>三、textrank算法</h2><p>TextRank算法基于PageRank，用于为文本生成关键字和摘要。</p>
<p>使用TextRank提取关键字 ：<br>•1）把给定的文本T按照完整句子进行分割<br>•2）对每个句子，进行分词和词性标注处理<br>–过滤掉停用词，只保留指定词性的单词，如名词、动词、形容词，即，其中是保留后的候选关键词。<br>•3）构建候选关键词图G = (V,E)，V为节点集，由候选关键词组成,采用共现关系（co-occurrence）构造任两点之间的边<br>•两个节点之间存在边仅当它们对应的词汇在长度为K的窗口中共现，K表示窗口大小，即最多共现K个单词。<br>•4）迭代传播各节点的权重，直至收敛。<br>•5）对节点权重进行倒序排序，从而得到最重要的T个单词，作为候选关键词。<br>•6）由（5）得到最重要的T个单词，在原始文本中进行标记，若形成相邻词组，则组合成多词关键词。</p>
<p>文本摘要任务：节点不再是词，而是句子。而句与句之间的联系，也不再使用”共现“来确定，而是利用相似度确定。</p>
<h1 id="第12讲-WEB图片数据"><a href="#第12讲-WEB图片数据" class="headerlink" title="第12讲  WEB图片数据"></a>第12讲  WEB图片数据</h1><h2 id="一、图像检索"><a href="#一、图像检索" class="headerlink" title="一、图像检索"></a>一、图像检索</h2><p>基于内容的图像检索CBIR：Content-based image retrieval 用户输入一张图片，以查找具有相同或相似内容的其他图片。</p>
<p>CBIR 的关键技术:<strong>图像特征提取和匹配</strong></p>
<h2 id="二、图像特征"><a href="#二、图像特征" class="headerlink" title="二、图像特征"></a>二、图像特征</h2><p><img src="1685698213550.png" alt="1685698213550"></p>
<p>图像的特征主要包括低层特征(Primitive Features)和语义特征(Semantic Features）</p>
<h2 id="三、颜色特征"><a href="#三、颜色特征" class="headerlink" title="三、颜色特征"></a>三、颜色特征</h2><p>颜色是彩色图像最底层、最直观的物理特征，通常对噪声，图像质量的退化，尺寸、分辨率和方向等的变化具有很强的鲁棒性，是绝大多数基于内容的图像和视频检索的多媒体数据库中使用的特征之一。</p>
<h3 id="3-1-颜色空间"><a href="#3-1-颜色空间" class="headerlink" title="3.1  颜色空间"></a>3.1  颜色空间</h3><p>颜色空间  color spaces，它的用途是在某些标准下用通常可接受的方式对彩色加以说明</p>
<p>1、RGB空间：RGB对应红绿蓝三种原色光，自然界的所有颜色都可以用这三种光混合而成。在描述时，用R、G、B作为相互垂直的坐标轴来表示，是一种加光模式。<strong>三种基色中的每一种都有一个0~255的值</strong>。所有基色相加形成白色<br>所有基色的值都为0时，得到黑色<br>2、HSV：人的视觉对亮度的敏感程度远强于对颜色浓淡的敏感程度。<br>三个分量分别代表色调（Hue）、饱和度（Saturation）和亮度（Intensity/value）</p>
<h3 id="3-2-颜色直方图-Color-Histogram"><a href="#3-2-颜色直方图-Color-Histogram" class="headerlink" title="3.2  颜色直方图(Color Histogram)"></a>3.2  颜色直方图(Color Histogram)</h3><p>最简单也是最常用的颜色特征</p>
<p>•核心思想：在颜色空间中采用一定的量化方法对颜色进行量化，然后统计每一个量化通道在整幅图像中所占的比重。</p>
<p>•每种颜色$C<em>i$  ,$ H</em>{ci}(I) $表示图片$I$中，颜色是$C_i$ 的像素数目</p>
<p>描述的是不同色彩在整幅图像中所占的比例<br>统计分布特性<br>具有平移、尺度、旋转不变性<br>特别适于描述那些难以进行自动分割的图像。</p>
<p>任一幅图像都能唯一的给出一幅与它对应的直方图，但不同的图像可能有相同的颜色分布——-子窗口直方图法，即将图像分割成子图像，一一建立索引。</p>
<h3 id="3-3-颜色矩-Color-Moment"><a href="#3-3-颜色矩-Color-Moment" class="headerlink" title="3.3  颜色矩(Color Moment)"></a>3.3  颜色矩(Color Moment)</h3><p>在颜色直方图的基础上计算出每个颜色的<strong>矩估计</strong>,颜色信息主要分布于低阶矩中<br>一阶矩(<strong>均值</strong>,mean),二阶矩(<strong>方差</strong>,viarance),三阶矩(<strong>斜度</strong>,skewness)<br>用这些统计量替代颜色的分布来表示颜色特征,它具有特征量少，处理简单的特点。</p>
<p>不需要颜色空间量化，特征向量维数低；但实验发现该方法的检索效率比较低，因而在实际应用中往往用来过滤图像以缩小检索范围。常和其他特征结合</p>
<h2 id="四、纹理特征"><a href="#四、纹理特征" class="headerlink" title="四、纹理特征"></a>四、纹理特征</h2><p>一般说纹理就是指在图像中反复出现的局部模式和它们的排列规则。<br>•纹理特征描述了图像或图像区域所对应景物的表面性质<br>并不能完全反映出物体的本质属性，当图像的分辨率变化的时候，所计算出来的纹理可能会有较大偏差。由于有可能受到光照、反射情况的影响，从2-D图像中反映出来的纹理不一定是3-D物体表面真实的纹理。虚假的纹理会对检索造成“误导”</p>
<p>•与颜色特征不同，纹理特征不是基于像素点的特征，它需要在包含多个像素点的区域中进行统计计算。<br>在模式匹配中，这种区域性的特征具有较大的优越性，不会由于局部的偏差而无法匹配成功</p>
<p>•作为一种统计特征，纹理特征常具有旋转不变性，并且对于噪声有较强的抵抗能力<br>•在检索具有粗细、疏密等方面较大差别的纹理图像时，利用纹理特征是一种有效的方法。<br>•但当纹理之间的粗细、疏密等易于分辨的信息之间相差不大的时候，通常的纹理特征很难准确地反映出人的视觉感觉不同的纹理之间的差别。</p>
<h3 id="4-1、-基于信号处理方法描述纹理特征"><a href="#4-1、-基于信号处理方法描述纹理特征" class="headerlink" title="4.1、  基于信号处理方法描述纹理特征"></a>4.1、  基于信号处理方法描述纹理特征</h3><p>•信号处理类方法的思路来源于视觉心理学<br>–动物：采取的分析与解译机制是对纹理图像中的频率分量和方向分量分别进行处理<br>–对<strong>图像信号的频率和方向</strong>进行选择性<strong>滤波</strong>，以此得到相应的图像特征。</p>
<p>•纹理图像<br>–空域(实域)分析法：就是对图像矩阵进行处理；<br>–频域分析法：基于一个假设：频域的能量分布能够鉴别纹理</p>
<p>•通过图像变换将图像从空域变换到频域，从另外一个角度来分析图像的特征并进行处理</p>
<p>–<strong>图像信号的频率和方向</strong><br>•信号处理类的纹理特征主要是利用某种线性变换、滤波器或者滤波器组将纹理转换到变换域，然后应用某种能量准则提取纹理特征。因此，基于信号处理的方法也称之为滤波方法。<br>•图像滤波方法主要有傅里叶变换和Gabor滤波器</p>
<h3 id="4-2、-LBP特征"><a href="#4-2、-LBP特征" class="headerlink" title="4.2、  LBP特征"></a>4.2、  LBP特征</h3><p>•局部二值模式 Local Binary Patterns<br>•结合了纹理图像结构和像素统计关系的纹理特征描述方法<br>•一种有效的<strong>纹理描述算子</strong>：度量和提取图像局部的纹理信息；记录<strong>像素点与其周围像素点的对比信息，或说差异</strong><br>•显著的优点：对光照具有不变性；具有旋转不变性；灰度不变性等</p>
<p>•LBP算子定义为在3*3的窗口内，以窗口中心像素为阈值，将相邻的8个像素的灰度值与其进行比较，若周围像素值大于中心像素值，则该像素点的位置被标记为1，否则为0。<br>•3<em>3邻域内的8个点经比较可产生8位二进制数（通常转换为十进制数即LBP码，共256种），即得到该窗口中心像素点的LBP值，并用这个值来反映该区域的纹理信息。<br>一般采用LBP特征谱的统计直方图作为特征向量用于分类识别。<br>•LBP特征跟位置信息是紧密相关的。会因为“位置没有对准”而产生很大的误差。<br>–可以将一幅图片划分为若干的子区域，对每个子区域内的每个像素点都提取LBP特征，然后，在每个子区域内建 立LBP特征的统计直方图。如此一来，每个子区域，就可以用一个统计直方图来进行描述；整个图片就由若干个统计直方图组成； 利用各种相似性度量函数，就可以判断两幅图像之间的<em>*相似性</em></em>了</p>
<h2 id="五、形状特征"><a href="#五、形状特征" class="headerlink" title="五、形状特征"></a>五、形状特征</h2><p>•低级图像特征主要有颜色、纹理和形状，低级图像特征包括局部特征和全局特征。</p>
<p>全局描述符基于整幅图像提取的描述符<br>•全局特征对图像的<strong>压缩率较高，但区分力不强</strong></p>
<p>局部特征是基于图像的某个区域提取的图像描述符<br>•如尺度不变特征SIFT(Scale Invariant Feature Transform)。<br>•局部特征的<strong>区分力强，但数目太多</strong></p>
<h3 id="5-1-HOG特征"><a href="#5-1-HOG特征" class="headerlink" title="5.1  HOG特征"></a>5.1  HOG特征</h3><p>方向梯度直方图Histogram of Oriented Gradient, HOG。一种在计算机视觉和图像处理中用来进行物体检测的特征描述子。通过计算和统计图像局部区域的梯度方向直方图来构成特征。</p>
<p>在图像中梯度的概念也是<strong>像素值变换最快的方向</strong>，把边缘（在图像合成中单一物体的轮廓叫做边缘）引入进来，边缘与梯度保持垂直方向</p>
<p>HOG主要思想： 在一副图像中，局部目标的表象和形状（appearance and shape）能够被梯度或边缘的方向密度分布很好地描述。<br>本质：梯度的统计信息，而梯度主要存在于边缘的地方。</p>
<p>具体的实现方法是：<br>将图像分成小的连通区域—-细胞单元。<br>采集细胞单元中各像素点的梯度的或边缘的方向直方图。<br>最后把这些直方图组合起来就可以构成特征描述器。 </p>
<p>优点：<br>由于HOG是在图像的局部方格单元上操作，所以它对图像几何的和光学的形变都能保持很好的不变性，这两种形变只会出现在更大的空间领域上。<br>其次，在粗的空域抽样、精细的方向抽样以及较强的局部光学归一化等条件下，只要行人大体上能够保持直立的姿势，可以容许行人有一些细微的肢体动作，这些细微的动作可以被忽略而不影响检测效果。因此HOG特征是特别适合于做图像中的人体检测的。</p>
<h3 id="5-2-SIFT特征"><a href="#5-2-SIFT特征" class="headerlink" title="5.2  SIFT特征"></a>5.2  SIFT特征</h3><p>尺度不变特征转换（Scale-invariant feature transform或SIF）在空间尺度中寻找极值点，并提取出其位置、尺度、旋转不变量。</p>
<p>•SIFT是一种检测局部特征的算法<br>•SIFT中每个feature需要用128维的向量来描述，因此计算量相对很大。<br>通过求一幅图中的特征点（interest points,or corner points）及其有关scale 和 orientation 的描述子得到特征<br>•SIFT特征不只具有尺度不变性，即使改变旋转角度，图像亮度或拍摄视角，仍然能够得到好的检测效果（Hog没有旋转和尺度不变性）</p>
<p>尺度空间理论：自然界中的物体随着观测尺度不同有不同的表现形态。<br><strong>尺度越大图像越模糊</strong></p>
<p>尺度是自然客观存在的，不是主观创造的。尺度空间的获取需要使用高斯模糊来实现，<strong>高斯卷积核</strong>是实现尺度变换的唯一变换核，并且是唯一的线性核。<strong>高斯卷积只是表现尺度空间的一种形式</strong></p>
<p>尺度空间：在图像信息处理模型中引入一个被视为尺度的参数，通过连续变化尺度参数获得多尺度下的尺度空间表示序列，对这些序列进行尺度空间主轮廓的提取，并以该主轮廓作为一种特征向量，实现边缘、角点检测和不同分辨率上的特征提取等。</p>
<p>高斯金字塔并不是一个金字塔，而是有很多组（Octave 子八度）金字塔构成，并且每组金字塔都包含若干层（Interval）。</p>
<p>chatgpt：”子八度”这个术语源于音乐领域。在音乐中，一个音调的频率是另一个音调的一半或两倍时，它们被认为是在同一个八度（octave）内。因此，将一个音符的频率降低一半，就处于该音符所在八度的下一个子八度（sub-octave）中。在图像处理领域，由于缩小图像尺寸的过程相当于减小了图像中特征的空间频率，因此将缩小一倍的图像称为子八度图像，以类比于音乐中的概念。</p>
<p>高斯金字塔构建过程：将原图像扩大一倍之后作为高斯金字塔的第1组，对该八度下的最模糊的一幅图像进行下采样的过程，长和宽分别缩短一倍，图像面积变为原来四分之一。这幅图像就是下一个八度的初始图像，在初始图像图像的基础上完成属于这个八度的高斯模糊处理，以此类推完成整个算法所需要的所有八度构建</p>
<p>高斯金字塔在多分辨率金字塔简单降采样基础上加了高斯滤波，也就是对金字塔每层图像用不同参数的σ做高斯模糊，使得每层金字塔有多张高斯模糊图像。将金字塔每层多张图像合称为一组(Octave)，金字塔每层只有一组图像，组数和金字塔层数相等</p>
<p>SIFT算法大致分为四个步骤：<br>步骤一：建立尺度空间，即建立高斯差分(DoG)金字塔<br>步骤二：在尺度空间中检测极值点，并进行精确定位和筛选<br>步骤三：特征点方向赋值，完成此步骤后，每个特征点有三个信息：位置、尺度、方向<br>步骤四：计算特征描述子</p>
<p>SIFT特征以其对旋转、尺度缩放、亮度等保持不变性，是一种非常稳定的局部特征<br>SIFT的缺点：<br>相对来说实时性还不够高。<br>有时特征点较少。<br>对边缘光滑的目标无法准确提取特征点</p>
<p>基于特征点的图像相似度计算，基于SIFT描述子<br>•通过找到匹配点的个数来判断两幅图像是否一致，<br>–暴力匹配<br>–计算向量的欧氏距离, 找距离最小的样本向量</p>
<p>•算法的好处是对于一个物体，两个不同角度下得到的照片依然可以找到很多的匹配点，可以通过找到的匹配特征点进行图像校正</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://jiang54864.github.io">姜将</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://jiang54864.github.io/%E7%AC%94%E8%AE%B0/WEB%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/">http://jiang54864.github.io/%E7%AC%94%E8%AE%B0/WEB%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://jiang54864.github.io" target="_blank">姜将的个人博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/web/">web</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E7%8E%B0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" title="《深度学习入门：基于python的理论与实现》读书笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">《深度学习入门：基于python的理论与实现》读书笔记</div></div></a></div><div class="next-post pull-right"><a href="/%E7%BC%96%E7%A8%8B/%E5%A4%9A%E6%A0%B8%E5%B9%B3%E5%8F%B0%E4%B8%8B%E7%9A%84%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%A4%8D%E4%B9%A0%E9%A2%98/" title="多核平台下的并行计算复习题"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">多核平台下的并行计算复习题</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">姜将</div><div class="author-info__description">记录学习中的知识与收获</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/JIANG54864"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/JIANG54864" target="_blank" title="我的Github主页"><i class="fab fa-github"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=2792663690&amp;website=www.oicqzone.com" target="_blank" title="点击添加我的QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="https://space.bilibili.com/262150061" target="_blank" title="我的Bilibili主页"><i class="fab fa-bilibili"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客网站！若有需要，可通过上方联系方式找到我。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC1%E8%AE%B2-%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">第1讲  前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC2%E8%AE%B2-%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF"><span class="toc-number">2.</span> <span class="toc-text">第2讲  网络爬虫技术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80-%E3%80%81%E7%88%AC%E8%99%AB%E5%AE%9A%E4%B9%89"><span class="toc-number">2.1.</span> <span class="toc-text">一 、爬虫定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E7%88%AC%E5%8F%96%E8%BF%87%E7%A8%8B"><span class="toc-number">2.2.</span> <span class="toc-text">二、爬取过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81-URL-%E5%88%A4%E9%87%8D"><span class="toc-number">2.3.</span> <span class="toc-text">三、 URL 判重</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81-%E5%BF%85%E9%A1%BB%E5%85%B7%E6%9C%89%E7%9A%84%E5%8A%9F%E8%83%BD"><span class="toc-number">2.4.</span> <span class="toc-text">四、  必须具有的功能</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%80%A24-1-%E7%A4%BC%E8%B2%8C%E6%80%A7"><span class="toc-number">2.4.1.</span> <span class="toc-text">•4.1 礼貌性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%80%A24-2-%E9%B2%81%E6%A3%92%E6%80%A7"><span class="toc-number">2.4.2.</span> <span class="toc-text">•4.2 鲁棒性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%80%A24-3-%E6%80%A7%E8%83%BD%E5%92%8C%E6%95%88%E7%8E%87"><span class="toc-number">2.4.3.</span> <span class="toc-text">•4.3 性能和效率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%80%A24-4-%E5%88%86%E5%B8%83%E5%BC%8F"><span class="toc-number">2.4.4.</span> <span class="toc-text">•4.4 分布式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%80%A24-5-%E6%96%B0%E9%B2%9C%E5%BA%A6"><span class="toc-number">2.4.5.</span> <span class="toc-text">•4.5 新鲜度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%80%A24-6-%E5%8A%9F%E8%83%BD%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7"><span class="toc-number">2.4.6.</span> <span class="toc-text">•4.6 功能可扩展性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E7%88%AC%E8%99%AB%E5%88%86%E7%B1%BB"><span class="toc-number">2.5.</span> <span class="toc-text">五、爬虫分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E5%9F%BA%E4%BA%8E%E6%95%B4%E4%B8%AAWeb%E7%9A%84%E4%BF%A1%E6%81%AF%E9%87%87%E9%9B%86"><span class="toc-number">2.5.1.</span> <span class="toc-text">5.1 基于整个Web的信息采集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E5%A2%9E%E9%87%8F%E5%BC%8FWeb%E4%BF%A1%E6%81%AF%E9%87%87%E9%9B%86"><span class="toc-number">2.5.2.</span> <span class="toc-text">5.2 增量式Web信息采集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E5%9F%BA%E4%BA%8E%E4%B8%BB%E9%A2%98%E7%9A%84Web%E4%BF%A1%E6%81%AF%E9%87%87%E9%9B%86"><span class="toc-number">2.5.3.</span> <span class="toc-text">5.3 基于主题的Web信息采集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E4%B8%AA%E6%80%A7%E5%8C%96%E7%9A%84Web%E4%BF%A1%E6%81%AF%E9%87%87%E9%9B%86"><span class="toc-number">2.5.4.</span> <span class="toc-text">5.4 基于用户个性化的Web信息采集</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC3%E8%AE%B2-%E7%BD%91%E9%A1%B5%E5%88%86%E6%9E%90%E6%8A%80%E6%9C%AF"><span class="toc-number">3.</span> <span class="toc-text">第3讲    网页分析技术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="toc-number">3.1.</span> <span class="toc-text">一、正则表达式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%9F%BA%E4%BA%8EHTML-DOM%E6%8F%90%E5%8F%96%E5%86%85%E5%AE%B9"><span class="toc-number">3.2.</span> <span class="toc-text">二、基于HTML DOM提取内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81-Beautiful-Soup%E6%A8%A1%E5%9D%97"><span class="toc-number">3.3.</span> <span class="toc-text">三、 Beautiful Soup模块</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81-python%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6-Scrapy"><span class="toc-number">3.4.</span> <span class="toc-text">四、  python爬虫框架 Scrapy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E5%85%83%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E"><span class="toc-number">3.5.</span> <span class="toc-text">五、元搜索引擎</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC4%E8%AE%B2-%E7%88%AC%E8%99%AB%E4%B8%8E%E7%BD%91%E7%AB%99%E7%9A%84%E5%8D%9A%E5%BC%88"><span class="toc-number">4.</span> <span class="toc-text">第4讲  爬虫与网站的博弈</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81Robot-%E5%8D%8F%E8%AE%AE"><span class="toc-number">4.1.</span> <span class="toc-text">一、Robot 协议</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81User-agent"><span class="toc-number">4.2.</span> <span class="toc-text">二、User-agent</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81IP%E5%B1%8F%E8%94%BD"><span class="toc-number">4.3.</span> <span class="toc-text">三、IP屏蔽</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E7%94%A8%E6%88%B7%E7%99%BB%E9%99%86"><span class="toc-number">4.4.</span> <span class="toc-text">四、用户登陆</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E6%A8%A1%E6%8B%9F%E6%B5%8F%E8%A7%88%E5%99%A8%E8%BF%9B%E8%A1%8C%E4%BA%A4%E4%BA%92"><span class="toc-number">4.5.</span> <span class="toc-text">五、模拟浏览器进行交互</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E9%AA%8C%E8%AF%81%E7%A0%81"><span class="toc-number">4.6.</span> <span class="toc-text">六、验证码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC5%E8%AE%B2-%E6%95%B0%E6%8D%AE%E6%8A%BD%E5%8F%96%E4%B8%8E%E5%8C%85%E8%A3%85%E5%99%A8"><span class="toc-number">5.</span> <span class="toc-text">第5讲  数据抽取与包装器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81WEB%E6%95%B0%E6%8D%AE%E6%8A%BD%E5%8F%96"><span class="toc-number">5.1.</span> <span class="toc-text">一、WEB数据抽取</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Web%E6%95%B0%E6%8D%AE%E6%8A%BD%E5%8F%96%E6%96%B9%E6%B3%95"><span class="toc-number">5.2.</span> <span class="toc-text">二、Web数据抽取方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81Web-%E6%95%B0%E6%8D%AE%E6%8A%BD%E5%8F%96%E8%AF%84%E4%BB%B7%E6%A0%87%E5%87%86"><span class="toc-number">5.3.</span> <span class="toc-text">三、Web 数据抽取评价标准</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%8C%85%E8%A3%85%E5%99%A8"><span class="toc-number">5.4.</span> <span class="toc-text">四、包装器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC6%E8%AE%B2-%E5%8C%85%E8%A3%85%E5%99%A8%E9%A1%B5%E9%9D%A2%E6%8A%BD%E5%8F%96%E6%96%B9%E6%B3%95"><span class="toc-number">6.</span> <span class="toc-text">第6讲 包装器页面抽取方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%95%B0%E6%8D%AE%E6%8A%BD%E5%8F%96%E5%AF%B9%E8%B1%A1%E2%80%94%E7%BD%91%E9%A1%B5%E7%9A%84%E5%88%86%E7%B1%BB"><span class="toc-number">6.1.</span> <span class="toc-text">一、数据抽取对象—网页的分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%A4%9A%E8%AE%B0%E5%BD%95%E6%95%B0%E6%8D%AE%E5%9E%8B%E9%A1%B5%E9%9D%A2%E7%9A%84%E6%8A%BD%E5%8F%96%E6%96%B9%E6%B3%95"><span class="toc-number">6.2.</span> <span class="toc-text">二、多记录数据型页面的抽取方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%8D%95%E8%AE%B0%E5%BD%95%E6%95%B0%E6%8D%AE%E5%9E%8B%E9%A1%B5%E9%9D%A2%E6%8A%BD%E5%8F%96%E6%96%B9%E6%B3%95"><span class="toc-number">6.3.</span> <span class="toc-text">三、单记录数据型页面抽取方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%8D%95%E6%96%87%E6%A1%A3%E5%9E%8B%E9%A1%B5%E9%9D%A2%E6%8A%BD%E5%8F%96%E6%96%B9%E6%B3%95"><span class="toc-number">6.4.</span> <span class="toc-text">四、单文档型页面抽取方法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC7%E8%AE%B2-web%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%B8%8E%E5%BA%94%E7%94%A8"><span class="toc-number">7.</span> <span class="toc-text">第7讲 web数据存储与应用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81-%E7%88%AC%E8%99%AB%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8"><span class="toc-number">7.1.</span> <span class="toc-text">一、 爬虫数据存储</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E7%BB%93%E6%9E%84%E5%8C%96%E6%96%87%E4%BB%B6"><span class="toc-number">7.1.1.</span> <span class="toc-text">1.1  结构化文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">7.1.2.</span> <span class="toc-text">1.2 数据库</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81-%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-number">7.2.</span> <span class="toc-text">二、 结构化数据处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97"><span class="toc-number">7.2.1.</span> <span class="toc-text">2.1  数据清洗</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%E5%BA%94%E7%94%A8"><span class="toc-number">7.2.2.</span> <span class="toc-text">2.2  结构化数据应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-number">7.2.3.</span> <span class="toc-text">2.3  特征工程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">7.2.4.</span> <span class="toc-text">2.4 机器学习的数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-%E8%BF%9E%E7%BB%AD%E5%80%BC%E4%B8%8E%E7%A6%BB%E6%95%A3%E5%80%BC"><span class="toc-number">7.2.5.</span> <span class="toc-text">2.5 连续值与离散值</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81-%E9%9D%9E%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-number">7.3.</span> <span class="toc-text">三、 非结构化数据处理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC8%E8%AE%B2-%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA"><span class="toc-number">8.</span> <span class="toc-text">第8讲  文本表示</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">8.1.</span> <span class="toc-text">1、文本预处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E5%8C%96"><span class="toc-number">8.2.</span> <span class="toc-text">2、文本向量化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E6%96%87%E6%A1%A3%E5%93%88%E5%B8%8C"><span class="toc-number">8.3.</span> <span class="toc-text">3、文档哈希</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-shingle%E7%AE%97%E6%B3%95"><span class="toc-number">8.3.1.</span> <span class="toc-text">3.1 shingle算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%B1%80%E9%83%A8%E6%95%8F%E6%84%9F%E5%93%88%E5%B8%8C-LSH"><span class="toc-number">8.3.2.</span> <span class="toc-text">3.2 局部敏感哈希   LSH</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC9%E8%AE%B2-%E8%AF%8D%E5%B5%8C%E5%85%A5word-embedding"><span class="toc-number">9.</span> <span class="toc-text">第9讲 词嵌入word embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E8%AF%8D%E5%B5%8C%E5%85%A5"><span class="toc-number">9.1.</span> <span class="toc-text">1、词嵌入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">9.2.</span> <span class="toc-text">2、语言模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">9.3.</span> <span class="toc-text">3、用神经网络训练语言模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1%E3%80%81-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">9.3.1.</span> <span class="toc-text">3.1、 神经网络语言模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2%E3%80%81-word2vec"><span class="toc-number">9.3.2.</span> <span class="toc-text">3.2、 word2vec</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1%E8%BF%9E%E7%BB%AD%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B-CBOW"><span class="toc-number">9.3.2.1.</span> <span class="toc-text">3.2.1连续词袋模型  CBOW</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-Skip-Gram%E6%A8%A1%E5%9E%8B"><span class="toc-number">9.3.2.2.</span> <span class="toc-text">3.2.2  Skip-Gram模型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3%E3%80%81-Doc2Vec"><span class="toc-number">9.3.3.</span> <span class="toc-text">3.3、 Doc2Vec</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4%E3%80%81-Glove"><span class="toc-number">9.3.4.</span> <span class="toc-text">3.4、 Glove</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC10%E8%AE%B2-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB"><span class="toc-number">10.</span> <span class="toc-text">第10讲 文本分类</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB"><span class="toc-number">10.1.</span> <span class="toc-text">1、文本分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81fastText"><span class="toc-number">10.2.</span> <span class="toc-text">2、fastText</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81TextCNN"><span class="toc-number">10.3.</span> <span class="toc-text">3、TextCNN</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC11%E8%AE%B2-web%E9%93%BE%E6%8E%A5%E5%88%86%E6%9E%90"><span class="toc-number">11.</span> <span class="toc-text">第11讲 web链接分析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E9%93%BE%E6%8E%A5%E5%88%86%E6%9E%90"><span class="toc-number">11.1.</span> <span class="toc-text">一、链接分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81PageRank%E7%AE%97%E6%B3%95"><span class="toc-number">11.2.</span> <span class="toc-text">二、PageRank算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81textrank%E7%AE%97%E6%B3%95"><span class="toc-number">11.3.</span> <span class="toc-text">三、textrank算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC12%E8%AE%B2-WEB%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE"><span class="toc-number">12.</span> <span class="toc-text">第12讲  WEB图片数据</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2"><span class="toc-number">12.1.</span> <span class="toc-text">一、图像检索</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81"><span class="toc-number">12.2.</span> <span class="toc-text">二、图像特征</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E9%A2%9C%E8%89%B2%E7%89%B9%E5%BE%81"><span class="toc-number">12.3.</span> <span class="toc-text">三、颜色特征</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E9%A2%9C%E8%89%B2%E7%A9%BA%E9%97%B4"><span class="toc-number">12.3.1.</span> <span class="toc-text">3.1  颜色空间</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E9%A2%9C%E8%89%B2%E7%9B%B4%E6%96%B9%E5%9B%BE-Color-Histogram"><span class="toc-number">12.3.2.</span> <span class="toc-text">3.2  颜色直方图(Color Histogram)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E9%A2%9C%E8%89%B2%E7%9F%A9-Color-Moment"><span class="toc-number">12.3.3.</span> <span class="toc-text">3.3  颜色矩(Color Moment)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E7%BA%B9%E7%90%86%E7%89%B9%E5%BE%81"><span class="toc-number">12.4.</span> <span class="toc-text">四、纹理特征</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E3%80%81-%E5%9F%BA%E4%BA%8E%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%8F%8F%E8%BF%B0%E7%BA%B9%E7%90%86%E7%89%B9%E5%BE%81"><span class="toc-number">12.4.1.</span> <span class="toc-text">4.1、  基于信号处理方法描述纹理特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E3%80%81-LBP%E7%89%B9%E5%BE%81"><span class="toc-number">12.4.2.</span> <span class="toc-text">4.2、  LBP特征</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E5%BD%A2%E7%8A%B6%E7%89%B9%E5%BE%81"><span class="toc-number">12.5.</span> <span class="toc-text">五、形状特征</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-HOG%E7%89%B9%E5%BE%81"><span class="toc-number">12.5.1.</span> <span class="toc-text">5.1  HOG特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-SIFT%E7%89%B9%E5%BE%81"><span class="toc-number">12.5.2.</span> <span class="toc-text">5.2  SIFT特征</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%AE%9E%E9%AA%8C/%E5%80%9F%E5%8A%A9ollama%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8Cqwen2-5/" title="借助ollama本地运行qwen2.5">借助ollama本地运行qwen2.5</a><time datetime="2025-01-12T08:37:49.000Z" title="发表于 2025-01-12 16:37:49">2025-01-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/others/2024%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/" title="2024年度总结">2024年度总结</a><time datetime="2025-01-01T05:26:53.000Z" title="发表于 2025-01-01 13:26:53">2025-01-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E7%BC%96%E7%A8%8B/%E4%B8%80%E4%BA%9B%E7%AE%97%E6%B3%95%E7%AC%94%E8%AF%95%E9%A2%98/" title="一些算法笔试题">一些算法笔试题</a><time datetime="2024-05-06T15:14:38.000Z" title="发表于 2024-05-06 23:14:38">2024-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E7%BC%96%E7%A8%8B/Latex%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%AD%A3%E5%B8%B8%E4%BD%BF%E7%94%A8/" title="Latex从入门到正常使用">Latex从入门到正常使用</a><time datetime="2024-03-30T12:22:20.000Z" title="发表于 2024-03-30 20:22:20">2024-03-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E7%BC%96%E7%A8%8B/C-%E5%9B%9E%E9%A1%BE/" title="C++回顾">C++回顾</a><time datetime="2023-06-16T06:28:25.000Z" title="发表于 2023-06-16 14:28:25">2023-06-16</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 姜将</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>