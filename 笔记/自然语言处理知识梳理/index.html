<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>自然语言处理知识梳理 | 姜将的个人博客</title><meta name="author" content="姜将"><meta name="copyright" content="姜将"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一、词汇向量化词汇向量化向量化是指采用计算空间的数值向量来表示离散语言文字的过程。词汇向量化是将最小语言功能单位词汇映射到计算空间的方法，并期望映射后的向量仍能保持词汇的语言学重要属性，如语义、语用或形态等。 词汇向量化方法可以分为两大类： 离散编码表示离散向量化方法是将词汇表示成整数形式的唯一标识符，或是编码成为一个整数向量。典型代表为独热编码，词汇向量即为词典数量对应的高维空间上的 0&#x2F;1 向">
<meta property="og:type" content="article">
<meta property="og:title" content="自然语言处理知识梳理">
<meta property="og:url" content="http://jiang54864.github.io/%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/index.html">
<meta property="og:site_name" content="姜将的个人博客">
<meta property="og:description" content="一、词汇向量化词汇向量化向量化是指采用计算空间的数值向量来表示离散语言文字的过程。词汇向量化是将最小语言功能单位词汇映射到计算空间的方法，并期望映射后的向量仍能保持词汇的语言学重要属性，如语义、语用或形态等。 词汇向量化方法可以分为两大类： 离散编码表示离散向量化方法是将词汇表示成整数形式的唯一标识符，或是编码成为一个整数向量。典型代表为独热编码，词汇向量即为词典数量对应的高维空间上的 0&#x2F;1 向">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg">
<meta property="article:published_time" content="2023-06-13T07:27:00.000Z">
<meta property="article:modified_time" content="2025-01-25T08:00:06.963Z">
<meta property="article:author" content="姜将">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg"><link rel="shortcut icon" href="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg"><link rel="canonical" href="http://jiang54864.github.io/%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '自然语言处理知识梳理',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-01-25 16:00:06'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = 'hidden';
    document.getElementById('loading-box').classList.remove("loaded")
  }
}

preloader.initLoading()
window.addEventListener('load',()=> { preloader.endLoading() })

if (false) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">27</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk/"><i class="fa-fw fa fa-comment"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="姜将的个人博客"><img class="site-icon" src="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg"/><span class="site-name">姜将的个人博客</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk/"><i class="fa-fw fa fa-comment"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">自然语言处理知识梳理</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-06-13T07:27:00.000Z" title="发表于 2023-06-13 15:27:00">2023-06-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-01-25T08:00:06.963Z" title="更新于 2025-01-25 16:00:06">2025-01-25</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">9.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>28分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="自然语言处理知识梳理"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="一、词汇向量化"><a href="#一、词汇向量化" class="headerlink" title="一、词汇向量化"></a>一、词汇向量化</h1><h2 id="词汇向量化"><a href="#词汇向量化" class="headerlink" title="词汇向量化"></a>词汇向量化</h2><p>向量化是指采用计算空间的数值向量来表示离散语言文字的过程。词汇向量化是<strong>将最小语言功能单位词汇映射到计算空间</strong>的方法，并期望映射后的向量仍能保持词汇的语言学重要属性，如语义、语用或形态等。</p>
<p>词汇向量化方法可以分为两大类：</p>
<p><strong>离散编码表示</strong><br>离散向量化方法是<strong>将词汇表示成整数形式的唯一标识符</strong>，<strong>或是编码成为一个整数向量</strong>。<br>典型代表为独热编码，词汇向量即为词典数量对应的高维空间上的 0/1 向量。向量维度和语义是可以理解和解释的。</p>
<p><strong>分布式表示</strong><br>词汇分布式表示或稠密向量表示是指<strong>采用固定维度的实数向量表示词汇</strong>，<strong>多个维度共同刻画词汇语义和词汇之间的关系</strong>，如向量之间的空间距离或是余弦相似度可以表示词汇的语义关系。向量空间的每个维度不直接对应特定的含义。</p>
<p>词嵌入是将词汇嵌入到一个低维连续稠密空间中，其维数相比词汇总量要小得多，每个单词对应一个实数域上的向量。随着预训练词向量的普及，在自然语言处理领域的许多情形中，词向量和词嵌入两个概念互用</p>
<p>独热编码方式（One-Hot）：每个词汇的编码即为词汇表空间的独热向量$ { {0, 1}  }^{|𝑉 |}$, 即只有当前词汇对应位置取值为“1”，其他维度均取值为零。</p>
<p>与词典密切相关，即词汇向量的大小和意义依据词典而不同，优点是向量可理解、可解释，缺点是<strong>高维稀疏且无法表示词汇之间的语义关系</strong>。</p>
<h2 id="基于统计的词汇向量方法"><a href="#基于统计的词汇向量方法" class="headerlink" title="基于统计的词汇向量方法"></a>基于统计的词汇向量方法</h2><p>依据<strong>词汇在文档中出现的上下文环境和全局语料库</strong>来构建词汇表示向量，属于分布式表示，如针对语料库中每一个文档，统计词与词、词与文档等的共现关系，形成统计矩阵来表达词汇之间语义关系，以及文档主题关系等。进而，通过矩阵分解对词汇和文本表示进行降维</p>
<p><strong>“词-文档”共现矩阵</strong>与“词-词”共现矩阵类似，其中“词-文档”共现矩阵的列对应特定文档，行对应词汇，矩阵元素是对应词在特定文档中出现的频次（或由 tf-idf 值替代）</p>
<p><img src="1686471767603.png" alt="1686471767603"></p>
<p>上述共现矩阵不妨表示为 𝑨𝑚×𝑛，其中词向量即为共现矩阵中的行向量，向量大小即预料中的文档数量或词汇表大小，具有<strong>高维稀疏</strong>的特点，而且<strong>不同维度存在较高相关性</strong>，很自然地选择是对其进行降维处理，采用潜在语义分析（Latent Semantic Analysis, LSA）方法。</p>
<p>词汇分布式表示的目标是将词向量压缩到一个低维空间 𝐷，从而可以用向量距离和余弦相似度来度量词汇之间的相关性，通常情况下向量维度远远小于词汇表大小。每一维度可以表示特定词汇性质或是几个维度联合表示一个词汇性质。</p>
<p>矩阵的奇异值分解 (Singular Value Decomposition, SVD)方法是<strong>选择最大不相关维度</strong>代表矩阵蕴含信息。根据 𝚺 中的 𝑘 个较大的奇异值 𝜎𝑖 来选择 𝑼𝑚×𝑚 中的对应列向量 𝒖𝑖 作为降维后词向量的保留维度。不足之处是初始向量维度仍然为词表大小，导致 SVD 分解计算量大，并且分解后得到的向量缺乏可解释性语义。</p>
<h2 id="预训练分布式词向量方法Word2Vec"><a href="#预训练分布式词向量方法Word2Vec" class="headerlink" title="预训练分布式词向量方法Word2Vec"></a>预训练分布式词向量方法Word2Vec</h2><p>词汇的语义分布假说核心思想是词汇的语义有不同纬度，其具体的语义与上下文相关，也与上下文语言相似。</p>
<h3 id="CBOW-模型"><a href="#CBOW-模型" class="headerlink" title="CBOW 模型"></a>CBOW 模型</h3><p>通过在海量语料库上滑动窗口，计算窗口内上下文词汇和中心词汇的语义关系，进行推断，优化词向量表示。</p>
<p><img src="1686481153720.png" alt="1686481153720"></p>
<h3 id="Skip-gram-模型"><a href="#Skip-gram-模型" class="headerlink" title="Skip-gram 模型"></a>Skip-gram 模型</h3><p>输入层：输入中心词的 One-Hot 编码形式，即𝒙𝑡 = OneHot(𝑤𝑡),</p>
<p><img src="1686481248074.png" alt="1686481248074"></p>
<h3 id="训练优化：层次化-softmax-、负采样"><a href="#训练优化：层次化-softmax-、负采样" class="headerlink" title="训练优化：层次化 softmax 、负采样"></a>训练优化：层次化 softmax 、负采样</h3><p><img src="1686481500726.png" alt="1686481500726"></p>
<p>缺点是，如果训练样本里的目标词 𝑤 是生僻词汇，那么哈夫曼树的路径很长，就会影响计算效率。</p>
<p>负采样：按照一定概率随机采样 𝐾 个词汇作为负例。</p>
<p>Word2Vec 相比于共现矩阵的统计信息<strong>更能表达词汇的语用信息</strong>，训练好的词向量可以在<strong>其他同领域任务上进行复用</strong>。缺点是它只利用了有限窗口内的上下文，而<strong>没有考虑句子全局信息</strong>。而且形成的词向量是<strong>静态向量</strong>，这种固定表示向量<strong>难以解决一词多义问题</strong>，无法根据特定场景进行动态适配和更新。</p>
<h2 id="融合全局词频信息的预训练分布式词向量-GloVe-方法"><a href="#融合全局词频信息的预训练分布式词向量-GloVe-方法" class="headerlink" title="融合全局词频信息的预训练分布式词向量 GloVe 方法"></a>融合全局词频信息的预训练分布式词向量 GloVe 方法</h2><p>GloVe 模型只训练“中心词-上下文词”共现矩阵中的非零元素，利用了<strong>词汇共现概率</strong>矩阵。另一个改进是设置了加权损失函数，以减少共现频率差异带来的问题。</p>
<p>Glove 模型的优点综合了以往全局统计矩阵和局部相关性两种建模方法的优点。容易实现并行计算，训练出来的词向量具有通用性，可以用于其他下游自然语言处理任务。缺点是相比Word2Vec 而言，需要耗费更多的内存资源来存储全局信息；训练出来的词向量是静态的，无法动态更新；同样难以解决一词多义的问题。</p>
<h2 id="分布式词向量评估"><a href="#分布式词向量评估" class="headerlink" title="分布式词向量评估"></a>分布式词向量评估</h2><p>内部评估，评价词向量是否可以反映词汇的语义信息和句法信息<br>外部评估依据词向量在下游任务中带来的效果进行评估</p>
<p>基于向量距离的最近邻方法：依据词向量之间的欧式距离。对词汇表中的每个特征进行归一化，然后计算余弦相似度</p>
<p>词汇类比分析方法：针对具有<strong>相同关系的两对词汇</strong>，分析其对应的<strong>词向量之差</strong>，计算<strong>两个差值之间余弦值</strong>，分析两对词之间的关系。两个差值越接近，表明词向量效果越能反映他们之间的关系。句法类比方法与词汇类比方法类似。</p>
<p>其他词向量方法：</p>
<p>基于字形态学的词向量化方法（汉字结构）</p>
<p>表外词词向量快速生成方法（在词向量训练过程，没有遇到该词汇）：第一类是通过多个段落训练目标新单词向量，第二类是基于子词和字符来推断目标词汇</p>
<h2 id="问题思考"><a href="#问题思考" class="headerlink" title="问题思考"></a>问题思考</h2><ol>
<li><p>预训练词向量的意义。</p>
<p>Word2Vec 相比于共现矩阵的统计信息<strong>更能表达词汇的语用信息</strong>，训练好的词向量可以在<strong>其他同领域任务上进行复用</strong>。</p>
</li>
<li><p>在基于全局矩阵分解的词向量方法中，进行 SVD 分解的意义何在？</p>
<p>向量大小具有高维稀疏的特点，而且不同维度存在较高相关性，进行 SVD 分解对其进行降维处理。得到的稠密向量不仅包含了全局统计信息，而且只选择含有重要信息的少数维度，便于后续计算和分析。</p>
</li>
<li><p>Skip-gram 和 CBOW 模型的整体思想存在怎样的关系？</p>
<p>分别使用上下文词汇预测中心词汇，以及使用中心词汇预测上下文词汇来建模语义关系，获得词汇的低维稠密词嵌入。Skip-gram 模型可以近似看做是 CBOW 模型的逆过程。</p>
</li>
<li><p>同样是基于共现矩阵，GloVe 相比于全局矩阵分解的方法有什么优势？</p>
<p>采用共现概率之比可以反映全局信息统计下的单词相关性而不是传统方法中使用共现概率本身。考虑上下文的同时引入了全局信息，综合了以往全局统计矩阵和局部相关性两种建模方法的优点。</p>
</li>
<li><p>相较于离散型词向量，分布式词向量有何优势？</p>
<p>具有更强的语义表达能力和泛化能力，同时具有较低的维度和更高的计算效率。</p>
</li>
<li><p>负采样在实际应用中十分常用，它的基本思想是什么？</p>
<p>按照一定概率随机采样 𝐾 个词汇作为负例。原来模型在词汇表 𝑉 上进行的 |𝑉| 分类问题变成了 𝐾 分类问题，就解决了每次中心词汇预测过程中需要更新输出层所有向量问题。</p>
</li>
<li><p>层次化 Softmax 方法如何实现的，有何缺点？</p>
<p>将词典中的每个词按照词频大小构建出一棵 Huffman 树，每一个词都处于这棵 Huffman 树上的某个叶子节点，叶子节点个数即为词汇表的大小。词频较大的词处于 Huffman 树相对浅的层次，词频较低的词相应的处于 Huffman 树较深层的叶子节点。这样，将原本词汇预测对应的词汇表大小 |𝑉|的多分类问题，变成了 log |𝑉| 次的二分类问题。</p>
<p>如果训练样本里的目标词 𝑤 是生僻词汇，那么哈夫曼树的路径很长，就会影响计算效率。</p>
</li>
<li><p>你认为预训练词向量方法 Word2Vec 带来主要贡献有哪些？</p>
<ol>
<li><p>提供了高效的词向量训练方法</p>
</li>
<li><p>学习到的词向量具有丰富的语义信息</p>
</li>
<li><p>可以进行向量运算，展现词语之间的关系</p>
</li>
<li><p>可以通过迁移学习在其他任务上取得好的效果</p>
</li>
</ol>
<p>为词义相似度计算、词语聚类、文本分类等任务提供了强有力的工具和基础。</p>
</li>
</ol>
<h1 id="二、语言模型"><a href="#二、语言模型" class="headerlink" title="二、语言模型"></a>二、语言模型</h1><p>语言模型：</p>
<p><img src="1686485583307.png" alt="1686485583307"></p>
<p>N-1阶马尔可夫假设:假定文本中的每个词ωi和前面的N-1个词有关，而与更前面的词无关，对应的语言模型称为N元模型(N-Gram Model)。</p>
<p>n=1：nuigram</p>
<p>n=2：bigram（前面出现的一个词）</p>
<p>n=3：trigram（前面出现的两个词）</p>
<p>如何计算概率：在大规模语料库中统计出现的频数</p>
<p><img src="1686486268315.png" alt="1686486268315"></p>
<p><img src="1686486372450.png" alt="1686486372450"></p>
<h1 id="三、文本序列标注"><a href="#三、文本序列标注" class="headerlink" title="三、文本序列标注"></a>三、文本序列标注</h1><p>文本序列标注任务是指给定文本和预定义标签集合，对于文本中的每个元素进行标记。<br>三种常用文本序列标注任务：中文分词、词性标注、命名实体识别</p>
<p>给定专家预定义的标签集合 S，给定文本 𝑥 = 𝑥1 . . . 𝑥𝑇 ，也称为观测序列，文本序列标注任务是对于输入 𝑥中每个元素 𝑥 = 𝑥𝑖，用 S 中的标签进行标注，𝑦 = 𝑦1 . . . 𝑦𝑇 , 𝑦𝑖 ∈ 𝑆 表示标注模型的输出，也称状态序列或标记序列，将观测序列映射到标记序列的问题叫做序列标注问题。</p>
<h2 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h2><p>隐马尔可夫模型（Hidden Markov Model, HMM）是一个五元组 (Q,V,p,A,E), 其中：<br>Q 是有限状态集，|Q|=N；<br>V 是观测状态的有限集，|V|=M；<br>p 是初始状态概率<br>A 是状态转移概率矩阵，其中</p>
<p><img src="1686367271709.png" alt="1686367271709"></p>
<h3 id="概率计算问题"><a href="#概率计算问题" class="headerlink" title="概率计算问题"></a>概率计算问题</h3><p>问题 1. 给定模型 𝜆 = (𝑆,𝑉, 𝑝, 𝐴, 𝐸)、观测序列 𝑋 = (𝑥1, . . . , 𝑥𝑇 ), 及状态序列 𝑌 = (y1, . . . , 𝑦𝑇 ), 计算 𝑃(𝑋,𝑌 |𝜆)。</p>
<p><img src="1686488147851.png" alt="1686488147851"></p>
<p><img src="1686488163504.png" alt="1686488163504"></p>
<p>问题 2. 给定 𝜆 = (𝑆,𝑉, 𝑝, 𝐴, 𝐸)、观测序列 𝑋 = (𝑥1, . . . , 𝑥𝑇 )，计算模型概率 𝑃(𝑋 | 𝜆)。</p>
<p><img src="1686488225939.png" alt="1686488225939"></p>
<p>此外还有动态规划算法-前向、后向，公式看不懂。</p>
<h3 id="状态估计问题"><a href="#状态估计问题" class="headerlink" title="状态估计问题"></a>状态估计问题</h3><p>问题 1. 已知模型 𝜆 = (𝑆,𝑉, 𝑝, 𝐴, 𝐸) 和观测序列 𝑋 = (𝑥1, . . . , 𝑥𝑇 ), 计算时刻 t 的状态概率 𝑃 (Yt = 𝑠𝑖 | 𝜆, 𝑋)</p>
<p><img src="1686492206344.png" alt="1686492206344"></p>
<p>chat：</p>
<ol>
<li><p>前向概率表示在给定观测序列的情况下，从模型的初始状态到达某一特定状态的概率。也可以理解为在时间步𝑡处于某一状态的概率。</p>
</li>
<li><p>后向概率表示在给定观测序列的情况下，从某一特定状态出发，到达模型的终止状态的概率。</p>
</li>
</ol>
<p>问题 2. 已知模型 𝜆 = (𝑆,𝑉, 𝑝, 𝐴, 𝐸) 和观测序列 𝑋 = (𝑥1, . . . , 𝑥𝑇 ), 求 𝑌∗ = argmax𝑌 𝑃 (y1, . . . , 𝑦𝑇 | 𝜆, 𝑋)。（使得P最大的Y）</p>
<p><img src="1686490930052.png" alt="1686490930052"></p>
<p><img src="1686631579987.png" alt="1686631579987"></p>
<h3 id="模型学习问题"><a href="#模型学习问题" class="headerlink" title="模型学习问题"></a>模型学习问题</h3><p>问题 1. 已知观测序列 𝑋 = (𝑥1, . . . , 𝑥𝑇 ) 集合，和对应的状态数据 𝑌 = (y1, . . . , 𝑦𝑇 ) 集合，估计模型 𝜆 =(𝑆,𝑉, 𝑝, 𝐴, 𝐸) 参数，使得该模型下观测序列概率 𝑃(𝑋,𝑌 | 𝜆) 最大，即 𝜆∗ = argmax𝜆 𝑃(𝑋,𝑌 | 𝜆)。</p>
<p><img src="1686492465076.png" alt="1686492465076"></p>
<p>问题 2. 已知观测序列 𝑋 = (𝑥1, . . . , 𝑥𝑇 ), 估计模型 𝜆 = (𝑆,𝑉, 𝑝, 𝐴, 𝐸) 参数，使得该模型下观测序列概率𝑃(𝑋 | 𝜆) 最大。 𝜆∗ = argmax𝜆 𝑃(𝑋 | 𝜆)</p>
<p><img src="1686492537610.png" alt="1686492537610"></p>
<p><img src="1686492645895.png" alt="1686492645895"></p>
<p><img src="1686367170768.png" alt="1686367170768"></p>
<h2 id="条件随机场"><a href="#条件随机场" class="headerlink" title="条件随机场"></a>条件随机场</h2><p><strong>马尔可夫随机场</strong>是指多个随机变量的联合概率分布,也称<strong>概率无向图模型</strong>，即对于随机向量𝑌 = (y1, . . . , 𝑦𝑁 )，联合概率分布 P(𝑌) 可以表示成无向图的形式，为 G = (𝑌, 𝐸), y𝑖 对应顶点, 随机变量 y𝑖 和 y𝑗 之间的依赖关系构成边 𝑒𝑖 𝑗, 联合概率分布 P(𝑌) 满足成对、局部或全局马尔可夫性:</p>
<p><img src="1686493186059.png" alt="1686493186059"></p>
<p>成对马尔可夫性：任意<strong>两个没有边连接的结点</strong></p>
<p><img src="1686493338887.png" alt="1686493338887"></p>
<p>局部马尔可夫性：点和它的“间接邻居”</p>
<p><img src="1686493479793.png" alt="1686493479793"></p>
<p>团：无向图 G 中的结点子集称为团当且仅当任何两个结点均有边连接<br>最大团：若 C 是无向图 G 的一个团，并且不能再加进任何一个 G 的结点使其成为一个更大的团，则称此 C 为最大团</p>
<p><img src="1686493602147.png" alt="1686493602147"></p>
<h2 id="问题思考-1"><a href="#问题思考-1" class="headerlink" title="问题思考"></a>问题思考</h2><p>文本序列标注问题的本质是什么？</p>
<p>   本质上是对于词序列进行<strong>词汇功能理解</strong>，根据语义和句法分析对词序列中的每个词进行标注。</p>
<p>文本序列标注问题和分类问题的区别和联系是什么？</p>
<p>   区别：文本序列标注问题是指对输入的文本进行逐个字符或单词的标注，分类问题则是给定一个文本，判断它属于哪一类。</p>
<p>   联系：都需要对文本进行分类，但不同之处在于分类粒度不同。文本序列标注任务更加细粒度，要求对文本的每个元素进行分类，而分类问题则是对整个文本进行分类。</p>
<p>写出隐马尔可夫模型的要素构成和三类基本问题解决方法，给出详细的推导公式。</p>
<p>隐马尔可夫模型在文本序列标注问题上的优缺点有哪些？</p>
<p>优点：有了齐次马尔科夫假设和观测独立假设及其假设成立的场景，可以大大<strong>简化 P(Y|X) 的计算</strong>。<br>缺点：序列标注问题和观察序列的长度，单词的上下文等信息相关，HMM只依赖于每一个状态和它对应的观察对象，<strong>不能全面反映现实情况</strong>。隐马模型学到的是<strong>状态和观察序列的联合分布 P(Y,X)</strong>，而预测问题中，我们需要的是条件概率 <strong>P(Y|X)</strong>，目标函数和预测目标函数不完全匹配</p>
<p>写出条件随机场模型的要素构成，以及模型针对<strong>实例评分问题</strong>和<strong>状态解码问题</strong>的详细推导公式。</p>
<p>要素构成包括：</p>
<ol>
<li><p>输入序列（观测序列）：𝑋 = {𝑥₁, 𝑥₂, …, 𝑥ₙ}，表示观测序列的输入。</p>
</li>
<li><p>输出序列（标记序列）：𝑌 = {𝑦₁, 𝑦₂, …, 𝑦ₙ}，表示对应于输入序列的标记序列的输出。</p>
</li>
<li><p>特征函数：𝑓(𝑦ₓ₊₁, 𝑦ₓ, 𝑋, 𝑥)，用于定义在输入序列和输出序列上的特征。特征函数是根据具体问题定义的，可以包括观测值和标记之间的关系、相邻标记之间的关系等。</p>
</li>
<li><p>参数：𝜆，用于表示特征函数的权重。</p>
</li>
</ol>
<p>实例评分问题：<br>实例评分问题是指给定输入序列𝑋和输出序列𝑌，计算模型对于该实例的得分𝑠(𝑋, 𝑌)。得分越高表示模型认为该实例的输出序列𝑌更合理。</p>
<p>CRF模型对于给定的输入序列𝑋和输出序列𝑌的得分计算公式为：</p>
<p>𝑠(𝑋, 𝑌) = ∑<em>{𝑛=1}^{𝑁} ∑</em>{𝑘=1}^{𝐾} 𝜆ₖ * 𝑓ₖ(𝑦ₙ₊₁=𝑘, 𝑦ₙ=𝑦, 𝑋, 𝑥ₙ)</p>
<p>其中𝑁是输入序列的长度，𝐾是输出序列的标记个数，𝑦ₙ₊₁表示第𝑛+1个位置的标记，𝑦ₙ表示第𝑛个位置的标记，𝑥ₙ表示第𝑛个位置的观测值，𝑓ₖ是特征函数，𝜆ₖ是对应的权重。</p>
<p>状态解码问题：<br>状态解码问题是指给定输入序列𝑋，找到最可能的输出序列𝑌，即找到使得模型得分𝑠(𝑋, 𝑌)最大化的输出序列。</p>
<p>CRF模型中使用维特比算法（Viterbi algorithm）来解决状态解码问题。维特比算法使用动态规划的方法，通过计算局部最优解来逐步构建全局最优解。</p>
<p>定义局部最优解𝑉(𝑛, 𝑘)为在位置𝑛处标记为𝑘的输出序列的最大得</p>
<p>分，𝑉(𝑛, 𝑘)表示从位置1到位置𝑛，并且位置𝑛的标记为𝑘的输出序列的最大得分。则局部最优解的递推公式为：</p>
<p>   𝑉(𝑛, 𝑘) = 𝑓ₖ(𝑦ₙ₊₁=𝑘, 𝑦ₙ=𝑦, 𝑋, 𝑥ₙ) + max_{𝑘’} {𝑉(𝑛-1, 𝑘’) + 𝜆ₖ’ * 𝑓ₖ’(𝑦ₙ₊₁=𝑘, 𝑦ₙ=𝑦’, 𝑋, 𝑥ₙ)}</p>
<p>   其中𝑘’是位置𝑛-1处标记的取值，𝑦’表示位置𝑛-1处的标记，𝑓ₖ’是特征函数，𝜆ₖ’是对应的权重。</p>
<p>   最终，状态解码问题可以通过回溯找到使得得分最大的输出序列𝑌。</p>
<p>条件随机场模型在文本序列标注问题上的优缺点有哪些？</p>
<p>优点：没有严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活，适用场景更宽泛。<br>缺点：比隐马模型训练代价大、复杂度高。</p>
<p>隐马尔可夫模型和条件随机场模型的区别是什么？</p>
<p>条件随机场模型减弱了 HMM 中两个假设，没有严格的独立性假设条件</p>
<p>针对文本序列标注问题，CRF 和 LSTM 如何结合使用？</p>
<p>LSTM用于提取文本序列的上下文信息，而CRF用于建模标记序列的依赖关系，从而解决文本序列标注问题。 </p>
<p>扩展思考：文本序列标注有个基本假设，即句子是序列结构，这一假设符合真实语言情况吗？文本序列标注任务对所有自然语言处理任务均是必要的吗？举例说明为什么。</p>
<p>句子是序列结构的假设在大多数情况下符合真实语言情况。在某些非常规的语言或语言变体中，句子的结构可能不符合严格的序列模式。</p>
<p>例如情感分类或垃圾邮件过滤，文本生成任务，如文本摘要或机器翻译，不是必要的。</p>
<h1 id="四、中文分词"><a href="#四、中文分词" class="headerlink" title="四、中文分词"></a>四、中文分词</h1><p>中文分词，即解决中文序列的词汇切分问题。</p>
<p>将字作为分词的最小单位，其粒度太小，无法表达完整意涵，而句子的粒度太大，承载的信息量多，很难复用。因此，文本理解通常以词作为基本单位，即通过分词把连续的汉字分隔成具有独立意义的词汇。</p>
<p>分词的挑战：</p>
<p>1.语言的固有歧义：交集型歧义：是指对于给定的字符序列，可以对应于多个具有字符重叠关系的常用词汇。组合型歧义：是指对于给定的字符序列，可以有不同的分割形式，分别对应于不同的意涵。真歧义：是指两种分词方式均是合理和常见的情况。</p>
<p>2.语言的开放性和动态性（不断出现新词）</p>
<p>3.语言学中的词汇规范和计算科学中的量化指标缺少直接对应</p>
<p>评估：</p>
<p>人工评价：专家评价。<br>外部评价：依据下游任务性能评价。<br><strong>精准率：预测正确分词占所有预测分词</strong>结果的比值。<br>召回率：预测正确分词占真实分词结果的比值。<br>F-score：精准率和召回率两者的调和平均值。<br><strong>IV Recall：正确识别出的词典中的词占标准分词中在词典中出现的词</strong>的比例。<br>OOV Recall：正确识别出的未登录词占标准分词中未在词典中出现的词的比例</p>
<h2 id="基于规则的分词算法"><a href="#基于规则的分词算法" class="headerlink" title="基于规则的分词算法"></a>基于规则的分词算法</h2><p>前向最大匹配算法自左至右进行搜索，寻找词典中的最长匹配词汇。设定最大长度如 𝐿，以限定算法复杂度。如果没有找到，就缩短长度继续寻找，直到找到字典中的词或者单字。后向最大匹配算法的思想与前向最大匹配算法类似，只不过是方向相反，从后向前进行匹配。</p>
<p>中文 90% 左右句子，正向最大匹配法和反向最大匹配法完全重合且正确，9.0% 的句子两种切分得到的结果不一样，其中只有一个是正确的（歧义检测成功），只有不到 1.0% 的句子，使用正向最大匹配法和逆向最大匹配法的切分虽重合却是错的，或者正向最大匹配法和逆向最大匹配法切分不同但两个都不对（歧义检测失败）。</p>
<p>双向最大匹配算法分别执行前向最大匹配算法和后向最大匹配算法，根据各自分词结果，采用汇总策略进行判断。例如:<br>如果前后向算法的匹配结果相同，则任一分词结果即为最终分词结果；<br>如果两者匹配结果不同，且分词词数不同，则选择<strong>分词数较少</strong>的作为分词结果；<br>结果分词词数相同，则选择<strong>单字较少</strong>的作为分词结果；<br>如果词数相同且单字数也相同，则返回任意一个。</p>
<h2 id="基于统计的分词算法"><a href="#基于统计的分词算法" class="headerlink" title="基于统计的分词算法"></a>基于统计的分词算法</h2><p>一种朴素的无监督评价方式是依据海量数据统计结果，选择概率最大的分词方案。</p>
<p>给定字符串 𝐶 = 𝐶1, 𝐶2, …, 𝐶𝑛，对应多个切分方案，构成集合 G，其中, 切分方案 𝑆 ∈ 𝐺 对应的词汇序列为：𝑆 = 𝑤1, 𝑤2, .., 𝑤𝑙，满足 𝑙 &lt;= 𝑛。分词任务的目标转化成下式：</p>
<p><img src="1686549933233.png" alt="1686549933233"></p>
<p><img src="1686549975839.png" alt="1686549975839"></p>
<p>N 元模型在一定程度上是分词精度和计算复杂度的一种权衡，当 N 在一定范围内时，N 越大，精度越高，计算量越大。现实中，当 N 超过一定范围时，当前位置词的划分，可能已经不受到前面一些分词的影响，更多的信息反而成了噪音。</p>
<p>新词的相关统计指标：</p>
<p><strong>内部凝固度</strong>衡量词搭配是否合理。点互信息 PMI被用来度量词搭配的凝固度。若 PMI 高，即两个词共现的频率远大于两个词自由拼接的乘积概率，则说明这两个词搭配更为合理一些：$𝑃𝑀𝐼(𝑥, 𝑦) = log\frac{𝑃(𝑥, 𝑦)}<br>{𝑃(𝑥)𝑃(𝑦)}$<br><strong>自由运用程度</strong>衡量一个词的左邻字与右邻字的丰富程度。自由运用程度反映了文本片段外部的稳定性，外部自由度越大，该文本片段的稳定性就越高。定义文本跨度 𝑤 为候选词汇，其左字符和右字符分别为 𝑙(𝑤)、𝑟(𝑤)，自由运用程度的定义如下：<br>𝑓𝑟𝑒𝑒(𝑤) = 𝑚𝑖𝑛{𝐸𝑛𝑡𝑟𝑜𝑝𝑦(𝑙(𝑤)), 𝐸𝑛𝑡𝑟𝑜𝑝𝑦(𝑟(𝑤))}，即左右字符熵的小者</p>
<h2 id="基于神经网络的BiLSTM-CRF"><a href="#基于神经网络的BiLSTM-CRF" class="headerlink" title="基于神经网络的BiLSTM+CRF"></a>基于神经网络的BiLSTM+CRF</h2><p>双向长短时网络 (Bi-Long Short Term Memory, BiLSTM) 是基础的序列编码网络，分别从句子顺序和句子逆序两个方向同时编码文本信息，使得编码同时包含了每个词汇的历史信息和未来信息，有利于对当前词进行标注。</p>
<p>BiLSTM+CRF:</p>
<p><img src="1686550746447.png" alt="1686550746447"></p>
<p>1.问题描述<br>输入：文本序列 𝑋 = (𝑥1, 𝑥2, …, 𝑥𝑛)，其中 n 是序列包含的字符数，𝑥𝑖 代表 X 中的第 i 个字。<br>输出：𝑦 = (𝑦1, 𝑦2, …, 𝑦𝑛)，其中，y 是对输入的句子 X 进行预测获得的，𝑦𝑖 ∈ {𝐵, 𝑀, 𝐸, 𝑆} 是 𝑥𝑖 的预测标签。<br>2.基本结构<br>建立输入 X 和标签 y 之间的概率关联关系，主要包含两部分，一是标签之间的关系，即从时间步 𝑖 − 1 标签 𝑦𝑖 转移时间步 𝑡 标签 𝑦𝑖+1 的规律；二是每个时间步标签 𝑦𝑖 和字符 𝑥𝑖 之间映射规律，分别表示为下列𝐴𝑦𝑖,𝑦𝑖+1 和 𝑃𝑥𝑖,𝑦𝑖 两个存储变量。</p>
<p><img src="1686551244279.png" alt="1686551244279"></p>
<p>其中，P 是 BiLSTM 网络部分的输出，作为完整文本序列 𝑋 的编码信息，𝑃𝑖, 𝑗 则代表句子中第 i 个单字获得 j 标记的分数。𝐴𝑖, 𝑗 代表从标记 i 到标记 j 的转移分数。</p>
<p>3.标签预测函数<br>在文本序列上的标签预测函数采用 softmax，产生序列 y 的概率。其中 𝑌𝑋 代表句子 X 的所有可能的标签序列：</p>
<p><img src="1686551468586.png" alt="1686551468586"></p>
<p>4.优化目标<br>训练目标是将序列中字符对应正确标签概率对数最大化，即模型激励神经网络产生正确的输出序列。</p>
<p><img src="1686551486191.png" alt="1686551486191"></p>
<p>5.模型推理<br>选择最大概率$y^*$作为序列中每个字符的预测标签，采用动态规划算法存储中间结果，可以加速算法效率。</p>
<p><img src="1686551497161.png" alt="1686551497161"></p>
<p>中文分词工具：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工具</th>
<th>优点</th>
</tr>
</thead>
<tbody>
<tr>
<td>HanLP</td>
<td>功能<strong>全面</strong>，性能较高，适用于多种任务场景</td>
</tr>
<tr>
<td>SnowNLP</td>
<td>面向中文自然语言处理，涉及任务较多</td>
</tr>
<tr>
<td>LTP</td>
<td>可以根据需求自由选择<strong>不同速度和指标的模型</strong>进行任务处理</td>
</tr>
<tr>
<td>NLPIR</td>
<td>分词速度快</td>
</tr>
<tr>
<td>THULAC</td>
<td>速度较快</td>
</tr>
</tbody>
</table>
</div>
<h2 id="问题思考-2"><a href="#问题思考-2" class="headerlink" title="问题思考"></a>问题思考</h2><ol>
<li><p>什么是中文分词？为什么要进行中文分词？如何评估中文分词结果？</p>
<p>解决中文序列的词汇切分问题；文本理解通常以词作为基本单位；</p>
</li>
<li><p>中文分词问题有哪些挑战？详细说明。</p>
</li>
<li><p>分别简述前向最大匹配算法和后向最大匹配算法，讨论两者在什么情况下会产生相同的分词结果，在什么情况下会产生不同的分词结果? 如何解决这种分歧？</p>
</li>
<li><p>基于统计的新词发现有哪些评价指标？</p>
</li>
<li><p>详细说明 BiLSTM-CRF 模型在分词任务上的技术细节，给出公式。</p>
</li>
<li><p>中文分词有哪些相关工具？各有什么优点和缺点？</p>
</li>
</ol>
<h1 id="五、命名实体识别与关系抽取"><a href="#五、命名实体识别与关系抽取" class="headerlink" title="五、命名实体识别与关系抽取"></a>五、命名实体识别与关系抽取</h1><p><img src="1686552057114.png" alt="1686552057114"></p>
<p>命名实体识别(Named Entity Recognition, NER) :从文本中识别预定义类型实体，并定位到该标识符，属于<strong>序列标注任务</strong></p>
<p>NER属于分类任务。在信息抽取、信息检索、问答系统、机器翻译等任务中有重要应用</p>
<p>方法：</p>
<p>1.基于规则的方法</p>
<p><img src="1686555762872.png" alt="1686555762872"></p>
<p>2.基于特征的监督学习方法</p>
<p><img src="1686555820541.png" alt="1686555820541"></p>
<p>3.基于深度学习的方法</p>
<p><img src="1686555850578.png" alt="1686555850578"></p>
<p>工具：spacy、NLTK</p>
<p>关系抽取：找到实体与实体之间有意义的关系，通常表示为三元组：&lt;头实体，关系，尾实体&gt;</p>
<p>传统方法：流水线式抽取，分为实体识别和抽取关系两个步骤；联合抽取，一步到位</p>
<p>挑战：<br>大部分现有的关系抽取模型基于有监督学习，依赖于大量的有<strong>标注数据</strong>，数据量不足的情况下性能无法得到保证<br>不同领域中关系抽取任务的数据集和抽取目标不通用，模型迁移困难<br>现有利用<strong>远程监督</strong>的方法获取到的数据含有较大的噪音，不利于模型的训练和收敛</p>
<p>作用：服务于创建结构化知识库，构建知识图谱，支持上层应用：问答、搜索、推理等</p>
<h1 id="六、句法分析"><a href="#六、句法分析" class="headerlink" title="六、句法分析"></a>六、句法分析</h1><p>句法分析是研究语言中单词和短语的组合方式，学习词汇构成句子的结构规律</p>
<p>句子可以细分为信息类、指令类、表达类和承诺类</p>
<p>句法分析技术主要分为两类：一类是<strong>转换生成语法理论</strong>，面向<strong>句子成分</strong>，分析主谓宾定状补的句法结构，并分析各成分之间的关系。另一类是<strong>依存句法分析理论</strong>，分析<strong>词汇之间的依存关系</strong>，如并列、从属、比较、递进等。</p>
<p>乔姆斯基重新确立了人们的语言观：<br>1) 语言的<strong>无限性</strong>：语言是无限的句子集合，句子长度有限，结构成分有限。<br>2) 语言的<strong>离散型</strong>：以有限的符号来构成无限的符号序列，任何<strong>连续的话语都可以切换为更为小的片段</strong>。<br>3) 语言的<strong>层次性</strong>：句子是在生成语法中被确定的，这种句子的线性特征中蕴含着层次关系</p>
<h2 id="基于上下文无关文法的转换生成语法理论"><a href="#基于上下文无关文法的转换生成语法理论" class="headerlink" title="基于上下文无关文法的转换生成语法理论"></a>基于上下文无关文法的转换生成语法理论</h2><p>自底向上分析方法：首先为每一个单词指定一个词性类别如名词、动词等。然后，依据特定语法，将词语组织建成短语 ；使用短语递归建立更复杂的结构，构成长句子。</p>
<p>强等价：两个语法生成相同的符号串集合，而且它们对于<strong>每一个句子都指派同样的短语结构</strong><br>弱等价：两个语法生成相同的符号串集合，但是<strong>不给每一个句子都指派同样的短语结构</strong></p>
<p>乔姆斯基范式文法 (CNF）：𝐴 →𝐵𝐶，𝐴 →𝛼，𝑆 →𝜖。如果每一个规则的右部是两个非终极符号，或是一个终极符号，那么这个上下文无关文法就是乔姆斯基范式的（CNF）。</p>
<h2 id="CYK算法"><a href="#CYK算法" class="headerlink" title="CYK算法"></a>CYK算法</h2><p>二维矩阵 𝑃，其元素 𝑃(𝑖, 𝑗) 表示输入句子位置 𝑖 开始的跨度大小为 𝑗（即 [i,i+j]）的<strong>所有可能形成的短语的非终结符</strong>的集合，横坐标 𝑖 代表输入文本序列的当前位置，即规则覆盖文本的跨度左侧的第一个词的位置，纵坐标 𝑗 代表着跨度，即包含的字符数。</p>
<p><img src="1686558890308.png" alt="1686558890308"></p>
<p>核心：对每一条规则 𝐴 → 𝐵𝐶，如果 𝐵 ∈ 𝑃(𝑖, 𝑘) 且 𝐶 ∈ 𝑃(𝑖 + 𝑘, 𝑗 − 𝑘), 那么将非终结符 𝐴 加入集合 𝑃(𝑖, 𝑗)</p>
<h2 id="概率上下文无关文法"><a href="#概率上下文无关文法" class="headerlink" title="概率上下文无关文法"></a>概率上下文无关文法</h2><p>一个概率上下文无关语法 𝐺 是一个四元组：G = ( Σ, N, S, PR)，其中，Σ 表示终结符合 (terminal symbols)，𝑁 表示非终结符的集合 (non-terminals)，𝑁 ∩ Σ = ∅，𝑆 ∈ 𝑁 表示特殊的开始符号，𝑃𝑅 表示概率生成规则集合，每一个生成规则的形式为 <strong>(𝐴 → 𝛽, 𝑝)</strong>，其中 𝐴 ∈ 𝑁 是非终极符号，𝛽 是由 (Σ ∪ 𝑁) 集合中的符号构成的符号串，<strong>𝑝 ∈ [0, 1] 表示概率</strong>。</p>
<p>向内算法： 𝐴 → 𝐵𝐶 的内部概率 𝛼𝑖, 𝑗 (𝐴) 是指该规则生成词串 𝑊 中 𝑤𝑖, …, 𝑤 𝑗 的概率，其中 𝐵 和 𝐶 进一步推出词串 𝑊 中的单词字串 𝑤𝑖, …, 𝑤𝑘 和词串 𝑊 中的单词字串 𝑤𝑘+1, …, 𝑤 𝑗。</p>
<p><img src="1686560191280.png" alt="1686560191280"></p>
<p><img src="1686560284588.png" alt="1686560284588"></p>
<p>向内向外算法：非终结符 𝐴 的外部概率为 𝛽𝑖, 𝑗 (𝐴)，用于记录<strong>串 𝑤𝑖…𝑤 𝑗 之外的生成概率</strong>。假设 𝐴 对应的父节点为 𝐶，则有两种情况。一种是 𝐴 在父节点对应的生成规则的左边，即 𝐶 → 𝐴𝐵；一种是 𝐴 在父节点对应的生成规则的右边，即 𝐶 → 𝐵𝐴，其中 𝐵 分别对应于生成词串 𝑤 𝑗+1…𝑤𝑘 和 𝑤ℎ…𝑤𝑖−1。</p>
<p><img src="1686560501026.png" alt="1686560501026"></p>
<p><img src="1686560521450.png" alt="1686560521450"></p>
<p><strong>计算最大概率句法的维特比算法</strong></p>
<p>首先引入变量 𝛿𝑖, 𝑗 (𝐴)，表示从非终结符 𝐴 推导出 𝑊 中词串 𝑤𝑖…𝑤 𝑗 对应的最大概率。向内变量 𝛼𝑖 𝑗 (𝐴) 计算从 𝐴 产生出词串 𝑤𝑖…𝑤 𝑗的所有可能推导的概率之和。</p>
<p><img src="1686565163661.png" alt="1686565163661"></p>
<p>𝑖 表示字串的起始位置，𝑗 表示字串的跨度。在循环中，计算始于位置 𝑖 的所有可能跨度即 𝑗 ∈ [1..𝑛] 的所有可能组合（即词串 [𝑖..𝑖 + 𝑗]）构成的产生式 𝐴 → 𝐵𝐶，其中 𝐵 覆盖 𝑤𝑖…𝑤𝑘，𝐶覆盖 𝑤𝑘+1…𝑤𝑖+𝑗，其生成式分别对应于概率 𝛿𝑖,𝑘(𝐵) 和 𝛿𝑘+1,𝑖+ 𝑗 (𝐶)。<strong>Δ 变量用来记录分析过程的历史</strong>。</p>
<p><strong>基于期望最大化算法的模型优化</strong>（课件里没有，简单看看即可）</p>
<p>期望最大化算法即 EM 算法，是一种迭代算法，采用最大似然估计或极大后验估计优化模型参数，主要分为期望计算步骤，即 E 步骤，和最大化概率计算步骤，即 M 步骤。</p>
<p><img src="1686565560976.png" alt="1686565560976"></p>
<p><img src="1686565630943.png" alt="1686565630943"></p>
<h2 id="依存句法理论"><a href="#依存句法理论" class="headerlink" title="依存句法理论"></a>依存句法理论</h2><p>如果一个词修饰另一个词，则称修饰词为从属词，被修饰的词语称为支配词 ，两者之间的语法关系称为依存关系。承担谓语功能的动词称为句子中心。被中心动词支配的成分称为行动元，一个动词能够支配的行动元个数就是它的价。</p>
<p>依存句法树4 个约束性的公理:根节点唯一性、整体连通性、无环和投射性</p>
<ol>
<li>一个句子中只有一个成分是独立的，即核心成分。</li>
<li>除了核心成分外的其它成分均直接依存于某一成分。</li>
<li>依存为二元关系，任何一个成分不能依存两个或两个以上的成分。</li>
<li>如果 A 成分直接依存于 B 成分，C 成分在句中位于 A 和 B 之间，则 C或者直接依存于 B，或者直接依存于 A 和 B 之间的某一成分。</li>
</ol>
<p>三种方法：基于规则的方法、基于统计的方法、基于深度学习的方法</p>
<p>基于状态转移的依存句法分析：<br>句法分析器主要包含以下四个组件：<br>堆栈 𝑆：用于存放<strong>已经处理但是尚未匹配</strong>依存关系的词汇，即尚未找到父节点。初始时栈中只有 root 元素，栈顶靠右。<br>缓冲区 𝛽：存放输入句子空间，第一个元素在最左边，按照句子的顺序开始使用。<br>依存关系集合：存放依存关系的集合 A。<br>转移操作集合：存放专家定义好的转移操作。</p>
<p>句法分析器有三种 transition 操作：<br>SHIFT: 当缓冲区 𝛽 不空时，即 𝛽 的长度大于等于 1，将单词 𝑤𝑖 从 𝛽 中出队，𝑤𝑖 进入堆栈 𝑆。LEFT-ARC(r): 在堆栈 𝑆 中增加一条 𝑤𝑖 到 𝑤 𝑗 的依存边 (𝑤𝑖, 𝑤 𝑗)，依存关系是 r,𝑤 𝑗 出栈。RIGHT-ARC(r): 在堆栈 𝑆 中增加一条 𝑤 𝑗 到 𝑤𝑖 的依存边 (𝑤 𝑗, 𝑤𝑖)，依存关系是 r,𝑤𝑖 出栈。如果最后堆栈 𝑆 中只有根节点 root 而缓冲区 𝛽 清空，则完成依存句法的匹配。</p>
<p>基于转移的依存句法分析只能建立投射性树, 如果要处理非投射性问题的话，解决方法有以下几种：</p>
<ol>
<li>直接默认不存在非投射性的依存关系</li>
<li>使用一种允许投射性表示的依存框架</li>
<li>预处理，找出非投射性依存并处理</li>
<li>算法中加入新的操作</li>
<li>使用那些没有对投射性有限制的句法分析机制</li>
</ol>
<p>基于图的依存句法分析：为每条边分配一个权重或概率，从中选择构成最大生成树MST的边集合。</p>
<p>依据语言学家制定的句法标记规范，将大量句子人工分解为树形结构，形成了一种语料库，称为树库。</p>
<p>评价指标：</p>
<p><img src="1686566969212.png" alt="1686566969212"></p>
<p>工具：StanfordNLP、HanLP、LTP </p>
<h1 id="七、文本分类"><a href="#七、文本分类" class="headerlink" title="七、文本分类"></a>七、文本分类</h1><p>给定训练集{𝑥!, 𝑦!}，𝑥表示输入文本，即词汇、句子或文档，𝑦表示是文本𝑥的标签，表明𝑥所属类别。分类问题目标是将𝑥!映射到𝑦!，即预测文本所属类别。</p>
<p>文本分类的应用：语义分析、新闻分类、主题标记、问答系统、自然语言推断、对话行为分类、关系分类、无意义文本过滤等</p>
<p>评价指标<br>单一标签（二分类及多分类）<br>n 准确率（Accuracy）和错误率<br>n 查准率、精准率（Precision）召回率（recall）和F1<br>n 精准匹配 EM(Exact Match)<br>n 平均倒数排名 MRR（Mean Reciprocal Rank）<br>多标签分类<br>n Micro-F1：考虑所有标签的总体准确性和召回率的度量，均衡样本重要性</p>
<p><img src="1686567861087.png" alt="1686567861087"></p>
<p>n Macro-F1：计算所有标签的平均F1，均衡类别重要性</p>
<p>传统机器学习分类<br>特征提取：词袋模型（例如TF-IDF）、词嵌入<br>分类器：<br>朴素贝叶斯NB：使用先验概率来计算后验概率<br>K-近邻：通过查找样本数量最多的类别对未标记样本进行分类𝑘最近的样本<br>SVM：最大化超平面与两类训练集之间的距离<br>决策树：树构建和树修剪</p>
<p>深度学习分类：ReNN、MLP、CNN、RNN、Attention、Transformer..</p>
<p>CNN：将输入文本的词向量拼接成矩阵。然后矩阵被送入卷积层</p>
<p>RNN：将嵌入的字向量逐个送入RNN单元</p>
<h1 id="八、机器翻译"><a href="#八、机器翻译" class="headerlink" title="八、机器翻译"></a>八、机器翻译</h1><p>⼴义上来说，翻译是指把⼀个事物转化为另⼀个事物的过程。</p>
<h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><p>评估的重要性：给模型反馈信息，指导模型的⽣成；对⽣成结果判定，判定模型优劣</p>
<p>BLEU-n</p>
<p><img src="1686571263694.png" alt="1686571263694"></p>
<p><img src="1686571312039.png" alt="1686571312039"></p>
<p><img src="1686571410670.png" alt="1686571410670"></p>
<p>NIST</p>
<p><img src="1686571666431.png" alt="1686571666431"></p>
<p>ROUGE-n</p>
<p><img src="1686571728155.png" alt="1686571728155"></p>
<p><img src="1686571813842.png" alt="1686571813842"></p>
<p>神经方法：构造训练数据集、使用多个自动化评估方法对 𝑠打分标注、预训练、微调</p>
<h2 id="统计机器翻译"><a href="#统计机器翻译" class="headerlink" title="统计机器翻译"></a>统计机器翻译</h2><p>核心想法：从数据中学习概率模型</p>
<p><img src="1686572223869.png" alt="1686572223869"></p>
<p>对齐 Alignment是翻译句子中特定词语之间的对应关系，可以多对一、一对多、多对多</p>
<p><img src="1686572328911.png" alt="1686572328911"></p>
<h2 id="神经机器翻译方法NMT"><a href="#神经机器翻译方法NMT" class="headerlink" title="神经机器翻译方法NMT"></a>神经机器翻译方法NMT</h2><p>基础机构参考深度学习中的seq2seq</p>
<p><img src="1686572608537.png" alt="1686572608537"></p>
<p>神经机器翻译的优点<br>更加流畅，能够更好地运用上下文信息，整体的翻译效果更好；<br>能够端到端地训练，不需要特征工程，消耗更少的人力物力；<br>一种模型可以运用到多种语言上。</p>
<p>简单的序列到序列模型的不足<br>整个句子<strong>编码到一个向量</strong>里可能会有<strong>信息丢失</strong>；<br>源语单词与目标语单词之间<strong>缺少对应</strong>。某种意义上讲，一个目标语单词的生成无法区分不同源语单词的贡献；<br>源语词和目标语词的<strong>对应并不是均匀的</strong>，甚至非常稀疏，比如，一些短语的生成仅依赖于源文中的少数词；<br>一些<strong>常识、成语等难以准确翻译</strong>。</p>
<p><img src="1686572681607.png" alt="1686572681607"></p>
<p>在Encoder中最后一个时间步的隐藏层难以涵盖整个源语句的信息，通过注意力机制，解码器在生成一个目标语单词时可以选择性地关注源句子中不同单词。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://jiang54864.github.io">姜将</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://jiang54864.github.io/%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/">http://jiang54864.github.io/%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://jiang54864.github.io" target="_blank">姜将的个人博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/%E7%BC%96%E7%A8%8B/C-%E5%9B%9E%E9%A1%BE/" title="C++回顾"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">C++回顾</div></div></a></div><div class="next-post pull-right"><a href="/%E7%AC%94%E8%AE%B0/%E4%BC%97%E6%99%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E7%BD%91%E7%BB%9C%E5%8C%96%E4%BA%A7%E4%B8%9A%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/" title="众智科学与网络化产业知识梳理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">众智科学与网络化产业知识梳理</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/others/%E4%B8%8EChatgpt%E8%81%8A%E8%81%8A%E5%A4%A9/" title="与Chatgpt聊聊天"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-06</div><div class="title">与Chatgpt聊聊天</div></div></a></div><div><a href="/%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E7%8E%B0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" title="《深度学习入门：基于python的理论与实现》读书笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-08</div><div class="title">《深度学习入门：基于python的理论与实现》读书笔记</div></div></a></div><div><a href="/%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%A4%84%E7%90%86%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" title="《深度学习进阶：自然语言处理处理》读书笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-08</div><div class="title">《深度学习进阶：自然语言处理处理》读书笔记</div></div></a></div><div><a href="/%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%B4%E7%90%86/" title="机器学习基础整理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-19</div><div class="title">机器学习基础整理</div></div></a></div><div><a href="/%E5%AE%9E%E9%AA%8C/%E5%80%9F%E5%8A%A9ollama%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8Cqwen2-5/" title="借助ollama本地运行各种开源大模型"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-12</div><div class="title">借助ollama本地运行各种开源大模型</div></div></a></div><div><a href="/%E5%AE%9E%E9%AA%8C/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E8%AE%BA%E9%A2%84%E6%B5%8B%E8%B4%B7%E6%AC%BE%E8%BF%9D%E7%BA%A6/" title="贝叶斯决策论预测贷款违约"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-11</div><div class="title">贝叶斯决策论预测贷款违约</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">姜将</div><div class="author-info__description">记录学习中的知识与收获</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">27</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/JIANG54864"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/JIANG54864" target="_blank" title="我的Github主页"><i class="fab fa-github"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=2792663690&amp;website=www.oicqzone.com" target="_blank" title="点击添加我的QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="https://www.helloimg.com/i/2025/01/25/6794f016e9b99.png" target="_blank" title="点击添加我的微信"><i class="fab fa-weixin"></i></a><a class="social-icon" href="https://space.bilibili.com/262150061" target="_blank" title="我的Bilibili主页"><i class="fab fa-bilibili"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客网站！若有需要，可通过上方联系方式找到我。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E8%AF%8D%E6%B1%87%E5%90%91%E9%87%8F%E5%8C%96"><span class="toc-number">1.</span> <span class="toc-text">一、词汇向量化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%8D%E6%B1%87%E5%90%91%E9%87%8F%E5%8C%96"><span class="toc-number">1.1.</span> <span class="toc-text">词汇向量化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%9A%84%E8%AF%8D%E6%B1%87%E5%90%91%E9%87%8F%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.</span> <span class="toc-text">基于统计的词汇向量方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E5%88%86%E5%B8%83%E5%BC%8F%E8%AF%8D%E5%90%91%E9%87%8F%E6%96%B9%E6%B3%95Word2Vec"><span class="toc-number">1.3.</span> <span class="toc-text">预训练分布式词向量方法Word2Vec</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CBOW-%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.1.</span> <span class="toc-text">CBOW 模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Skip-gram-%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.2.</span> <span class="toc-text">Skip-gram 模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E4%BC%98%E5%8C%96%EF%BC%9A%E5%B1%82%E6%AC%A1%E5%8C%96-softmax-%E3%80%81%E8%B4%9F%E9%87%87%E6%A0%B7"><span class="toc-number">1.3.3.</span> <span class="toc-text">训练优化：层次化 softmax 、负采样</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%9E%8D%E5%90%88%E5%85%A8%E5%B1%80%E8%AF%8D%E9%A2%91%E4%BF%A1%E6%81%AF%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E5%88%86%E5%B8%83%E5%BC%8F%E8%AF%8D%E5%90%91%E9%87%8F-GloVe-%E6%96%B9%E6%B3%95"><span class="toc-number">1.4.</span> <span class="toc-text">融合全局词频信息的预训练分布式词向量 GloVe 方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E8%AF%8D%E5%90%91%E9%87%8F%E8%AF%84%E4%BC%B0"><span class="toc-number">1.5.</span> <span class="toc-text">分布式词向量评估</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E6%80%9D%E8%80%83"><span class="toc-number">1.6.</span> <span class="toc-text">问题思考</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text">二、语言模型</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E6%96%87%E6%9C%AC%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8"><span class="toc-number">3.</span> <span class="toc-text">三、文本序列标注</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#HMM"><span class="toc-number">3.1.</span> <span class="toc-text">HMM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E7%8E%87%E8%AE%A1%E7%AE%97%E9%97%AE%E9%A2%98"><span class="toc-number">3.1.1.</span> <span class="toc-text">概率计算问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8A%B6%E6%80%81%E4%BC%B0%E8%AE%A1%E9%97%AE%E9%A2%98"><span class="toc-number">3.1.2.</span> <span class="toc-text">状态估计问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98"><span class="toc-number">3.1.3.</span> <span class="toc-text">模型学习问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA"><span class="toc-number">3.2.</span> <span class="toc-text">条件随机场</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E6%80%9D%E8%80%83-1"><span class="toc-number">3.3.</span> <span class="toc-text">问题思考</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D"><span class="toc-number">4.</span> <span class="toc-text">四、中文分词</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95"><span class="toc-number">4.1.</span> <span class="toc-text">基于规则的分词算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%9A%84%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95"><span class="toc-number">4.2.</span> <span class="toc-text">基于统计的分词算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84BiLSTM-CRF"><span class="toc-number">4.3.</span> <span class="toc-text">基于神经网络的BiLSTM+CRF</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E6%80%9D%E8%80%83-2"><span class="toc-number">4.4.</span> <span class="toc-text">问题思考</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E4%B8%8E%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96"><span class="toc-number">5.</span> <span class="toc-text">五、命名实体识别与关系抽取</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90"><span class="toc-number">6.</span> <span class="toc-text">六、句法分析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E6%97%A0%E5%85%B3%E6%96%87%E6%B3%95%E7%9A%84%E8%BD%AC%E6%8D%A2%E7%94%9F%E6%88%90%E8%AF%AD%E6%B3%95%E7%90%86%E8%AE%BA"><span class="toc-number">6.1.</span> <span class="toc-text">基于上下文无关文法的转换生成语法理论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CYK%E7%AE%97%E6%B3%95"><span class="toc-number">6.2.</span> <span class="toc-text">CYK算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E7%8E%87%E4%B8%8A%E4%B8%8B%E6%96%87%E6%97%A0%E5%85%B3%E6%96%87%E6%B3%95"><span class="toc-number">6.3.</span> <span class="toc-text">概率上下文无关文法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E7%90%86%E8%AE%BA"><span class="toc-number">6.4.</span> <span class="toc-text">依存句法理论</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB"><span class="toc-number">7.</span> <span class="toc-text">七、文本分类</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AB%E3%80%81%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91"><span class="toc-number">8.</span> <span class="toc-text">八、机器翻译</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0"><span class="toc-number">8.1.</span> <span class="toc-text">评估</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91"><span class="toc-number">8.2.</span> <span class="toc-text">统计机器翻译</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%96%B9%E6%B3%95NMT"><span class="toc-number">8.3.</span> <span class="toc-text">神经机器翻译方法NMT</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%AE%9E%E9%AA%8C/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%E4%B8%8E%E9%9D%9E%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/" title="参数估计与非参数估计">参数估计与非参数估计</a><time datetime="2025-02-12T07:53:06.000Z" title="发表于 2025-02-12 15:53:06">2025-02-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%AE%9E%E9%AA%8C/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E8%AE%BA%E9%A2%84%E6%B5%8B%E8%B4%B7%E6%AC%BE%E8%BF%9D%E7%BA%A6/" title="贝叶斯决策论预测贷款违约">贝叶斯决策论预测贷款违约</a><time datetime="2025-02-11T04:14:12.000Z" title="发表于 2025-02-11 12:14:12">2025-02-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%AE%9E%E9%AA%8C/%E8%A3%85%E7%B3%BB%E7%BB%9F%E8%AE%B0%E5%BD%95/" title="装系统记录">装系统记录</a><time datetime="2025-02-10T14:50:02.000Z" title="发表于 2025-02-10 22:50:02">2025-02-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%AE%9E%E9%AA%8C/%E5%80%9F%E5%8A%A9ollama%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8Cqwen2-5/" title="借助ollama本地运行各种开源大模型">借助ollama本地运行各种开源大模型</a><time datetime="2025-01-12T08:37:49.000Z" title="发表于 2025-01-12 16:37:49">2025-01-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/others/2024%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/" title="2024年度总结">2024年度总结</a><time datetime="2025-01-01T05:26:53.000Z" title="发表于 2025-01-01 13:26:53">2025-01-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 姜将</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>