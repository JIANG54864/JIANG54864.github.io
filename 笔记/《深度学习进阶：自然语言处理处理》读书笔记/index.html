<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>《深度学习进阶：自然语言处理处理》读书笔记 | 姜将的个人博客</title><meta name="author" content="姜将"><meta name="copyright" content="姜将"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="第1章 前言这是学习《深度学习进阶：自然语言处理》一书的过程中所作摘要。作者是斋藤康毅。 《深度学习入门：基于python的理论与实现》和《深度学习进阶：自然语言处理》两书深入浅出，写的很精彩，个人认为即使是非计算机专业的人员也能够几乎没有门槛的阅读。原书第一章是对神经网络的复习，故不再赘述。感兴趣的读者可阅读《深度学习入门：基于python的理论与实现》读书笔记这篇博客。  第2章 自然语言和单">
<meta property="og:type" content="article">
<meta property="og:title" content="《深度学习进阶：自然语言处理处理》读书笔记">
<meta property="og:url" content="http://jiang54864.github.io/%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%A4%84%E7%90%86%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="姜将的个人博客">
<meta property="og:description" content="第1章 前言这是学习《深度学习进阶：自然语言处理》一书的过程中所作摘要。作者是斋藤康毅。 《深度学习入门：基于python的理论与实现》和《深度学习进阶：自然语言处理》两书深入浅出，写的很精彩，个人认为即使是非计算机专业的人员也能够几乎没有门槛的阅读。原书第一章是对神经网络的复习，故不再赘述。感兴趣的读者可阅读《深度学习入门：基于python的理论与实现》读书笔记这篇博客。  第2章 自然语言和单">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg">
<meta property="article:published_time" content="2023-06-08T09:38:39.000Z">
<meta property="article:modified_time" content="2025-01-25T07:58:36.289Z">
<meta property="article:author" content="姜将">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg"><link rel="shortcut icon" href="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg"><link rel="canonical" href="http://jiang54864.github.io/%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%A4%84%E7%90%86%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '《深度学习进阶：自然语言处理处理》读书笔记',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-01-25 15:58:36'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = 'hidden';
    document.getElementById('loading-box').classList.remove("loaded")
  }
}

preloader.initLoading()
window.addEventListener('load',()=> { preloader.endLoading() })

if (false) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk/"><i class="fa-fw fa fa-comment"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="姜将的个人博客"><img class="site-icon" src="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg"/><span class="site-name">姜将的个人博客</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk/"><i class="fa-fw fa fa-comment"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">《深度学习进阶：自然语言处理处理》读书笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-06-08T09:38:39.000Z" title="发表于 2023-06-08 17:38:39">2023-06-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-01-25T07:58:36.289Z" title="更新于 2025-01-25 15:58:36">2025-01-25</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>15分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="《深度学习进阶：自然语言处理处理》读书笔记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="第1章-前言"><a href="#第1章-前言" class="headerlink" title="第1章 前言"></a>第1章 前言</h1><p>这是学习《深度学习进阶：自然语言处理》一书的过程中所作摘要。作者是斋藤康毅。 《深度学习入门：基于python的理论与实现》和《深度学习进阶：自然语言处理》两书深入浅出，写的很精彩，个人认为即使是非计算机专业的人员也能够几乎没有门槛的阅读。原书第一章是对神经网络的复习，故不再赘述。感兴趣的读者可阅读《深度学习入门：基于python的理论与实现》读书笔记这篇博客。</p>
<p><img src="1686217358939.png" alt="1686217358939"></p>
<h1 id="第2章-自然语言和单词的分布式表示"><a href="#第2章-自然语言和单词的分布式表示" class="headerlink" title="第2章 自然语言和单词的分布式表示"></a>第2章 自然语言和单词的分布式表示</h1><p>我们平常使用的语言称为自然语言。所谓自然语言处理（NLP），顾名思义，就是处理自然语言的科学。一种能够让计算机理解人类语言的技术。</p>
<p>同义词词典：具有相同含义的单词（同义词）或含义类似的单词（近义词）被归类到同一个组中。</p>
<p>问题：<br><strong>难以顺应时代变化</strong>：新词不断出现，而那些落满尘埃的旧词不知哪天就会被遗忘。语言的含义也会随着时间的推移而变化。<br><strong>人力成本高</strong><br><strong>无法表示单词的微妙差异</strong></p>
<p>语料库就是大量的文本数据。</p>
<p>预处理是指，将文本分割为单词（分词），并将分割后的单词列表转化为单词 ID 列表。</p>
<p>单词的分布式表示将单词表示为固定长度的向量。</p>
<p>“<strong>某个单词的含义由它周围的单词形成</strong>”，称为分布式假设。</p>
<p>上下文的大小（即周围的单词有多少个）称为窗口大小。窗口大小为 2，上下文包含左右各2个单词，以此类推。</p>
<p><img src="1685866891750.png" alt="1685866891750">的共现矩阵：</p>
<p><img src="1685866879252.png" alt="1685866879252"></p>
<p>共现矩阵：用表格汇总各个单词的上下文中包含的单词的频数</p>
<p>测量单词的向量表示的相似度方面，余弦相似度（数量积除以模的乘积）是很常用的。相似单词的排序的实现：<br>❶ 取出查询词的单词向量。<br>❷ 分别求得查询词的单词向量和其他所有单词向量的余弦相似度。<br>❸ 基于余弦相似度的结果，按降序显示它们的值。</p>
<p>对于随机变量 x 和 y，它们的点互信息PMI ：$PMI(x,y)=\log_2\frac{P(x,y)}{P(x)P(y)}$</p>
<p>降维:减少向量维度。尽量保留“重要信息”的基础上减少。</p>
<p>降维方法：SVD奇异值分解。SVD 将任意矩阵分解为 3 个矩阵的乘积：$X=USV^T$,其中 U 和 V 是列向量彼此正交的正交矩阵，S 是除了对角线元素以外其余元素均为 0 的对角矩阵。</p>
<p>总结：</p>
<p>• 使用 WordNet 等同义词词典，可以获取近义词或测量单词间的相似度等<br>• 使用同义词词典的方法存在创建词库需要大量人力、新词难更新等问题<br>• 目前，使用语料库对单词进行向量化是主流方法<br>• 近年来的单词向量化方法大多基于“单词含义由其周围的单词构成”这一分布式假设<br>• 在基于计数的方法中，对语料库中的每个单词周围的单词的出现频数进行计数并汇总（= 共现矩阵）<br>• 通过将共现矩阵转化为 PPMI 矩阵并降维，可以将大的稀疏向量转变为小的密集向量<br>• 在单词的向量空间中，含义上接近的单词距离上理应也更近</p>
<h1 id="第3章-word2vec"><a href="#第3章-word2vec" class="headerlink" title="第3章 word2vec"></a>第3章 word2vec</h1><p>注：word2vec 一词最初用来指程序或者工具，但是随着该词的流行，在某些语境下，也指神经网络的模型。正确地说，CBOW 模型和 skip-gram 模型是 word2vec 中使用的两个神经网络。</p>
<p>基于计数的方法的问题：现实世界中，语料库处理的单词数量非常大，比如如果词汇量超过 100 万个，那么使用基于计数的方法就需要生成一个 100 万 × 100万的庞大矩阵，对如此庞大的矩阵执行 SVD 显然是不现实的。</p>
<p>基于推理的方法使用神经网络，通常在 mini-batch 数据上进行学习。这意味着神经网络一次只需要看一部分学习数据（mini-batch），并反复更新权重。</p>
<p><img src="1685868990883.png" alt="1685868990883"></p>
<p>基于计数的方法一次性处理全部学习数据；反之，基于推理的方法使用部分学习数据逐步学习。这意味着，在词汇量很大的语料库中，即使 SVD 等的计算量太大导致计算机难以处理，神经网络也可以在部分数据上学习。并且，神经网络的学习可以使用<strong>多台机器、多个 GPU 并行</strong>执行，从而<strong>加速</strong>整个学习过程。在这方面，基于推理的方法更有优势。此外基于推理的方法支持参数的增量学习（添加新词）。</p>
<p>当给出周围的单词（上下文）时，预测目标词处会出现什么单词，这就是推理。</p>
<p>CBOW连续词袋模型：</p>
<p><img src="1685872463380.png" alt="1685872463380"></p>
<p>如果对上下文考虑 N 个单词，则输入层会有 N 个。</p>
<p>从输入层到中间层的变换由相同的全连接层（权重为$ W<em>{in} $）完成，从中间层到输出层神经元的变换由另一个全连接层（权重为 $ W</em>{out} $）完成。中间层的神经元是各个输入层经全连接层变换后得到的值的“平均”。输出层的各个神经元对应于各个单词。输出层的神经元是各个单词的得分，它的值越大，说明对应单词的出现概率就越高。</p>
<p>全连接层的权重 $ W<em>{in} $就是我们要的单词的分布式表示。输出侧的权重 $ W</em>{out} $也同样保存了对单词含义进行了编码的向量，在列方向上保存了各个单词的分布式表示。最受欢迎的是方案是只使用输入侧的权重。</p>
<p>CBOW 模型从上下文的多个单词预测中间的单词（目标词），而 skip-gram 模型则从中间的单词（目标词）预测周围的多个单词（上下文）。</p>
<p><img src="1685873611771.png" alt="1685873611771"></p>
<p>skip-gram 模型的输入层只有一个，输出层的数量则与上下文的单词个数相等。</p>
<p>从单词的分布式表示的准确度来看，在大多数情况下，<strong>skip-grm 模型的结果更好</strong>。特别是随着语料库规模的增大，在低频词和类推问题的性能方面，skip-gram 模型往往会有更好的表现。<br>就学习速度而言，<strong>CBOW 模型比 skip-gram 模型要快</strong>。</p>
<p>考虑需要向词汇表添加新词并更新单词的分布式表示的场景。此时，<strong>基于计数的方法需要从头开始计算</strong>。<strong>基于推理的方法（word2vec）允许参数的增量学习</strong>。可以将<strong>之前学习到的权重作为下一次学习的初始值</strong>，在<strong>不损失之前学习到的经验</strong>的情况下，高效地更新单词的分布式表示。在这方面，基于推理的方法（word2vec）具有优势。</p>
<p>就分布式表示的性质而言，基于计数的方法主要是<strong>编码单词的相似性</strong>，而 word2vec（特别是 skip-gram 模型）除了单词的相似性以外，还能<strong>理解更复杂的单词之间的模式</strong>。</p>
<p>总结：</p>
<p>• 基于推理的方法以预测为目标，同时获得了作为副产物的单词的分布式表示<br>• word2vec 是基于推理的方法，由简单的 2 层神经网络构成<br>• word2vec 有 skip-gram 模型和 CBOW 模型<br>• CBOW 模型从多个单词（上下文）预测 1 个单词（目标词）<br>• skip-gram 模型反过来从 1 个单词（目标词）预测多个单词（上下文）<br>• 由于 word2vec 可以进行权重的增量学习，所以能够高效地更新或添加单词的分布式表示</p>
<h1 id="第4章-word2vec的高速化"><a href="#第4章-word2vec的高速化" class="headerlink" title="第4章 word2vec的高速化"></a>第4章 word2vec的高速化</h1><p>CBOW的问题是，随着语料库中处理的词汇量的增加，计算量也随之增加。</p>
<p>对上一章中简单的 word2vec 进行两点改进：引入名为Embedding 层的新层（解决输入层的 one-hot 表示和权重矩阵 $W_{in}$的乘积计算瓶颈），以及引入名为 Negative Sampling 的新损失函数（解决中间层和权重矩阵 Wout 的乘积以及 Softmax 层的计算瓶颈）。</p>
<p><img src="1685875451395.png" alt="1685875451395"></p>
<p>将矩阵乘积称为 MatMul 节点。MatMul 是 Matrix Multiply 的缩写。</p>
<p>Embedding 层：创建一个<strong>从权重参数中抽取“单词 ID 对应行（向量）”的层</strong>。在这个 Embedding 层存放词嵌入（分布式表示）。</p>
<p>将 word2vec（CBOW 模型）的实现中的输入侧的 MatMul 层换成 Embedding 层。既能<strong>减少内存使用量</strong>，又能<strong>避免不必要的计算</strong>。</p>
<p>负采样：用二分类拟合多分类</p>
<p>负采样方法既可以求将正例作为目标词时的损失，同时也可以采样（选出）若干个负例，对这些负例求损失。然后，将这些数据（正例和采样出来的负例）的损失加起来，将其结果作为最终的损失。</p>
<p>如何抽取负例：基于语料库的<strong>统计数据</strong>进行采样，让语料库中<strong>经常出现的单词容易被抽到</strong>。先计算语料库中各个单词的出现次数，并将其表示为“概率分布”，然后使用这个概率分布对单词进行采样</p>
<p>考虑到计算的复杂度，有必要将负例限定在较小范围内（5 个或者 10 个）。</p>
<p>word2vec 中提出的负采样对刚才的概率分布增加了一个步骤，对原来的概率分布取 0.75 (不固定，小于1即可)次方。这是为了<strong>防止低频单词被忽略</strong>。通过取 0.75 次方，低频单词的概率将稍微变高。</p>
<p>迁移学习是指在某个领域学到的知识可以被应用于其他领域。</p>
<p>总结：</p>
<p>• Embedding 层保存单词的分布式表示，在正向传播时，提取单词 ID对应的向量<br>• 因为 word2vec 的计算量会随着词汇量的增加而成比例地增加，所以最好使用近似计算来加速<br>• 负采样技术采样若干负例，使用这一方法可以将多分类问题转化为二分类问题进行处理<br>• 基于 word2vec 获得的单词的分布式表示内嵌了单词含义，在相似的上下文中使用的单词在单词向量空间上处于相近的位置<br>• word2vec 的单词的分布式表示的一个特性是可以基于向量的加减法运算来求解类推问题<br>• word2vec 的迁移学习能力非常重要，它的单词的分布式表示可以应用于各种各样的自然语言处理任务</p>
<h1 id="第5章-RNN"><a href="#第5章-RNN" class="headerlink" title="第5章 RNN"></a>第5章 RNN</h1><p>前馈（feedforward）是指网络的传播方向是单向的。</p>
<p>RNN（Recurrent Neural Network，循环神经网络）的特征在于拥有一个环路（或回路）。这个环路可以使数据不断循环。通过数据的循环，RNN 一边记住过去的数据，一边更新到最新的数据。</p>
<p>语言模型（language model）给出了单词序列发生的概率。具体来说，就是使用概率来评估一个单词序列发生的可能性，即在多大程度上是自然的单词序列。</p>
<p><img src="1685879504149.png" alt="1685879504149"></p>
<p>各个时刻的 RNN 层接收传给该层的输入和前一个RNN 层的输出，然后据此计算当前时刻的输出。</p>
<p>困惑度表示“概率的倒数”（这个解释在数据量为 1 时严格一致）。困惑度越小，分叉度越小，表明模型越好。</p>
<p>Backpropagation Through Time（基于时间的反向传播），简称 BPTT。将时间轴方向上过长的网络在合适的位置进行截断，从而创建多个小型网络，然后对截出来的小型网络执行误差反向传播法，这个方法称为 Truncated BPTT（截断的 BPTT）。</p>
<p>总结：</p>
<p>• RNN 具有环路，因此可以在内部记忆隐藏状态<br>• 通过展开 RNN 的循环，可以将其解释为多个 RNN 层连接起来的神经网络，可以通过常规的误差反向传播法进行学习（= BPTT）<br>• 在学习长时序数据时，要生成长度适中的数据块，进行以块为单位的 BPTT 学习（= Truncated BPTT）<br>• 语言模型将单词序列解释为概率<br>• 理论上，使用 RNN 层的条件语言模型可以记忆所有已出现单词的信息</p>
<h1 id="第6章-Gated-RNN"><a href="#第6章-Gated-RNN" class="headerlink" title="第6章 Gated RNN"></a>第6章 Gated RNN</h1><p>RNN 之所以不擅长学习时序数据的长期依赖关系，是因为 BPTT 会发生梯度消失和梯度爆炸的问题。</p>
<p>RNN 层通过向过去传递“有意义的梯度”，能够学习时间方向上的依赖关系。此时梯度（理论上）包含了那些应该学到的有意义的信息，通过将这些信息向过去传递，RNN 层学习长期的依赖关系。但是，<strong>如果这个梯度在中途变弱</strong>（甚至没有包含任何信息），则<strong>权重参数将不会被更新</strong>。也就是说，RNN 层无法学习长期的依赖关系。</p>
<p>梯度的大小随时间步长呈指数级增加，这就是梯度爆炸；梯度呈指数级减小，这就是梯度消失。</p>
<p>为什么会出现这样的指数级变化呢？因为矩阵 $W_h$ 被反复乘了 T 次。</p>
<p>如果矩阵的奇异值的最大值大于 1，则可以预测梯度很有可能会呈指数级增加（必要条件）；而如果奇异值的最大值小于 1，则可以判断梯度会呈指数级减小。</p>
<p>解决梯度爆炸的方法：梯度裁剪：如果梯度的 L2 范数 ∥g∥ (各维度平方的和开根号)大于或等于阈值，就修正梯度：$g=\frac{threshold}{||g||}g$</p>
<p>为了解决梯度消失，需要从根本上改变 RNN 层的结构： Gated RNN，其中具有代表性的有LSTM（长短期记忆网络，Long Short-Term Memory） ：</p>
<p><img src="1685881923071.png" alt="1685881923071"></p>
<p>注:tanh 函数（双曲正切函数）tanh的输出是−1.0 ~ 1.0的实数。我们可以认为这个−1.0 ~ 1.0的数值表示某种被编码的“信息”的强弱（程度）。</p>
<p>针对 tanh($c_t$) 的各个元素，<strong>调整它们作为下一时刻的隐藏状态的重要程度</strong>。由于这个门管理下一个隐藏状态 $h_t$ 的输出，所以称为输出门：</p>
<p><img src="1685882274340.png" alt="1685882274340"></p>
<p>输出门的开合程度（流出比例）根据输入$x<em>t$ 和上一个状态$h</em>{t-1}$ 求出。</p>
<p>遗忘门:明确告诉记忆单元需要“忘记什么”,在记忆单元 $c_{t-1}$上添加一个忘记不必要记忆的门:</p>
<p><img src="1685882632834.png" alt="1685882632834"></p>
<p>（门进行的计算表示为 σ,乘积为对应元素的乘积）</p>
<p>记忆单元只会忘记信息。现在我们还想向这个记忆单元添加一些应当记住的新信息，为此我们添加新的 tanh 节点,这个 tanh 节点的作用不是门，而是将新的信息添加到记忆单元中。：</p>
<p><img src="1685882848510.png" alt="1685882848510"></p>
<p>最后，我们给上图中的 g 添加门，这里将这个新添加的门称为输入门：</p>
<p><img src="1685883452860.png" alt="1685883452860"></p>
<p>输入门判断新增信息 g 的各个元素的价值有多大。输入门会添加加权后的新信息。</p>
<p>RNN 比常规的前馈神经网络更容易发生过拟合，抑制过拟合：常规的Dropout，在训练时随机忽略层的一部分神经元。权重共享。</p>
<p>无论沿时间方向（水平方向）前进多少，信息都不会丢失。Dropout 与时间轴独立，仅在深度方向（垂直方向）上起作用。</p>
<p><img src="1685883990254.png" alt="1685883990254"></p>
<p>（变分dropout可以应用在时间方向上）</p>
<p>权重共享：</p>
<p><img src="1685884301302.png" alt="1685884301302"></p>
<p>绑定（共享）Embedding 层和 Affine 层的权重的技巧在于权重共享。通过在这两个层之间共享权重，可以大大减少学习的参数数量。尽管如此，它仍能提高精度。</p>
<p>共享权重可以减少需要学习的参数数量，从而促进学习。另外，参数数量减少，还能抑制过拟合。</p>
<p>总结：</p>
<p>• 在简单 RNN 的学习中，存在梯度消失和梯度爆炸问题<br>• 梯度裁剪对解决梯度爆炸有效，LSTM、GRU 等 Gated RNN 对解决梯度消失有效<br>• LSTM 中有 3 个门：输入门、遗忘门和输出门<br>• 门有专门的权重，并使用 sigmoid 函数输出 0.0 ～ 1.0 的实数<br>• LSTM 的多层化、Dropout 和权重共享等技巧可以有效改进语言模型<br>• RNN 的正则化很重要，人们提出了各种基于 Dropout 的方法</p>
<h1 id="第7章-基于RNN生成文本"><a href="#第7章-基于RNN生成文本" class="headerlink" title="第7章 基于RNN生成文本"></a>第7章 基于RNN生成文本</h1><p>seq2seq 模型也称为 Encoder-Decoder 模型。有两个模块——Encoder（编码器）和 Decoder（解码器）。</p>
<p><img src="1685884632072.png" alt="1685884632072"></p>
<p>编码器首先对“吾輩は猫である”这句话进行编码，然后将编码好的信息传递给解码器，由解码器生成目标文本。此时，编码器编码的信息<strong>浓缩了翻译所必需的信息</strong>，解码器基于这个浓缩的信息生成目标文本。</p>
<p>seq2seq的改进：</p>
<p>反转输入数据（Reverse）。直观上可以认为，反转数据后梯度的传播可以更平滑。</p>
<p>偷窥（Peeky）：将编码好的信息分配给解码器的其他层，这可以解释为其他层也能“偷窥”到编码信息。重要的信息不是一个人专有，而是多人共享，这样我们或许可以做出更加正确的判断。</p>
<p>总结：<br>• 通过组合两个 RNN，可以将一个时序数据转换为另一个时序数据（seq2seq）<br>• 在 seq2seq 中，编码器对输入语句进行编码，解码器接收并解码这个编码信息，获得目标输出语句<br>• 反转输入语句（Reverse）和将编码信息分配给解码器的多个层（Peeky）可以有效提高 seq2seq 的精度</p>
<h1 id="第8章-Attention"><a href="#第8章-Attention" class="headerlink" title="第8章 Attention"></a>第8章 Attention</h1><p>seq2seq 中使用编码器对时序数据进行编码，然后将编码信息传递给解码器。此时，编码器的输出是固定长度的向量。实际上，这个“固定长度”存在很大问题。因为固定长度的向量意味着，<strong>无论输入语句的长度如何（无论多长），都会被转换为长度相同的向量</strong>。</p>
<p><img src="1686144255406.png" alt="1686144255406"></p>
<p>编码器的改进：<strong>将编码器的全部时刻的隐藏状态取出来</strong>，编码器可以根据输入语句的长度，成比例地编码信息。使用各个时刻（各个单词）的隐藏状态向量，可以获得和输入的单词数相同数量的向量。编码器就摆脱了“一个固定长度的向量”的制约。</p>
<p><strong>仅关注必要的信息，并根据该信息进行时序转换</strong>。这个机制称为 Attention。</p>
<p><img src="1685885980810.png" alt="1685885980810"></p>
<p>Attention Weight 层关注编码器输出的各个单词向量 hs，并计算各个单词的权重 a；然后，Weight Sum 层计算 a 和 hs 的加权和，并输出上下文向量 c。我们将进行这一系列计算的层称为 Attention 层。</p>
<p>将这个 Attention 层放在 LSTM 层和 Affine 层的中间：</p>
<p><img src="1685886749155.png" alt="1685886749155"></p>
<p>总结：</p>
<p>• 在翻译、语音识别等将一个时序数据转换为另一个时序数据的任务中，时序数据之间常常存在对应关系<br>• Attention 从数据中学习两个时序数据之间的对应关系<br>• Attention 使用向量内积（方法之一）计算向量之间的相似度，并输出这个相似度的加权和向量<br>• 因为 Attention 中使用的运算是可微分的，所以可以基于误差反向传播法进行学习</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://jiang54864.github.io">姜将</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://jiang54864.github.io/%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%A4%84%E7%90%86%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">http://jiang54864.github.io/%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%A4%84%E7%90%86%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://jiang54864.github.io" target="_blank">姜将的个人博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/%E7%AC%94%E8%AE%B0/%E4%BC%97%E6%99%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E7%BD%91%E7%BB%9C%E5%8C%96%E4%BA%A7%E4%B8%9A%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/" title="众智科学与网络化产业知识梳理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">众智科学与网络化产业知识梳理</div></div></a></div><div class="next-post pull-right"><a href="/%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E7%8E%B0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" title="《深度学习入门：基于python的理论与实现》读书笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">《深度学习入门：基于python的理论与实现》读书笔记</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/others/%E4%B8%8EChatgpt%E8%81%8A%E8%81%8A%E5%A4%A9/" title="与Chatgpt聊聊天"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-06</div><div class="title">与Chatgpt聊聊天</div></div></a></div><div><a href="/%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E7%8E%B0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" title="《深度学习入门：基于python的理论与实现》读书笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-08</div><div class="title">《深度学习入门：基于python的理论与实现》读书笔记</div></div></a></div><div><a href="/%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%B4%E7%90%86/" title="机器学习基础整理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-19</div><div class="title">机器学习基础整理</div></div></a></div><div><a href="/%E7%AC%94%E8%AE%B0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/" title="自然语言处理知识梳理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-13</div><div class="title">自然语言处理知识梳理</div></div></a></div><div><a href="/%E5%AE%9E%E9%AA%8C/%E5%80%9F%E5%8A%A9ollama%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8Cqwen2-5/" title="借助ollama本地运行各种开源大模型"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-12</div><div class="title">借助ollama本地运行各种开源大模型</div></div></a></div><div><a href="/%E5%AE%9E%E9%AA%8C/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E8%AE%BA%E9%A2%84%E6%B5%8B%E8%B4%B7%E6%AC%BE%E8%BF%9D%E7%BA%A6/" title="贝叶斯决策论预测贷款违约"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-11</div><div class="title">贝叶斯决策论预测贷款违约</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://pic1.imgdb.cn/item/6794d01ad0e0a243d4f7e064.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">姜将</div><div class="author-info__description">记录学习中的知识与收获</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/JIANG54864"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=2792663690&amp;website=www.oicqzone.com" target="_blank" title="点击添加我的QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="https://www.helloimg.com/i/2025/01/25/6794f016e9b99.png" target="_blank" title="点击添加我的微信"><i class="fab fa-weixin"></i></a><a class="social-icon" href="https://x.com/JIANG2024521" target="_blank" title="我的X（twitter）主页"><i class="fab fa-x-twitter"></i></a><a class="social-icon" href="mailto:jiangsw2022@163.com" target="_blank" title="给我发邮件"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客网站！若有需要，可通过上方联系方式找到我。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC1%E7%AB%A0-%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">第1章 前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC2%E7%AB%A0-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%92%8C%E5%8D%95%E8%AF%8D%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E8%A1%A8%E7%A4%BA"><span class="toc-number">2.</span> <span class="toc-text">第2章 自然语言和单词的分布式表示</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC3%E7%AB%A0-word2vec"><span class="toc-number">3.</span> <span class="toc-text">第3章 word2vec</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC4%E7%AB%A0-word2vec%E7%9A%84%E9%AB%98%E9%80%9F%E5%8C%96"><span class="toc-number">4.</span> <span class="toc-text">第4章 word2vec的高速化</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC5%E7%AB%A0-RNN"><span class="toc-number">5.</span> <span class="toc-text">第5章 RNN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC6%E7%AB%A0-Gated-RNN"><span class="toc-number">6.</span> <span class="toc-text">第6章 Gated RNN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC7%E7%AB%A0-%E5%9F%BA%E4%BA%8ERNN%E7%94%9F%E6%88%90%E6%96%87%E6%9C%AC"><span class="toc-number">7.</span> <span class="toc-text">第7章 基于RNN生成文本</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC8%E7%AB%A0-Attention"><span class="toc-number">8.</span> <span class="toc-text">第8章 Attention</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%AE%9E%E9%AA%8C/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E9%A2%84%E6%B5%8B%E7%94%9F%E8%BF%98%E8%80%85/" title="集成学习预测生还者">集成学习预测生还者</a><time datetime="2025-02-17T11:37:32.000Z" title="发表于 2025-02-17 19:37:32">2025-02-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%AE%9E%E9%AA%8C/%E4%B8%89%E7%A7%8D%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB%E6%B1%BD%E8%BD%A6%E5%93%81%E7%89%8C/" title="三种决策树算法分类汽车品牌">三种决策树算法分类汽车品牌</a><time datetime="2025-02-16T15:25:10.000Z" title="发表于 2025-02-16 23:25:10">2025-02-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%AE%9E%E9%AA%8C/BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%86%E5%88%AB%E6%95%B0%E5%AD%97/" title="BP神经网络识别数字">BP神经网络识别数字</a><time datetime="2025-02-14T15:10:15.000Z" title="发表于 2025-02-14 23:10:15">2025-02-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%AE%9E%E9%AA%8C/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%E4%B8%8E%E9%9D%9E%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/" title="参数估计与非参数估计">参数估计与非参数估计</a><time datetime="2025-02-12T07:53:06.000Z" title="发表于 2025-02-12 15:53:06">2025-02-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%AE%9E%E9%AA%8C/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E8%AE%BA%E9%A2%84%E6%B5%8B%E8%B4%B7%E6%AC%BE%E8%BF%9D%E7%BA%A6/" title="贝叶斯决策论预测贷款违约">贝叶斯决策论预测贷款违约</a><time datetime="2025-02-11T04:14:12.000Z" title="发表于 2025-02-11 12:14:12">2025-02-11</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 姜将</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>