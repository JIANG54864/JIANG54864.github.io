<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>《深度学习入门：基于python的理论与实现》读书笔记 | 姜将的个人博客</title><meta name="author" content="姜将"><meta name="copyright" content="姜将"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="第1章 前言这是学习《深度学习入门：基于python的理论与实现》一书的过程中所作摘要。作者是斋藤康毅。 《深度学习入门：基于python的理论与实现》和《深度学习进阶：自然语言处理》两书深入浅出，写的很精彩，个人认为即使是非计算机专业的人员也能够几乎没有门槛的阅读。原书第一章是介绍python基础知识，故不再赘述。  第2章 感知机感知机接收多个输入信号，输出一个信号： $ y&#x3D;\begin{c">
<meta property="og:type" content="article">
<meta property="og:title" content="《深度学习入门：基于python的理论与实现》读书笔记">
<meta property="og:url" content="http://jiang54864.github.io/%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E7%8E%B0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="姜将的个人博客">
<meta property="og:description" content="第1章 前言这是学习《深度学习入门：基于python的理论与实现》一书的过程中所作摘要。作者是斋藤康毅。 《深度学习入门：基于python的理论与实现》和《深度学习进阶：自然语言处理》两书深入浅出，写的很精彩，个人认为即使是非计算机专业的人员也能够几乎没有门槛的阅读。原书第一章是介绍python基础知识，故不再赘述。  第2章 感知机感知机接收多个输入信号，输出一个信号： $ y&#x3D;\begin{c">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/JIANG54864/PictureCDN/main/blog/profile.jpg">
<meta property="article:published_time" content="2023-06-08T09:38:35.000Z">
<meta property="article:modified_time" content="2023-06-08T09:47:43.126Z">
<meta property="article:author" content="姜将">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/JIANG54864/PictureCDN/main/blog/profile.jpg"><link rel="shortcut icon" href="https://raw.githubusercontent.com/JIANG54864/PictureCDN/main/blog/profile.jpg"><link rel="canonical" href="http://jiang54864.github.io/%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E7%8E%B0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '《深度学习入门：基于python的理论与实现》读书笔记',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-06-08 17:47:43'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = 'hidden';
    document.getElementById('loading-box').classList.remove("loaded")
  }
}

preloader.initLoading()
window.addEventListener('load',()=> { preloader.endLoading() })

if (false) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://raw.githubusercontent.com/JIANG54864/PictureCDN/main/blog/profile.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk/"><i class="fa-fw fa fa-comment"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="姜将的个人博客"><img class="site-icon" src="https://raw.githubusercontent.com/JIANG54864/PictureCDN/main/blog/profile.jpg"/><span class="site-name">姜将的个人博客</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk/"><i class="fa-fw fa fa-comment"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">《深度学习入门：基于python的理论与实现》读书笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-06-08T09:38:35.000Z" title="发表于 2023-06-08 17:38:35">2023-06-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-06-08T09:47:43.126Z" title="更新于 2023-06-08 17:47:43">2023-06-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/">知识梳理</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>21分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="《深度学习入门：基于python的理论与实现》读书笔记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="第1章-前言"><a href="#第1章-前言" class="headerlink" title="第1章 前言"></a>第1章 前言</h1><p>这是学习《深度学习入门：基于python的理论与实现》一书的过程中所作摘要。作者是斋藤康毅。 《深度学习入门：基于python的理论与实现》和《深度学习进阶：自然语言处理》两书深入浅出，写的很精彩，个人认为即使是非计算机专业的人员也能够几乎没有门槛的阅读。原书第一章是介绍python基础知识，故不再赘述。</p>
<p><img src="1686216817517.png" alt="1686216817517"></p>
<h1 id="第2章-感知机"><a href="#第2章-感知机" class="headerlink" title="第2章 感知机"></a>第2章 感知机</h1><p>感知机接收多个输入信号，输出一个信号： $ y=<br>\begin{cases}<br>0, &amp; (b+w_1x_1+w_2x_2\le0)\<br>1, &amp; (b+w_1x_1+w_2x_2&gt;0)<br>\end{cases}$</p>
<p>感知机会计算输入信号和权重的乘积，然后加上偏置，如果这个值大于0则输出1，否则输出0。  </p>
<p>具体地说，$w_1$和$w_2$表示各个信号的权重，是控制输入信号的重要性的参数，而偏置是调整神经元被激活的容易程度（输出信号为 1 的程度）的参数。</p>
<p>与门：and 全1为1否则为0 与非门则相反<br>或门：or 有1为1<br>异或门：xor 仅当有1有0时为1</p>
<p>单层感知机的局限性就在于它只能表示由一条直线分割的空间（线性空间），无法分离非线性空间。</p>
<p><img src="1685713104394.png" alt="1685713104394"></p>
<p>因此单层感知机不能表示异或门。可通过多层感知机实现。</p>
<p><img src="1685713367323.png" alt="1685713367323"></p>
<p>总结：</p>
<p>• 感知机是具有输入和输出的算法。给定一个输入后，将输出一个既定的值。<br>• 感知机将权重和偏置设定为参数。<br>• 使用感知机可以表示与门和或门等逻辑电路。<br>• 异或门无法通过单层感知机来表示。<br>• 使用2层感知机可以表示异或门。<br>• 单层感知机只能表示线性空间，而多层感知机可以表示非线性空间。<br>• 多层感知机（在理论上）可以表示计算机。</p>
<h1 id="第3章-神经网络"><a href="#第3章-神经网络" class="headerlink" title="第3章 神经网络"></a>第3章 神经网络</h1><p>神经网络：我们把最左边的一列称为输入层，最右边的一列称为输出层，中间的一列称为中间层。中间层有时也称为隐藏层。</p>
<p>激活函数(activation function)。 如“激活”所示，激活函数的作用在于决定如何来激活输入信号的总和。</p>
<p><img src="1685714187176.png" alt="1685714187176"></p>
<p>首先，计算加权输入信号和偏置的总和，记为a：$a = b + w_1x_1 + w_2x_2$，然后用h()函数将a转换为输出y：$y = h(a)$。</p>
<h2 id="阶跃函数"><a href="#阶跃函数" class="headerlink" title="阶跃函数"></a>阶跃函数</h2><p>$ y=<br>\begin{cases}<br>0, &amp; x\le0\<br>1, &amp; x&gt;0<br>\end{cases}$，激活函数以阈值为界，一旦输入超过阈值，就切换输出。这样的函数称为“阶跃函数”。 </p>
<p>感知机使用了阶跃函数。如果将激活函数从阶跃函数换成其他函数，就可以进入神经网络的世界了。</p>
<h2 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h2><p>$h(x)=\frac{1}{1+exp(-x)}$，exp(−x)表示$e^{-x}$的意思，或者$h(x)=\frac{1}{1+e^{-x}}$</p>
<p>sigmoid函数和阶跃函数的比较（虚线是阶跃函数）：</p>
<p><img src="1685714544685.png" alt="1685714544685"></p>
<p>不同：</p>
<p>1.sigmoid函数是一条<strong>平滑</strong>的曲线，输出随着输入发生<strong>连续性的变化</strong>。而阶跃函数以0为界，输出发生<strong>急剧性的变化</strong>。</p>
<p>2.相对于阶跃函数只能返回<strong>0或1</strong>，sigmoid函数可以返回0.731 . . .、0.880 . . .等<strong>实数</strong>（这一点和刚才的平滑性有关）。也就是说，感知机中神经元之间流动的是0或1的二元信号，而神经网络中流动的是连续的实数值信号。</p>
<p>相同：</p>
<p>1.两者的结构均是“<strong>输入小时，输出接近0</strong>（为0）；随着<strong>输入增大，输出向1靠近</strong>（变成1）”。也就是说，当输入信号为<strong>重要信息</strong>时，阶跃函数和sigmoid函数都会输出<strong>较大的值</strong>；当输入信号为<strong>不重要的信息</strong>时，两者都输出<strong>较小的值</strong>。</p>
<p>2.还有一个共同点是，不管输入信号有多小，或者有多大，输出信号的值都在<strong>0到1之间</strong>。</p>
<p>3.两者均为<strong>非线性</strong>函数。</p>
<p>神经网络的激活函数为什么不能使用线性函数呢？</p>
<p>因为使用线性函数的话，加深神经网络的层数就没有意义了。<br>线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。 例如 h(x) = cx，y(x) = h(h(h(x))) 的运算对应 3 层神经网络，但是同样的处理可以由y(x) = ax（注意，a = $c^3$）这一次乘法运算（即没有隐藏层的神经网络）来表示。使用线性函数时，无法发挥多层网络带来的优势。因此，为了发挥叠加层所带来的优势，激活函数必须使用非线性函数。</p>
<h2 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h2><p>Rectified Linear Unit线性整流函数：$ h(x)=<br>\begin{cases}<br>x, &amp; x&gt;0,\<br>0, &amp; x \le 0<br>\end{cases}$</p>
<p><img src="1685715218718.png" alt="1685715218718"></p>
<p>在矩阵的乘积运算中，对应维度的元素个数要保持一致</p>
<p>权重符号的书写：</p>
<p><img src="1685716094170.png" alt="1685716094170"></p>
<p>恒等函数会将输入按原样输出，对于输入的信息，不加以任何改动地直接输出。恒等函数进行的转换处理可以用一根箭头来表示</p>
<h2 id="softmax函数"><a href="#softmax函数" class="headerlink" title="softmax函数"></a>softmax函数</h2><p>$y<em>k=\frac{exp(a_k)}{\sum</em>{i=1}^nexp(a<em>i)}$或者$y_k=\frac{e^{a_k}}{\sum</em>{i=1}^ne^{a_i}}$,分子是输入信号$a_k$的指数函数，分母是所有输入信号的指数函数的和。</p>
<p><img src="1685716704174.png" alt="1685716704174"></p>
<p>用图表示softmax函数，输出通过箭头与所有的输入信号相连，这是因为输出层的各个神经元都受到所有输入信号的影响。</p>
<p><img src="../../../../../noteshelf/Untitled%2520Folder/1686197744965.png" alt="1686197744965"></p>
<p>softmax函数的实现改进:</p>
<p><img src="1685717017418.png" alt="1685717017418"></p>
<p>在进行softmax的指数函数的运算时，加上（或者减去）某个常数并不会改变运算的结果。为了防止溢出，一般会使用输入信号中的最大值。</p>
<p>softmax函数的输出是0.0到1.0之间的实数。并且，softmax函数的<strong>输出值的总和是1</strong>。</p>
<p>和求解机器学习问题的步骤（分成<strong>学习和推理</strong>两个阶段进行）一样，使用神经网络解决问题时，也需要首先使用训练数据（学习数据）进行权重参数的学习；进行推理时，使用刚才学习到的参数，对输入数据进行分类。</p>
<p>把数据限定到某个范围内的处理称为正规化（normalization）。此外，对神经网络的输入数据进行某种既定的转换称为预处理（pre-processing）。</p>
<p>为什么批处理可以缩短处理时间呢？这是因为大多数处理数值计算的库都进行了能够高效处理大型数组运算的最优化。并且，在神经网络的运算中，当数据传送成为瓶颈时，批处理可以减轻数<br>据总线的负荷（严格地讲，相对于数据读入，可以将更多的时间用在计算上）。</p>
<p>批处理输入数据和权重参数的“形状”：</p>
<p><img src="1685717595659.png" alt="1685717595659"></p>
<p><img src="1685717606255.png" alt="1685717606255"></p>
<p>这种打包式的输入数据称为批（batch）。</p>
<p>总结：</p>
<p>• 神经网络中的激活函数使用平滑变化的sigmoid函数或ReLU函数。<br>• 机器学习的问题大体上可以分为回归问题和分类问题。<br>• 关于输出层的激活函数，<strong>回归问题中一般用恒等函数</strong>，<strong>分类问题中一般用softmax函数</strong>。<br>• 分类问题中，输出层的神经元的数量设置为要分类的类别数。<br>• 输入数据的集合称为批。通过以批为单位进行推理处理，能够实现高速的运算。</p>
<p>注：回归输出值（连续），分类输出类别（离散）</p>
<h1 id="第4章-神经网络的学习"><a href="#第4章-神经网络的学习" class="headerlink" title="第4章 神经网络的学习"></a>第4章 神经网络的学习</h1><p>神经网络的特征就是可以从数据中学习。（“从数据中学习”，是指可以由数据自动决定权重参数的值。）</p>
<p><img src="1685802145778.png" alt="1685802145778"></p>
<p>深度学习有时也称为端到端机器学习（end-to-end machine learning）。这里所说的端到端是从原始数据（输入）中获得目标结果（输出）的意思。</p>
<p>机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验等。训练数据也可以称为监督数据。</p>
<p>使用训练数据进行学习，寻找最优的参数；然后，使用测试数据评价训练得到的模型的实际能力。</p>
<p>泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的能力。获得泛化能力是机器学习的最终目标。</p>
<p>只对某个数据集过度拟合的状态称为过拟合（over fitting）。</p>
<p>神经网络以某个指标为线索寻找最优权重参数。神经网络的学习中所用的指标称为损失函数（loss function）。损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。</p>
<p>均方误差：$E=\frac{1}{2}\sum\limits_{k}(y_k-t_k)^2$<br>$y_k$是表示神经网络的输出，$t_k$表示监督数据，k表示数据的维数</p>
<p>交叉熵误差：$E=-\sum\limits_{k}t_klogy_k$<br>log表示以e为底数的自然对数（$log_e$）。$y_k$是神经网络的输出，$t_k$是正确解标签。并且，$t_k$中只有正确解标签的索引为1，其他均为0（one-hot表示）。因此，实际上只计算对应正确解标签的输出的自然对数。</p>
<p>将正确解标签表示为1，其他标签表示为0的表示方法称为one-hot表示。</p>
<p>为什么要导入损失函数呢？<br>对该权重参数的损失函数求导，表示的是“如果稍微改变这个权重参数的值，损失函数的值会如何变化”。<br>在进行神经网络的学习时，不能将识别精度作为指标。因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为0。<br>sigmoid函数的导数在任何地方都不为0。这对神经网络的学习非常重要。得益于这个<strong>斜率不会为0</strong>的性质，神经网络的学习得以正确进行。</p>
<p>像$(\frac{\partial f}{ {\partial x_0} },\frac{\partial f}{ {\partial x_1} })$由全部变量的偏导数汇总而成的向量称为梯度，梯度指示的方向是各点处的函数值减小最多的方向。</p>
<p>函数的极小值、最小值以及被称为鞍点（saddle point）的地方，梯度为0。梯度法是要寻找梯度为0的地方，但是那个地方不一定就是最小值（也有可能是极小值或者鞍点）。此外，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入被称为“学习高原”的无法前进的停滞期。</p>
<p>在梯度法中，函数的取值从当前位置沿着梯度方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度方向前进，如此反复，不断地沿梯度方向前进。像这样，通过不断地沿梯度方向前进，逐渐减小函数值的过程就是梯度法。</p>
<p>梯度法：$x_0=x_0-\eta \frac{\partial f}{\partial x_0}$,$x_1=x_1-\eta \frac{\partial f}{\partial x_1}$</p>
<p>η表示更新量，在神经网络的学习中，称为学习率（learning rate）。学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数。</p>
<p>超参数是一种和神经网络的参数（权重和偏置）性质不同的参数。超参数是人工设定的。包括各层的神经元数量、batch大小、参数更新时的学习率或权值衰减等。</p>
<p><strong>神经网络的学习步骤</strong>：<br>前提<br>神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”。神经网络的学习分成下面4个步骤。<br>步骤1（mini-batch）<br>从训练数据中<strong>随机</strong>选出<strong>一部分</strong>数据，这部分数据称为mini-batch。我们的目标是减小mini-batch的损失函数的值。<br>步骤2（计算梯度）<br>为了减小mini-batch的损失函数的值，需要求<strong>出各个权重参数的梯度</strong>。梯度表示损失函数的值减小最多的方向。<br>步骤3（更新参数）<br>将权重参数<strong>沿梯度方向进行微小更新</strong>。<br>步骤4（重复）<br>重复步骤1、步骤2、步骤3。</p>
<p>这里使用的数据是随机选择的mini batch数据，所以又称为随机梯度下降法（“对随机选择的数据进行的梯度下降法”）。一般由一个名为SGD的函数来实现。</p>
<p>要评价神经网络的泛化能力，就必须使用不包含在训练数据中的数据。</p>
<p>epoch是一个单位。一个epoch表示学习中<strong>所有训练数据均被使用过一次时的更新次数</strong>。</p>
<p>总结</p>
<p>• 机器学习中使用的数据集分为训练数据和测试数据。<br>• 神经网络用训练数据进行学习，并用测试数据评价学习到的模型的泛化能力。<br>• 神经网络的学习以损失函数为指标，更新权重参数，以使损失函数的值减小。</p>
<h1 id="第5章-误差反向传播法"><a href="#第5章-误差反向传播法" class="headerlink" title="第5章 误差反向传播法"></a>第5章 误差反向传播法</h1><p>计算图通过节点和箭头表示计算过程。节点用○表示，○中是计算的内容。将计算的中间结果写在箭头的上方，表示各个节点的计算结果从左向右传递。</p>
<p>用计算图解题的情况下，需要按如下流程进行。<br>1.构建计算图。<br>2.在计算图上，从左向右进行计算。</p>
<p>“从左向右进行计算”是一种正方向上的传播，简称为正向传播（forward propagation）。正向传播是从计算图出发点到结束点的传播。反向传播使用与正方向相反的箭头（粗线）表示。反向传播传递“局部导数”，将导数的值写在箭头的下方。计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值。</p>
<p><img src="1685846146920.png" alt="1685846146920"></p>
<p>ReLU函数：$ h(x)=<br>\begin{cases}<br>x, &amp; x&gt;0,\<br>0, &amp; x \le 0<br>\end{cases}$的计算图：</p>
<p><img src="1685847082124.png" alt="1685847082124"></p>
<p>sigmoid层（$h(x)=\frac{1}{1+exp(-x)}$）的计算图（拆分成几个步骤推导）：</p>
<p><img src="1685847457708.png" alt="1685847457708"></p>
<p>神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”。因此，这里将进行仿射变换的处理实现为“Affine层”。几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算。</p>
<p>乘以一个矩阵，偏导为矩阵的转置，注意标明形状：</p>
<p><img src="1685848171569.png" alt="1685848171569"></p>
<p>总结：</p>
<p>• 通过使用计算图，可以直观地把握计算过程。<br>• 计算图的节点是由局部计算构成的。局部计算构成全局计算。<br>• 计算图的正向传播进行一般的计算。通过计算图的反向传播，可以计算各个节点的导数。<br>• 通过将神经网络的组成元素实现为层，可以高效地计算梯度（反向传播法）。</p>
<h1 id="第6章-与学习相关的技巧"><a href="#第6章-与学习相关的技巧" class="headerlink" title="第6章 与学习相关的技巧"></a>第6章 与学习相关的技巧</h1><p>寻找最优参数的问题，解决这个问题的过程称为最优化。</p>
<p>SGD随机梯度下降法的缺点是，如果函数的形状非均向（anisotropic）（比如$y=\frac{1}{20}x^2+y^2$），比如呈延伸状，搜索的路径就会非常低效。根本原因是，梯度的方向并没有指向最小值的方向。</p>
<p>对SGD做出改进的三种方法：</p>
<p><img src="1685848911965.png" alt="1685848911965"></p>
<p>Momentum方法:新出现了一个变量v，对应物理上的<strong>速度</strong>。物体在梯度方向上受力，在这个力的作用下，物体的速度增加这一物理法则。在物体不受任何力时，物体逐渐减速，对应物理上的地面摩擦或空气阻力。</p>
<p>和SGD相比，我们发现“之”字形的“程度”减轻了。这是因为虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速。反过来，虽然y轴方向上受到的力很大，但是因为<strong>交互地受到正方向和反方向的力，它们会互相抵消</strong>，所以y轴方向上的速度不稳定。因此，和SGD时的情形相比，可以更快地朝x轴方向靠近，减弱“之”字形的变动程度。</p>
<p>AdaGrad:为参数的每个元素适当地<strong>调整学习率</strong>，与此同时进行学习。参数的元素中<strong>变动较大</strong>（被大幅更新）的元素的<strong>学习率将变小</strong>。也就是说，可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小。</p>
<p>函数的取值高效地向着最小值移动。由于y轴方向上的梯度较大，因此刚开始变动较大，但是后面会根据这个较大的变动按比例进行调整，减小更新的步伐。因此，y轴方向上的更新程度被减 弱，“之”字形的变动程度有所衰减。</p>
<p>Adam：融合了Momentum和AdaGrad的方法。通过组合前面两个方法的优点，有望实现参数空间的高效搜索。此外，进行<strong>超参数的“偏置校正”</strong>也是Adam的特征。</p>
<p>基于 Adam 的更新过程就像小球在碗中滚动一样。虽然Momentun也有类似的移动，但是相比之下，Adam的小球左右摇晃的程度有所减轻。这得益于学习的更新程度被适当地调整了。</p>
<p>（目前）并不存在能在所有问题中都表现良好的方法。这4种方法各有各的特点，都有各自擅长解决的问题和不擅长解决的问题。</p>
<p>为什么不能将权重初始值设成一样的值呢？这是因为在误差反向传播法中，所有的权重值都会进行相同的更新。使得神经网络拥有许多不同的权重的意义丧失了。为了防止“权重均一化”（严格地讲，是为了<strong>瓦解权重的对称结构</strong>），必须随机生成初始值。</p>
<p>Xavier初始值：与前一层有n个节点连接时，初始值使用标准差为$\sqrt\frac{1}$的分布</p>
<p>He 初始值:当前一层的节点数为n 时，使用标准差为$\sqrt\frac{2}$的高斯分布。（直观上）可以解释为，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数。</p>
<p>激活函数使用<strong>ReLU</strong>时，权重初始值使用<strong>He初始值</strong>，当激活函数为 sigmoid或 tanh等S型曲线函数时，初始值使用Xavier初始值。</p>
<p>Batch Norm(强制性调整激活值的分布)的优点：<br>• 可以使学习快速进行（可以增大学习率）。<br>• 不那么依赖初始值（对于初始值不用那么神经质）。<br>• <strong>抑制过拟合</strong>（降低Dropout等的必要性）。</p>
<p>发生过拟合的原因，主要有以下两个：<br>• 模型拥有大量参数、表现力强。<br>• 训练数据少。</p>
<p>抑制过拟合的办法：<br>1.权值衰减，该方法通过在学习的过程中<strong>对大的权重进行惩罚</strong>，来抑制过拟合。<br>2.Dropout是一种在学习的过程中<strong>随机删除神经元</strong>的方法。训练时，随机选出隐藏层的神经元，然后将其删除。被删除的神经元<strong>不再进行信号的传递</strong>。</p>
<p>集成学习，就是让多个模型单独进行学习，推理时再取多个模型的输出的平均值。</p>
<p>可以将Dropout理解为，通过在学习过程中随机删除神经元，从而每一次都让不同的模型进行学习。并且，推理时，通过对神经元的输出乘以删除比例（比如0.5等），可以取得模型的平均值。也就是说，可以理解成，Dropout将集成学习的效果（模拟地）通过一个网络实现了。</p>
<p>调整超参数时，必须使用超参数专用的确认数据。用于调整超参数的数据，一般称为验证数据（validation data）。如果使用测试数据调整超参数，超参数的值会<strong>对测试数据发生过拟合</strong>。</p>
<p>超参数的最优化：<br>步骤0 <strong>设定</strong>超参数的<strong>范围</strong>。<br>步骤1 从设定的超参数范围中<strong>随机采样</strong>。<br>步骤2 使用步骤1中采样到的超参数的值进行学习，通过验证数据<strong>评估识别精度</strong>（但是要将epoch设置得很小）。<br>步骤3 重复步骤1和步骤2（100次等），根据它们的识别精度的结果，<strong>缩小</strong>超参数的<strong>范围</strong>。</p>
<p>总结：</p>
<p>• 参数的更新方法，除了SGD之外，还有Momentum、AdaGrad、Adam等方法。<br>• 权重初始值的赋值方法对进行正确的学习非常重要。<br>• 作为权重初始值，Xavier初始值、He初始值等比较有效。<br>• 通过使用Batch Normalization，可以加速学习，并且对初始值变得健壮。<br>• 抑制过拟合的正则化技术有权值衰减、Dropout等。<br>• 逐渐缩小“好值”存在的范围是搜索超参数的一个有效方法。</p>
<h1 id="第7章-卷积神经网络"><a href="#第7章-卷积神经网络" class="headerlink" title="第7章 卷积神经网络"></a>第7章 卷积神经网络</h1><p>之前介绍的神经网络中，相邻层的所有神经元之间都有连接，这称为全连接（fully-connected）。</p>
<p><img src="1685862637667.png" alt="1685862637667"></p>
<p>CNN 中 新 增 了 Convolution 层（卷积层） 和 Pooling 层（池化层）。Pooling 层有时会被省略。</p>
<p>全连接层存在什么问题：数据的形状被“忽视”了。形状中应该含有重要的空间信息。因为全连接层会忽视形状，将全部的输入数据作为相同的神经元（同一维度的神经元）处理，所以无法利用与形状相关的信息。另一个问题是<strong>参数量巨大</strong>，因此全连接神经网络一般层数不深。</p>
<p><img src="1685863556864.png" alt="1685863556864"></p>
<p>卷积运算：以一定间隔滑动滤波器的窗口并应用，将各个位置上滤波器的元素和输入的<strong>对应元素相乘</strong>，然后再<strong>求和</strong></p>
<p><img src="1685863837638.png" alt="1685863837638"></p>
<p>使用填充主要是为了调整输出的大小。比如，对大小为(4, 4)的输入数据应用(3, 3)的滤波器时，输出大小变为(2, 2)。将填充的幅度设为1（输入数据变成了(6, 6)的形状），那么相对于输入大小(4, 4)，输出大小也保持为原来的(4, 4)。“幅度为1的填充”是指用<strong>幅度为1像素的0</strong>填充周围。</p>
<p>应用滤波器的位置间隔称为步幅（stride）。</p>
<p>假设输入大小为(H, W)，滤波器大小为(FH, FW)，输出大小为(OH, OW)，填充为P，步幅为S。此时，输出大小=输入+2倍填充-滤波器的结果除以步幅再加一：</p>
<p>$OH=\frac{H+2P-FH}{S}+1$,$OW=\frac{W+2P-FW}{S}+1$</p>
<p>上图中的计算例子：输入大小：(4, 4)；填充：1；步幅：1；滤波器大小：(3, 3)</p>
<p>3维数据的卷积运算中，输入数据和滤波器的通道数要设为相同的值。</p>
<p><img src="1685864424993.png" alt="1685864424993"></p>
<p>把3维数据表示为多维数组时，书写顺序为（channel, height, width）。比如，通道数为 C、高度为 H、长度为W的数据的形状可以写成（C, H, W）。</p>
<p>卷积运算对应的批处理：将在各层间传递的数据保存为4维数据。按(batch_num, channel, height, width)的顺序保存数据。</p>
<p><img src="1685864781873.png" alt="1685864781873"></p>
<p>图上的例子是按步幅2进行2 × 2的Max池化时的处理顺序。一般来说，池化的窗口大小会和步幅设定成相同的值。</p>
<p>池化层特征：<br><strong>没有要学习的参数</strong><br>池化层和卷积层不同，没有要学习的参数。池化只是从目标区域中取最大值（或者平均值），所以不存在要学习的参数。<br><strong>通道数不发生变化</strong><br>经过池化运算，输入数据和输出数据的通道数不会发生变化。计算是按通道独立进行的。<br><strong>对微小的位置变化具有鲁棒性（健壮）</strong><br>输入数据发生微小偏差时，池化仍会返回相同的结果。因此，池化对输入数据的微小偏差具有鲁棒性。</p>
<p>具有代表性的CNN：LeNet和AlexNet</p>
<p>总结：</p>
<p>• CNN在此前的全连接层的网络中新增了卷积层和池化层。<br>• LeNet和AlexNet是CNN的代表性网络。</p>
<h1 id="第8章-深度学习"><a href="#第8章-深度学习" class="headerlink" title="第8章 深度学习"></a>第8章 深度学习</h1><p>进一步提高识别精度的技术和线索：集成学习、学习率衰减、Data Augmentation（数据扩充）等都有助于提高识别精度。</p>
<p>Data Augmentation基于算法“人为地”扩充输入图像（训练图像）。对于输入图像，通过施加<strong>旋转、垂直或水平方向上的移动</strong>等微小变化，增加图像的数量。这在数据集的图像数量有限时尤其有效。还可以通过<strong>裁剪</strong>图像的 “crop处理”、将图像左右<strong>翻转</strong>的“flip处理” 等。<strong>亮度</strong>等外观上的变化、<strong>放大缩小</strong>等尺度上的变化也是有效的。</p>
<p>加深层的动机：层越深，识别性能也越高。可以减少网络的参数数量。与没有加深层的网络相比，加深了层的网络可以用更少的参数达到同等水平（或者更强）的表现力。</p>
<p>eg.一次5 × 5的卷积运算的区域可以由两次3 × 3的卷积运算抵充。前者的参数数量25（5 × 5），后者一共是18（2 × 3 × 3），通过叠加卷积层，参数数量减少了。</p>
<p>叠加小型滤波器来加深网络的好处是可以减少参数的数量，扩大<strong>感受野</strong>（receptive field，<strong>给神经元施加变化的某个局部空间区域</strong>）。并且，通过叠加层，将ReLU等激活函数夹在卷积层的中间，进一步提高了网络的表现力。这是因为向网络添加了基于激活函数的“非线性”表现力，通过非线性函数的叠加，可以表现更加复杂的东西。</p>
<p>另一个好处就是使学习更加高效。与没有加深层的网络相比，通过加深层，可以减少学习数据，从而高效地进行学习。<strong>可以分层次地传递信息</strong>，将各层要解决的问题分解成容易解决的简单问题。</p>
<p>总结：性能更高、减少参数、学习更高效、分层次传递信息</p>
<p>本章总结：</p>
<p>• 对于大多数的问题，都可以期待通过加深网络来提高性能。<br>• VGG、GoogLeNet、ResNet等是几个著名的网络。<br>• 深度学习（神经网络）不仅可以用于物体识别，还可以用于物体检测、图像分割。<br>• 深度学习的应用包括图像标题的生成、图像的生成、强化学习等。最近，深度学习在自动驾驶上的应用也备受期待。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://jiang54864.github.io">姜将</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://jiang54864.github.io/%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E7%8E%B0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">http://jiang54864.github.io/%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E7%8E%B0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://jiang54864.github.io" target="_blank">姜将的个人博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="https://raw.githubusercontent.com/JIANG54864/PictureCDN/main/blog/profile.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%A4%84%E7%90%86%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" title="《深度学习进阶：自然语言处理处理》读书笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">《深度学习进阶：自然语言处理处理》读书笔记</div></div></a></div><div class="next-post pull-right"><a href="/%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/WEB%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/" title="WEB数据管理知识梳理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">WEB数据管理知识梳理</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/others/%E4%B8%8EChatgpt%E8%81%8A%E8%81%8A%E5%A4%A9/" title="与Chatgpt聊聊天"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-06</div><div class="title">与Chatgpt聊聊天</div></div></a></div><div><a href="/%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%A4%84%E7%90%86%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" title="《深度学习进阶：自然语言处理处理》读书笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-08</div><div class="title">《深度学习进阶：自然语言处理处理》读书笔记</div></div></a></div><div><a href="/%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/" title="自然语言处理知识梳理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-13</div><div class="title">自然语言处理知识梳理</div></div></a></div><div><a href="/%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%B4%E7%90%86/" title="机器学习基础整理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-19</div><div class="title">机器学习基础整理</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://raw.githubusercontent.com/JIANG54864/PictureCDN/main/blog/profile.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">姜将</div><div class="author-info__description">记录学习中的知识与收获</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/JIANG54864"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/JIANG54864" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=2792663690&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="https://space.bilibili.com/262150061" target="_blank" title="Bilibili"><i class="fab fa-bilibili"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客，如有错误还请指正。本人正在考研中，博客暂缓打理，可通过头像下方联系我。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC1%E7%AB%A0-%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">第1章 前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC2%E7%AB%A0-%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">2.</span> <span class="toc-text">第2章 感知机</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC3%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">3.</span> <span class="toc-text">第3章 神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%98%B6%E8%B7%83%E5%87%BD%E6%95%B0"><span class="toc-number">3.1.</span> <span class="toc-text">阶跃函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sigmoid%E5%87%BD%E6%95%B0"><span class="toc-number">3.2.</span> <span class="toc-text">sigmoid函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ReLU%E5%87%BD%E6%95%B0"><span class="toc-number">3.3.</span> <span class="toc-text">ReLU函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#softmax%E5%87%BD%E6%95%B0"><span class="toc-number">3.4.</span> <span class="toc-text">softmax函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC4%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0"><span class="toc-number">4.</span> <span class="toc-text">第4章 神经网络的学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC5%E7%AB%A0-%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">第5章 误差反向传播法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC6%E7%AB%A0-%E4%B8%8E%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%9A%84%E6%8A%80%E5%B7%A7"><span class="toc-number">6.</span> <span class="toc-text">第6章 与学习相关的技巧</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC7%E7%AB%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">7.</span> <span class="toc-text">第7章 卷积神经网络</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC8%E7%AB%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-number">8.</span> <span class="toc-text">第8章 深度学习</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/" title="自然语言处理知识梳理">自然语言处理知识梳理</a><time datetime="2023-06-13T07:27:00.000Z" title="发表于 2023-06-13 15:27:00">2023-06-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/%E4%BC%97%E6%99%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E7%BD%91%E7%BB%9C%E5%8C%96%E4%BA%A7%E4%B8%9A%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/" title="众智科学与网络化产业知识梳理">众智科学与网络化产业知识梳理</a><time datetime="2023-06-12T04:43:52.000Z" title="发表于 2023-06-12 12:43:52">2023-06-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%A4%84%E7%90%86%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" title="《深度学习进阶：自然语言处理处理》读书笔记">《深度学习进阶：自然语言处理处理》读书笔记</a><time datetime="2023-06-08T09:38:39.000Z" title="发表于 2023-06-08 17:38:39">2023-06-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E7%8E%B0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" title="《深度学习入门：基于python的理论与实现》读书笔记">《深度学习入门：基于python的理论与实现》读书笔记</a><time datetime="2023-06-08T09:38:35.000Z" title="发表于 2023-06-08 17:38:35">2023-06-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/WEB%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/" title="WEB数据管理知识梳理">WEB数据管理知识梳理</a><time datetime="2023-06-07T07:46:51.000Z" title="发表于 2023-06-07 15:46:51">2023-06-07</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By 姜将</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>